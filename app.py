# app.py ‚Äî "HO√ÄN H·∫¢O NH·∫§T" phi√™n b·∫£n t·ªëi ∆∞u cho openai==0.28.0, FAISS fallback, ∆∞u ti√™n l·∫•y FIELD trong c√πng TOUR
# M·ª•c ti√™u: lu√¥n tr·∫£ l·ªùi b·∫±ng tr∆∞·ªùng (field) ƒë√∫ng c·ªßa tour khi user nh·∫Øc ƒë·∫øn t√™n tour ho·∫∑c h·ªèi keyword li√™n quan.
# Gi·ªØ nguy√™n m·ªçi h√†nh vi quan tr·ªçng t·ª´ file g·ªëc, th√™m robust tour-detection, t·ª´-kh√≥a‚Üífield mapping,
# an to√†n v·ªõi nhi·ªÅu SDK/phi√™n b·∫£n, fallback deterministic embedding, v√† kh·∫£ nƒÉng ch·∫°y tr√™n Render (low RAM).
# === Ch√∫ √Ω s·ª≠a duy nh·∫•t ƒë·ªÉ h·ªó tr·ª£ "ng·ªØ c·∫£nh √¥ k·∫ø li·ªÅn ph√≠a tr√™n": th√™m nh·ªõ message tr∆∞·ªõc c·ªßa c√πng user (im-memory).
# Kh√¥ng thay ƒë·ªïi logic kh√°c ngo√†i ph·∫ßn x·ª≠ l√Ω ng·ªØ c·∫£nh y√™u c·∫ßu.

import os
import json
import threading
import logging
import re
import unicodedata
from functools import lru_cache
from typing import List, Tuple, Dict, Optional
from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
import openai
import numpy as np

# Try FAISS; fallback to numpy-only index if missing
HAS_FAISS = False
try:
    import faiss  # type: ignore
    HAS_FAISS = True
except Exception:
    HAS_FAISS = False

# ---------- Logging ----------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("rbw")

# ---------- Config ----------
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
if OPENAI_API_KEY:
    openai.api_key = OPENAI_API_KEY
else:
    logger.warning("OPENAI_API_KEY not set ‚Äî embeddings/chat will fallback to deterministic behavior when possible.")

KNOWLEDGE_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")

EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.environ.get("CHAT_MODEL", "gpt-4o-mini")
TOP_K = int(os.environ.get("TOP_K", "5"))
FAISS_ENABLED = os.environ.get("FAISS_ENABLED", "true").lower() in ("1", "true", "yes")

# ---------- Flask ----------
app = Flask(__name__)
CORS(app)

# ---------- Global state ----------
KNOW: Dict = {}
FLAT_TEXTS: List[str] = []
MAPPING: List[dict] = []  # list of {"path": "...", "text": "..."}
INDEX = None
INDEX_LOCK = threading.Lock()

# Mapping normalized tour name -> index (populated after load/build)
TOUR_NAME_TO_INDEX: Dict[str, int] = {}

# ---------- In-memory per-client last-message store (for "previous-chat-cell" context) ----------
# Keying: prefer explicit client id header "X-RBW-USER"; fallback to remote_addr.
LAST_USER_MSG: Dict[str, str] = {}
LAST_USER_LOCK = threading.Lock()
# Keep only the immediate previous message per client (requirement: "√¥ k·∫ø li·ªÅn ph√≠a tr√™n")

# ---------- Keyword -> field mapping (priority) ----------
# Keep comprehensive keyword lists; preserve existing mapping semantics.
KEYWORD_FIELD_MAP: Dict[str, Dict] = {
    "tour_list": {
        "keywords": [
            "t√™n tour", "tour g√¨", "danh s√°ch tour", "c√≥ nh·ªØng tour n√†o", "li·ªát k√™ tour",
            "show tour", "tour hi·ªán c√≥", "tour available", "li·ªát k√™ c√°c tour ƒëang c√≥",
            "list tour", "tour ƒëang b√°n", "tour hi·ªán h√†nh", "tour n√†o", "tours",
            "li·ªát k√™ c√°c tour", "li·ªát k√™ c√°c h√†nh tr√¨nh", "list tours", "show tours",
            "c√°c tour hi·ªán t·∫°i", "c√°c tour ruby wings", "c√°c h√†nh tr√¨nh ƒëang m·ªü"
        ],
        "field": "tour_name"
    },

    "mission": {
        "keywords": [
            "t·∫ßm nh√¨n", "s·ª© m·ªánh", "gi√° tr·ªã c·ªët l√µi", "tri·∫øt l√Ω", "vision", "mission",
            "ƒë·ªãnh h∆∞·ªõng ph√°t tri·ªÉn", "gi√° tr·ªã th∆∞∆°ng hi·ªáu", "culture", "vƒÉn ho√° c√¥ng ty"
        ],
        "field": "mission"
    },

    "summary": {
        "keywords": [
            "t√≥m t·∫Øt ch∆∞∆°ng tr√¨nh tour", "t√≥m t·∫Øt", "t√≥m l∆∞·ª£c", "overview", "brief",
            "m√¥ t·∫£ ng·∫Øn", "tour n√†y n√≥i chung th·∫ø n√†o", "gi·ªõi thi·ªáu tour", "n·ªôi dung ch√≠nh"
        ],
        "field": "summary"
    },

    "style": {
        "keywords": [
            "phong c√°ch h√†nh tr√¨nh", "t√≠nh ch·∫•t h√†nh tr√¨nh", "concept tour", "vibe tour",
            "style", "tour ki·ªÉu g√¨", "t√≠nh ch·∫•t tour", "tour mang phong c√°ch g√¨",
            "ƒë·ªãnh d·∫°ng h√†nh tr√¨nh", "tone chuy·∫øn ƒëi", "kh√¥ng kh√≠ tour"
        ],
        "field": "style"
    },

    "transport": {
        "keywords": [
            "v·∫≠n chuy·ªÉn", "ph∆∞∆°ng ti·ªán", "di chuy·ªÉn", "xe g√¨", "transportation",
            "ƒëi b·∫±ng g√¨", "ƒëi ph∆∞∆°ng ti·ªán g√¨", "ph∆∞∆°ng ti·ªán di chuy·ªÉn", "xe bao nhi√™u ch·ªó",
            "ƒëi √¥ t√¥ hay xe m√°y", "xe ƒë∆∞a ƒë√≥n"
        ],
        "field": "transport"
    },

    "includes": {
        "keywords": [
            "l·ªãch tr√¨nh chi ti·∫øt", "ch∆∞∆°ng tr√¨nh chi ti·∫øt", "chi ti·∫øt h√†nh tr√¨nh",
            "itinerary", "schedule", "includes", "ƒëi nh·ªØng ƒë√¢u", "h√†nh tr√¨nh g·ªìm g√¨",
            "tour g·ªìm nh·ªØng ƒëi·ªÉm n√†o", "l·ªô tr√¨nh", "timeline tour"
        ],
        "field": "includes"
    },

    "location": {
        "keywords": [
            "·ªü ƒë√¢u", "ƒëi ƒë√¢u", "ƒë·ªãa ph∆∞∆°ng n√†o", "n∆°i n√†o", "ƒëi·ªÉm ƒë·∫øn", "destination",
            "location", "t·ªânh n√†o", "th√†nh ph·ªë n√†o", "tour ƒëi ƒë·ªãa ƒëi·ªÉm n√†o"
        ],
        "field": "location"
    },

    "price": {
        "keywords": [
            "gi√° tour", "chi ph√≠", "bao nhi√™u ti·ªÅn", "price", "cost",
            "gi√° bao nhi√™u", "tour gi√°", "b·∫£ng gi√°", "chi ph√≠ tr·ªçn g√≥i"
        ],
        "field": "price"
    },

    "notes": {
        "keywords": [
            "l∆∞u √Ω", "ghi ch√∫", "notes", "c·∫ßn ch√∫ √Ω", "c·∫ßn bi·∫øt",
            "l∆∞u √Ω g√¨", "ƒëi·ªÅu c·∫ßn bi·∫øt", "ch√∫ √Ω tr∆∞·ªõc khi ƒëi"
        ],
        "field": "notes"
    },

    "accommodation": {
        "keywords": [
            "ch·ªó ·ªü", "n∆°i l∆∞u tr√∫", "kh√°ch s·∫°n", "homestay", "·ªü kh√°ch s·∫°n g√¨",
            "l∆∞u tr√∫ ·ªü ƒë√¢u", "ph√≤ng ngh·ªâ", "resort", "ng·ªß ·ªü ƒë√¢u", "ti√™u chu·∫©n ph√≤ng",
            "ti√™u chu·∫©n kh√°ch s·∫°n", "hotel"
        ],
        "field": "accommodation"
    },

    "meals": {
        "keywords": [
            "ƒÉn u·ªëng", "·∫©m th·ª±c", "meals", "th·ª±c ƒë∆°n", "b·ªØa",
            "ƒÉn g√¨", "ƒÉn s√°ng", "ƒÉn tr∆∞a", "ƒÉn t·ªëi", "su·∫•t ƒÉn", "ƒë·ªì ƒÉn",
            "bao g·ªìm ƒÉn u·ªëng", "b·ªØa ƒÉn trong tour"
        ],
        "field": "meals"
    },

    "duration": {
        "keywords": [
            "m·∫•y ng√†y", "bao l√¢u", "th·ªùi gian tour", "k√©o d√†i", "ng√†y ƒë√™m",
            "2 ng√†y 1 ƒë√™m", "3 ng√†y 2 ƒë√™m", "tour bao nhi√™u ng√†y", "th·ªùi l∆∞·ª£ng tour",
            "duration", "tour d√†i bao l√¢u"
        ],
        "field": "duration"
    },

    "event_support": {
        "keywords": [
            "h·ªó tr·ª£", "d·ªãch v·ª• h·ªó tr·ª£", "event support", "d·ªãch v·ª• tƒÉng c∆∞·ªùng",
            "h·ªó tr·ª£ s·ª± ki·ªán", "d·ªãch v·ª• b·ªï tr·ª£", "h·ªó tr·ª£ ƒëo√†n", "ph·ª•c v·ª• ƒëo√†n"
        ],
        "field": "event_support"
    },

    "cancellation_policy": {
        "keywords": [
            "ph√≠ hu·ª∑", "ch√≠nh s√°ch hu·ª∑", "cancellation", "refund policy",
            "h·ªßy tour", "ƒë·ªïi l·ªãch", "h·ªßy h√†nh tr√¨nh"
        ],
        "field": "cancellation_policy"
    },

    "booking_method": {
        "keywords": [
            "ƒë·∫∑t ch·ªó", "ƒë·∫∑t tour", "booking", "c√°ch ƒë·∫∑t",
            "book tour", "li√™n h·ªá ƒë·∫∑t tour", "ph∆∞∆°ng th·ª©c ƒë·∫∑t"
        ],
        "field": "booking_method"
    },

    "who_can_join": {
        "keywords": [
            "ph√π h·ª£p ƒë·ªëi t∆∞·ª£ng", "ai tham gia", "who should join",
            "ƒë·ªëi t∆∞·ª£ng tham gia", "ai ƒëi ƒë∆∞·ª£c", "d√†nh cho ai"
        ],
        "field": "who_can_join"
    },

    "hotline": {
        "keywords": [
            "hotline", "s·ªë ƒëi·ªán tho·∫°i", "li√™n h·ªá", "contact number",
            "ƒëi·ªán tho·∫°i t∆∞ v·∫•n", "g·ªçi ai", "sdt c√¥ng ty"
        ],
        "field": "hotline"
    },
}


# ---------- Utilities ----------
def normalize_text_simple(s: str) -> str:
    """Lowercase, remove diacritics, strip punctuation, collapse spaces."""
    if not s:
        return ""
    s = s.lower()
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")  # remove diacritics
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# ---------- Index-tour-name helpers ----------
def index_tour_names():
    """Populate TOUR_NAME_TO_INDEX from MAPPING entries that end with .tour_name."""
    global TOUR_NAME_TO_INDEX
    TOUR_NAME_TO_INDEX = {}
    for m in MAPPING:
        path = m.get("path", "")
        if path.endswith(".tour_name"):
            txt = m.get("text", "") or ""
            norm = normalize_text_simple(txt)
            if not norm:
                continue
            # extract index within brackets: root.tours[2].tour_name
            match = re.search(r"\[(\d+)\]", path)
            if match:
                idx = int(match.group(1))
                prev = TOUR_NAME_TO_INDEX.get(norm)
                if prev is None:
                    TOUR_NAME_TO_INDEX[norm] = idx
                else:
                    # prefer longer original name (heuristic)
                    try:
                        prev_txt = next(m2.get("text","") for m2 in MAPPING if re.search(rf"\[{prev}\]", m2.get('path','')))
                        if len(txt) > len(prev_txt):
                            TOUR_NAME_TO_INDEX[norm] = idx
                    except StopIteration:
                        pass

def find_tour_indices_from_message(message: str) -> List[int]:
    """
    Robust detection of tour indices from a user message:
      - Prefer longest substring match between normalized tour names and normalized message.
      - If no substring matches, use token overlap heuristic.
    Returns sorted unique indices (may be multiple if multiple tour names mentioned).
    """
    if not message:
        return []
    msg_n = normalize_text_simple(message)
    if not msg_n:
        return []
    matches: List[Tuple[int, str]] = []
    for norm_name, idx in TOUR_NAME_TO_INDEX.items():
        if norm_name in msg_n or msg_n in norm_name:
            matches.append((len(norm_name), norm_name))
    if matches:
        matches.sort(reverse=True)
        max_len = matches[0][0]
        selected = [TOUR_NAME_TO_INDEX[nm] for ln, nm in matches if ln == max_len]
        return sorted(set(selected))
    # token-overlap fallback
    msg_tokens = set(msg_n.split())
    scored: List[Tuple[int, str]] = []
    for norm_name, idx in TOUR_NAME_TO_INDEX.items():
        tkn_overlap = len(msg_tokens & set(norm_name.split()))
        if tkn_overlap > 0:
            scored.append((tkn_overlap, norm_name))
    if not scored:
        return []
    scored.sort(reverse=True)
    best_score = scored[0][0]
    selected = [TOUR_NAME_TO_INDEX[nm] for sc, nm in scored if sc == best_score]
    return sorted(set(selected))

# ---------- MAPPING helpers ----------
def get_passages_by_field(field_name: str, limit: int = 50, tour_indices: Optional[List[int]] = None) -> List[Tuple[float, dict]]:
    """
    Return passages whose path ends with field_name.
    If tour_indices provided, restrict to entries matching those tour index brackets.
    Returned score is 1.0 (for priority).
    """
    out: List[Tuple[float, dict]] = []
    for m in MAPPING:
        path = m.get("path", "")
        if path.endswith(f".{field_name}") or f".{field_name}" in path:
            if tour_indices:
                matched = False
                for ti in tour_indices:
                    if f"[{ti}]" in path:
                        matched = True
                        break
                if not matched:
                    continue
            out.append((1.0, m))
    return out[:limit]

# ---------- Embeddings (robust) ----------
@lru_cache(maxsize=8192)
def embed_text(text: str) -> Tuple[List[float], int]:
    """
    Return (embedding list, dim)
    Tries openai.Embedding.create (SDK 0.28.0). If API key missing or call fails, return deterministic fallback 1536-dim.
    """
    if not text:
        return [], 0
    short = text if len(text) <= 2000 else text[:2000]
    if OPENAI_API_KEY:
        try:
            resp = openai.Embedding.create(model=EMBEDDING_MODEL, input=short)
            emb = None
            if isinstance(resp, dict) and "data" in resp and len(resp["data"]) > 0:
                emb = resp["data"][0].get("embedding") or resp["data"][0].get("vector")
            elif hasattr(resp, "data") and len(resp.data) > 0:
                emb = getattr(resp.data[0], "embedding", None)
            if emb:
                return emb, len(emb)
        except Exception:
            logger.exception("OpenAI embedding call failed ‚Äî falling back to deterministic embedding.")
    # Deterministic fallback (stable across runs)
    try:
        h = abs(hash(short)) % (10 ** 12)
        fallback_dim = 1536
        vec = [(float((h >> (i % 32)) & 0xFF) + (i % 7)) / 255.0 for i in range(fallback_dim)]
        return vec, fallback_dim
    except Exception:
        logger.exception("Fallback embedding generation failed")
        return [], 0

# ---------- Index management ----------
def _index_dim(idx) -> Optional[int]:
    # Try common attributes then faiss-specific
    try:
        d = getattr(idx, "d", None)
        if isinstance(d, int) and d > 0:
            return d
    except Exception:
        pass
    try:
        d = getattr(idx, "dim", None)
        if isinstance(d, int) and d > 0:
            return d
    except Exception:
        pass
    try:
        if HAS_FAISS and isinstance(idx, faiss.Index):
            return int(idx.d)
    except Exception:
        pass
    return None

def choose_embedding_model_for_dim(dim: int) -> str:
    if dim == 1536:
        return "text-embedding-3-small"
    if dim == 3072:
        return "text-embedding-3-large"
    return os.environ.get("EMBEDDING_MODEL", EMBEDDING_MODEL)

class NumpyIndex:
    """Simple in-memory numpy index with cosine-similarity (via normalized dot product)."""
    def __init__(self, mat: Optional[np.ndarray] = None):
        if mat is None or getattr(mat, "size", 0) == 0:
            self.mat = np.empty((0, 0), dtype="float32")
            self.dim = None
        else:
            self.mat = mat.astype("float32")
            self.dim = self.mat.shape[1]

    def add(self, mat: np.ndarray):
        if getattr(mat, "size", 0) == 0:
            return
        mat = mat.astype("float32")
        if getattr(self.mat, "size", 0) == 0:
            self.mat = mat.copy()
            self.dim = mat.shape[1]
        else:
            if mat.shape[1] != self.dim:
                raise ValueError("Dimension mismatch")
            self.mat = np.vstack([self.mat, mat])

    def search(self, qvec: np.ndarray, k: int):
        if self.mat is None or getattr(self.mat, "size", 0) == 0:
            return np.array([[]], dtype="float32"), np.array([[]], dtype="int64")
        q = qvec.astype("float32")
        q = q / (np.linalg.norm(q, axis=1, keepdims=True) + 1e-12)
        m = self.mat / (np.linalg.norm(self.mat, axis=1, keepdims=True) + 1e-12)
        sims = np.dot(q, m.T)
        idx = np.argsort(-sims, axis=1)[:, :k]
        scores = np.take_along_axis(sims, idx, axis=1)
        return scores.astype("float32"), idx.astype("int64")

    @property
    def ntotal(self):
        return 0 if getattr(self.mat, "size", 0) == 0 else self.mat.shape[0]

    def save(self, path):
        try:
            np.savez_compressed(path, mat=self.mat)
        except Exception:
            logger.exception("Failed to save fallback vectors")

    @classmethod
    def load(cls, path):
        try:
            arr = np.load(path)
            mat = arr["mat"]
            return cls(mat=mat)
        except Exception:
            logger.exception("Failed to load fallback vectors")
            return cls(None)

def load_mapping_from_disk(path=FAISS_MAPPING_PATH):
    global MAPPING, FLAT_TEXTS
    try:
        with open(path, "r", encoding="utf-8") as f:
            MAPPING[:] = json.load(f)
        FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
        logger.info("Loaded mapping from %s (%d entries)", path, len(MAPPING))
        return True
    except Exception:
        logger.exception("Failed to load mapping from disk")
        return False

def save_mapping_to_disk(path=FAISS_MAPPING_PATH):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(MAPPING, f, ensure_ascii=False, indent=2)
        logger.info("Saved mapping to %s", path)
    except Exception:
        logger.exception("Failed to save mapping")

def build_index(force_rebuild: bool = False) -> bool:
    """
    Build or load index. If FAISS enabled and available, use it; otherwise NumpyIndex.
    Will auto-detect saved index+mapping and choose embedding model if dims known.
    """
    global INDEX, MAPPING, FLAT_TEXTS, EMBEDDING_MODEL
    with INDEX_LOCK:
        use_faiss = FAISS_ENABLED and HAS_FAISS

        # try loading persisted structures first
        if not force_rebuild:
            if use_faiss and os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_MAPPING_PATH):
                try:
                    idx = faiss.read_index(FAISS_INDEX_PATH)
                    if load_mapping_from_disk(FAISS_MAPPING_PATH):
                        FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
                    idx_dim = _index_dim(idx)
                    if idx_dim:
                        EMBEDDING_MODEL = choose_embedding_model_for_dim(idx_dim)
                        logger.info("Detected FAISS index dim=%s -> embedding_model=%s", idx_dim, EMBEDDING_MODEL)
                    INDEX = idx
                    index_tour_names()
                    logger.info("‚úÖ FAISS index loaded from disk.")
                    return True
                except Exception:
                    logger.exception("Failed to load FAISS index; will rebuild.")
            if os.path.exists(FALLBACK_VECTORS_PATH) and os.path.exists(FAISS_MAPPING_PATH):
                try:
                    idx = NumpyIndex.load(FALLBACK_VECTORS_PATH)
                    if load_mapping_from_disk(FAISS_MAPPING_PATH):
                        FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
                    INDEX = idx
                    idx_dim = getattr(idx, "dim", None)
                    if idx_dim:
                        EMBEDDING_MODEL = choose_embedding_model_for_dim(int(idx_dim))
                        logger.info("Detected fallback vectors dim=%s -> embedding_model=%s", idx_dim, EMBEDDING_MODEL)
                    index_tour_names()
                    logger.info("‚úÖ Fallback index loaded from disk.")
                    return True
                except Exception:
                    logger.exception("Failed to load fallback vectors; will rebuild.")

        # need to build from FLAT_TEXTS
        if not FLAT_TEXTS:
            logger.warning("No flattened texts to index (build aborted).")
            INDEX = None
            return False

        logger.info("üîß Building embeddings for %d passages (model=%s)...", len(FLAT_TEXTS), EMBEDDING_MODEL)
        vectors = []
        dims = None
        for text in FLAT_TEXTS:
            emb, d = embed_text(text)
            if not emb:
                continue
            if dims is None:
                dims = d
            vectors.append(np.array(emb, dtype="float32"))
        if not vectors or dims is None:
            logger.warning("No vectors produced; index build aborted.")
            INDEX = None
            return False

        try:
            mat = np.vstack(vectors).astype("float32")
            # normalize rows for cosine similarity
            row_norms = np.linalg.norm(mat, axis=1, keepdims=True)
            mat = mat / (row_norms + 1e-12)

            if use_faiss:
                index = faiss.IndexFlatIP(dims)
                index.add(mat)
                INDEX = index
                try:
                    faiss.write_index(INDEX, FAISS_INDEX_PATH)
                    save_mapping_to_disk()
                except Exception:
                    logger.exception("Failed to persist FAISS index/mapping")
                index_tour_names()
                logger.info("‚úÖ FAISS index built (dims=%d, n=%d).", dims, index.ntotal)
                return True
            else:
                idx = NumpyIndex(mat)
                INDEX = idx
                try:
                    idx.save(FALLBACK_VECTORS_PATH)
                    save_mapping_to_disk()
                except Exception:
                    logger.exception("Failed to persist fallback vectors/mapping")
                index_tour_names()
                logger.info("‚úÖ Numpy fallback index built (dims=%d, n=%d).", dims, idx.ntotal)
                return True
        except Exception:
            logger.exception("Error while building index")
            INDEX = None
            return False

# ---------- Query index ----------
def query_index(query: str, top_k: int = TOP_K) -> List[Tuple[float, dict]]:
    global INDEX
    if not query:
        return []
    if INDEX is None:
        built = build_index(force_rebuild=False)
        if not built or INDEX is None:
            logger.warning("Index not available; semantic search skipped.")
            return []
    emb, d = embed_text(query)
    if not emb:
        return []
    vec = np.array(emb, dtype="float32").reshape(1, -1)
    vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)

    idx_dim = _index_dim(INDEX)
    if idx_dim and vec.shape[1] != idx_dim:
        logger.error("Query dim %s != index dim %s; will attempt rebuild with matching model.", vec.shape[1], idx_dim)
        desired_model = choose_embedding_model_for_dim(idx_dim)
        if OPENAI_API_KEY:
            global EMBEDDING_MODEL
            EMBEDDING_MODEL = desired_model
            logger.info("Setting EMBEDDING_MODEL=%s and rebuilding index...", EMBEDDING_MODEL)
            rebuilt = build_index(force_rebuild=True)
            if not rebuilt:
                logger.error("Rebuild failed; cannot perform search.")
                return []
            emb2, d2 = embed_text(query)
            if not emb2:
                return []
            vec = np.array(emb2, dtype="float32").reshape(1, -1)
            vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)
        else:
            logger.error("No OPENAI_API_KEY; cannot rebuild model-matched index.")
            return []
    try:
        D, I = INDEX.search(vec, top_k)
    except Exception:
        logger.exception("Error executing index.search")
        return []

    results: List[Tuple[float, dict]] = []
    try:
        scores = D[0].tolist() if getattr(D, "shape", None) else []
        idxs = I[0].tolist() if getattr(I, "shape", None) else []
        for score, idx in zip(scores, idxs):
            if idx < 0 or idx >= len(MAPPING):
                continue
            results.append((float(score), MAPPING[idx]))
    except Exception:
        logger.exception("Failed to parse search results")
    return results

# ---------- Prompt composition ----------
def compose_system_prompt(top_passages: List[Tuple[float, dict]]) -> str:
    header = (
        "B·∫°n l√† tr·ª£ l√Ω AI c·ªßa Ruby Wings ‚Äî chuy√™n t∆∞ v·∫•n ng√†nh du l·ªãch tr·∫£i nghi·ªám, retreat, thi·ªÅn, kh√≠ c√¥ng, h√†nh tr√¨nh ch·ªØa l√†nh.\n"
        "Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, t·ª≠ t·∫ø.\n\n"
    )
    if not top_passages:
        return header + "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n·ªôi b·ªô ph√π h·ª£p."
    content = header + "D·ªØ li·ªáu n·ªôi b·ªô (theo ƒë·ªô li√™n quan):\n"
    for i, (score, m) in enumerate(top_passages, start=1):
        content += f"\n[{i}] (score={score:.3f}) ngu·ªìn: {m.get('path','?')}\n{m.get('text','')}\n"
    content += "\n---\n∆Øu ti√™n tr√≠ch d·∫´n d·ªØ li·ªáu n·ªôi b·ªô; kh√¥ng b·ªãa; vƒÉn phong l·ªãch s·ª±."
    return content

# ---------- Routes ----------
@app.route("/")
def home():
    return jsonify({
        "status": "ok",
        "knowledge_count": len(FLAT_TEXTS),
        "index_exists": INDEX is not None,
        "index_dim": _index_dim(INDEX),
        "embedding_model": EMBEDDING_MODEL,
        "faiss_available": HAS_FAISS,
        "faiss_enabled": FAISS_ENABLED
    })

@app.route("/reindex", methods=["POST"])
def reindex():
    # require header or env allow
    secret = request.headers.get("X-RBW-ADMIN", "")
    if not secret and os.environ.get("RBW_ALLOW_REINDEX", "") != "1":
        return jsonify({"error": "reindex not allowed (set RBW_ALLOW_REINDEX=1 or provide X-RBW-ADMIN)"}), 403
    load_knowledge()  # reload raw knowledge before building
    ok = build_index(force_rebuild=True)
    return jsonify({"ok": ok, "count": len(FLAT_TEXTS)})

@app.route("/chat", methods=["POST"])
def chat():
    """
    Chat endpoint behavior:
      - If user message contains keywords mapping to a field, prioritize returning that field.
      - If a tour name is mentioned, restrict to that tour's field values.
      - If user asked for tour listing (tour_name), list all tour_name entries.
      - Else fallback to semantic search and LLM reply.

    Enhancement: use immediate previous user message (same client) to resolve follow-up short questions.
    Only the previous message is considered (requirement: "√¥ k·∫ø li·ªÅn ph√≠a tr√™n").
    """

    data = request.get_json() or {}
    user_message = (data.get("message") or "").strip()
    if not user_message:
        return jsonify({"reply": "B·∫°n ch∆∞a nh·∫≠p c√¢u h·ªèi."})

    # Determine client id: prefer explicit header (X-RBW-USER) else remote_addr
    client_id = request.headers.get("X-RBW-USER") or request.remote_addr or "anon"

    # Retrieve previous message for this client (if any)
    with LAST_USER_LOCK:
        prev_msg = LAST_USER_MSG.get(client_id, "")

    # Combine previous message + current message for better context detection
    combined_message = (prev_msg + " " + user_message).strip()

    # Detect requested field based on keywords
    text_l = combined_message.lower()
    requested_field: Optional[str] = None
    for k, v in KEYWORD_FIELD_MAP.items():
        for kw in v["keywords"]:
            if kw in text_l:
                requested_field = v["field"]
                break
        if requested_field:
            break

    # Detect tour indices using combined_message (context-aware)
    tour_indices = find_tour_indices_from_message(combined_message)

    # Update this client's last message (store ONLY current message as required)
    with LAST_USER_LOCK:
        LAST_USER_MSG[client_id] = user_message

    # Continue as original logic (KH√îNG thay ƒë·ªïi g√¨ ph·∫ßn b√™n d∆∞·ªõi)


    # Build combined short context for detection ONLY (do not feed full prev into LLM prompt to avoid confusion)
    # Strategy: if previous exists and current query is short or clearly a follow-up (like single word or short phrase),
    # combine prev + current for better tour/field detection. Otherwise, use current only.
    def is_follow_up(curr: str) -> bool:
        # treat as follow-up if very short or punctuation-only or contains only a keyword like "gi√°?", "n∆°i ·ªü?"
        if not curr:
            return False
        if len(curr.split()) <= 3:
            return True
        # if contains only 1-2 tokens and ends with question mark or is one token like "gi√°?"
        return False

    # For simplicity and robustness, always combine previous + current for detection,
    # but only if prev exists and prev is recent non-empty string.
    # This aggressively improves contextual detection and matches user's "immediate previous cell" requirement.
    if prev_msg:
        combined_for_detection = prev_msg + " " + user_message
    else:
        combined_for_detection = user_message

    text_l = combined_for_detection.lower()

    # keyword detection (maintain insertion order of KEYWORD_FIELD_MAP)
    requested_field: Optional[str] = None
    for k, v in KEYWORD_FIELD_MAP.items():
        for kw in v["keywords"]:
            if kw in text_l:
                requested_field = v["field"]
                break
        if requested_field:
            break

       # detect tour mentions using combined_for_detection (so follow-ups like "gi√° tour?" use prev context)
    tour_indices = find_tour_indices_from_message(combined_for_detection)

    # If tour indices found but user didn't ask a specific field, list matching tour names immediately
    if tour_indices and not requested_field:
        top_results = get_passages_by_field("tour_name", limit=1000, tour_indices=tour_indices)
        if top_results:
            names = [m.get("text", "") for _, m in top_results]
            seen = set()
            names_u = [x for x in names if x and not (x in seen or seen.add(x))]
            reply = "C√°c tour ph√π h·ª£p:\n" + "\n".join(f"- {n}" for n in names_u)
            return jsonify({"reply": reply, "sources": [m for _, m in top_results]})

    top_results: List[Tuple[float, dict]] = []

    # If user explicitly asked for tour_name listing -> list all tour names (not restricted)

    if requested_field == "tour_name":
        top_results = get_passages_by_field("tour_name", tour_indices=None, limit=1000)
    elif requested_field and tour_indices:
        # user asked for a specific field AND mentioned a tour -> return that field restricted to the tour(s)
        top_results = get_passages_by_field(requested_field, limit=TOP_K, tour_indices=tour_indices)
        # If none found for the specific tour(s), fallback to global field
        if not top_results:
            top_results = get_passages_by_field(requested_field, limit=TOP_K, tour_indices=None)
    elif requested_field:
        # user asked for a field but didn't name a tour -> return global matches for that field
        top_results = get_passages_by_field(requested_field, limit=TOP_K, tour_indices=None)
        if not top_results:
            # fallback to semantic search
            top_results = query_index(user_message, TOP_K)
    else:
        # No keyword matched -> semantic search (use current message for search relevance)
        top_k = int(data.get("top_k", TOP_K))
        top_results = query_index(user_message, top_k)

    # Compose system prompt from the top_results and call LLM for nicer phrasing
    system_prompt = compose_system_prompt(top_results)
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_message}]

    reply = ""
    # Try chat with OpenAI if API key present
    if OPENAI_API_KEY:
        try:
            # SDK 0.28.0 pattern
            resp = openai.ChatCompletion.create(
                model=CHAT_MODEL,
                messages=messages,
                temperature=0.2,
                max_tokens=int(data.get("max_tokens", 700)),
                top_p=0.95
            )
            # robust parsing
            if isinstance(resp, dict):
                choices = resp.get("choices", [])
                if choices:
                    first = choices[0]
                    if isinstance(first.get("message"), dict):
                        reply = first["message"].get("content", "") or ""
                    elif "text" in first:
                        reply = first.get("text", "")
                    else:
                        reply = str(first)
                else:
                    reply = str(resp)
            else:
                choices = getattr(resp, "choices", None)
                if choices and len(choices) > 0:
                    first = choices[0]
                    msg = getattr(first, "message", None)
                    if msg and isinstance(msg, dict):
                        reply = msg.get("content", "")
                    else:
                        reply = str(first)
                else:
                    reply = str(resp)
        except Exception:
            logger.exception("OpenAI chat failed; will fallback to deterministic reply.")

    # If LLM returned nothing (or not allowed), build deterministic reply favoring requested_field and tour restriction
    if not reply:
        if top_results:
            # If requested_field was tour_name, return a clean deduped list of tour names
            if requested_field == "tour_name":
                names = [m.get("text", "") for _, m in top_results]
                seen = set()
                names_u = [x for x in names if x and not (x in seen or seen.add(x))]
                reply = "C√°c tour hi·ªán c√≥:\n" + "\n".join(f"- {n}" for n in names_u)
            elif requested_field and tour_indices:
                # Provide the requested field values grouped by tour
                parts = []
                for ti in tour_indices:
                    # find tour_name by index
                    tour_name = None
                    for m in MAPPING:
                        p = m.get("path", "")
                        if p.endswith(f"tours[{ti}].tour_name"):
                            tour_name = m.get("text", "")
                            break
                    # collect requested field passages for this tour
                    field_passages = [m.get("text", "") for score, m in top_results if f"[{ti}]" in m.get("path", "")]
                    if not field_passages:
                        # explicit fetch per tour to ensure correctness if top_results were global
                        field_passages = [m.get("text", "") for _, m in get_passages_by_field(requested_field, limit=TOP_K, tour_indices=[ti])]
                    if field_passages:
                        label = f'Tour "{tour_name}"' if tour_name else f"Tour #{ti}"
                        parts.append(label + ":\n" + "\n".join(f"- {t}" for t in field_passages))
                if parts:
                    reply = "\n\n".join(parts)
                else:
                    # fallback to snippet list
                    snippets = "\n\n".join([f"- {m.get('text')}" for _, m in top_results[:5]])
                    reply = f"T√¥i t√¨m th·∫•y:\n\n{snippets}"
            else:
                # No tour restriction or not field-request -> provide top snippets
                snippets = "\n\n".join([f"- {m.get('text')}" for _, m in top_results[:5]])
                reply = f"T√¥i t√¨m th·∫•y th√¥ng tin n·ªôi b·ªô li√™n quan:\n\n{snippets}"
        else:
            reply = "Xin l·ªói ‚Äî hi·ªán kh√¥ng c√≥ d·ªØ li·ªáu n·ªôi b·ªô li√™n quan."

    # Update last user message for client so next immediate query can use it
    with LAST_USER_LOCK:
        try:
            LAST_USER_MSG[client_id] = user_message
        except Exception:
            logger.exception("Failed to update LAST_USER_MSG")

    return jsonify({"reply": reply, "sources": [m for _, m in top_results]})

# ---------- Knowledge loader ----------
def load_knowledge(path: str = KNOWLEDGE_PATH):
    """Load knowledge.json and flatten into FLAT_TEXTS + MAPPING; then index tour names."""
    global KNOW, FLAT_TEXTS, MAPPING
    try:
        with open(path, "r", encoding="utf-8") as f:
            KNOW = json.load(f)
    except Exception:
        logger.exception("Could not open knowledge.json; continuing with empty knowledge.")
        KNOW = {}
    FLAT_TEXTS = []
    MAPPING = []

    def scan(obj, prefix="root"):
        if isinstance(obj, dict):
            for k, v in obj.items():
                scan(v, f"{prefix}.{k}")
        elif isinstance(obj, list):
            for i, v in enumerate(obj):
                scan(v, f"{prefix}[{i}]")
        elif isinstance(obj, str):
            t = obj.strip()
            if t:
                FLAT_TEXTS.append(t)
                MAPPING.append({"path": prefix, "text": t})
        else:
            try:
                s = str(obj).strip()
                if s:
                    FLAT_TEXTS.append(s)
                    MAPPING.append({"path": prefix, "text": s})
            except Exception:
                pass

    scan(KNOW)
    index_tour_names()
    logger.info("‚úÖ Knowledge loaded: %d passages", len(FLAT_TEXTS))

# ---------- Initialization ----------
# Load knowledge and try to load or build index at import time (safe for Gunicorn workers)
try:
    load_knowledge()
    # try loading existing mapping file into MAPPING if present (ensures mapping order stable)
    if os.path.exists(FAISS_MAPPING_PATH):
        try:
            with open(FAISS_MAPPING_PATH, "r", encoding="utf-8") as f:
                file_map = json.load(f)
            # only update if file_map length matches current flattened passages OR if MAPPING empty
            if file_map and (len(file_map) == len(MAPPING) or len(MAPPING) == 0):
                MAPPING[:] = file_map
                FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
                index_tour_names()
                logger.info("Mapping overwritten from disk mapping.json")
        except Exception:
            logger.exception("Could not load FAISS_MAPPING_PATH at startup; proceeding with runtime-scan mapping.")

    # If index exists on disk, build_index will try to load it; otherwise it will build in background
    t = threading.Thread(target=build_index, kwargs={"force_rebuild": False}, daemon=True)
    t.start()
except Exception:
    logger.exception("Initialization error")

# When run directly, run flask dev server (note: for production use Gunicorn)
if __name__ == "__main__":
    # ensure mapping saved for reproducibility
    if MAPPING and not os.path.exists(FAISS_MAPPING_PATH):
        save_mapping_to_disk()
    # block startup building index to ensure readiness
    built = build_index(force_rebuild=False)
    if not built:
        logger.warning("Index not ready at startup; endpoint will attempt on-demand build.")
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 10000)))
