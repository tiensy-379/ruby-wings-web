#!/usr/bin/env python3
"""
app.py ‚Äî Ruby Wings Tour Chatbot v·ªõi Ng·ªØ C·∫£nh Tour ∆Øu Ti√™n (v2.0)
T∆∞∆°ng th√≠ch ho√†n to√†n v·ªõi knowledge.json, field_keywords.json, build_index.py m·ªõi
ƒê·∫£m b·∫£o chatbot nh·ªõ ng·ªØ c·∫£nh v√† ∆∞u ti√™n th√¥ng tin trong tour hi·ªán t·∫°i
"""

import os
import json
import re
import unicodedata
import threading
import logging
import uuid
from datetime import datetime, timedelta
from functools import lru_cache
from typing import List, Dict, Optional, Tuple, Any
from flask import Flask, request, jsonify
from flask_cors import CORS
import numpy as np
import difflib
from collections import defaultdict

# Optional: redis session store
try:
    import redis
except Exception:
    redis = None

# Optional FAISS
HAS_FAISS = False
try:
    import faiss
    HAS_FAISS = True
except Exception:
    HAS_FAISS = False

# OpenAI new SDK
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# ---------- Logging ----------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ruby_wings_chatbot")

# ---------- Config ----------
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.environ.get("CHAT_MODEL", "gpt-4o-mini")
KNOWLEDGE_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FIELD_KEYWORDS_PATH = os.environ.get("FIELD_KEYWORDS_PATH", "field_keywords.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")
FAISS_ENABLED = os.environ.get("FAISS_ENABLED", "true").lower() in ("1", "true", "yes")
TOP_K = int(os.environ.get("TOP_K", "8"))
TOP_K_CONTEXT = int(os.environ.get("TOP_K_CONTEXT", "15"))
SESSION_TIMEOUT = int(os.environ.get("SESSION_TIMEOUT", str(60 * 10)))  # 10 ph√∫t
SESSION_STORE = os.environ.get("SESSION_STORE", "memory")  # or 'redis'
REDIS_URL = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
CONTEXT_MEMORY = int(os.environ.get("CONTEXT_MEMORY", "5"))  # S·ªë l∆∞·ª£t h·ªèi gi·ªØ context tour

# Initialize OpenAI client if possible
client = None
if OPENAI_API_KEY and OpenAI is not None:
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        logger.info("‚úÖ OpenAI client initialized")
    except Exception:
        logger.exception("‚ùå OpenAI client init failed")
else:
    logger.info("‚ÑπÔ∏è OpenAI client not available; using deterministic responses")

# ---------- Flask app ----------
app = Flask(__name__)
CORS(app, resources={r"/api/*": {"origins": "*"}})

# ---------- Global state ----------
KNOWLEDGE_DATA: Dict = {}
MAPPING: List[dict] = []
METADATA: List[dict] = []
INDEX = None
INDEX_LOCK = threading.Lock()

# T·ª´ kh√≥a tr∆∞·ªùng d·ªØ li·ªáu
FIELD_KEYWORDS: Dict[str, List[str]] = {}
REVERSE_FIELD_KEYWORDS: Dict[str, str] = {}  # T·ª´ kh√≥a -> tr∆∞·ªùng

# Map tour name -> tour index
TOUR_NAME_TO_INDEX: Dict[str, int] = {}
TOUR_INDEX_TO_INFO: Dict[int, Dict] = {}

# Session backend
USER_SESSIONS: Dict[str, dict] = {}
if SESSION_STORE == "redis" and redis is not None:
    try:
        REDIS_CLIENT = redis.from_url(REDIS_URL)
        logger.info("‚úÖ Using Redis session store: %s", REDIS_URL)
    except Exception:
        logger.exception("‚ùå Redis init failed; falling back to memory store")
        REDIS_CLIENT = None
else:
    REDIS_CLIENT = None

# ---------- Utilities ----------

def normalize_text(text: str) -> str:
    """Chu·∫©n h√≥a vƒÉn b·∫£n: lowercase, b·ªè d·∫•u, chu·∫©n h√≥a kho·∫£ng tr·∫Øng"""
    if not text:
        return ""
    
    # Chuy·ªÉn th√†nh lowercase
    text = text.lower()
    
    # Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
    text = unicodedata.normalize('NFD', text)
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
    
    # Thay th·∫ø c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát b·∫±ng kho·∫£ng tr·∫Øng
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def token_set(text: str) -> set:
    """Chuy·ªÉn vƒÉn b·∫£n th√†nh set token"""
    return set(normalize_text(text).split())


def jaccard_similarity(set1: set, set2: set) -> float:
    """T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng Jaccard gi·ªØa 2 set"""
    if not set1 and not set2:
        return 0.0
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0


def levenshtein_similarity(a: str, b: str) -> float:
    """T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng d·ª±a tr√™n Levenshtein (approximate)"""
    if not a or not b:
        return 0.0
    return difflib.SequenceMatcher(None, a, b).ratio()


def extract_keywords(text: str) -> List[str]:
    """Tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ vƒÉn b·∫£n"""
    text = normalize_text(text)
    words = text.split()
    
    # Lo·∫°i b·ªè stopwords ƒë∆°n gi·∫£n (c√≥ th·ªÉ m·ªü r·ªông)
    stopwords = {'c√≥', 'v√†', 'ho·∫∑c', 'cho', 'v·ªÅ', 't·ª´', 'ƒë·∫øn', '·ªü', 't·∫°i', 
                 'l√†', 'c·ªßa', 'v·ªõi', 'b·∫±ng', 'theo', 'khi', 'n√†o', 'g√¨', 'bao', 'nhi√™u'}
    
    keywords = [w for w in words if w not in stopwords and len(w) > 1]
    
    # Th√™m bigram cho ƒë·ªô d√†i v·ª´a
    if len(words) >= 2:
        for i in range(len(words) - 1):
            bigram = f"{words[i]}_{words[i+1]}"
            keywords.append(bigram)
    
    return keywords


# ---------- Field Keywords Management ----------

def load_field_keywords():
    """T·∫£i t·ª´ kh√≥a tr∆∞·ªùng d·ªØ li·ªáu t·ª´ file"""
    global FIELD_KEYWORDS, REVERSE_FIELD_KEYWORDS
    
    if not os.path.exists(FIELD_KEYWORDS_PATH):
        logger.warning("‚ö†Ô∏è Field keywords file not found, using defaults")
        # T·∫°o keywords m·∫∑c ƒë·ªãnh d·ª±a tr√™n c·∫•u tr√∫c knowledge
        create_default_field_keywords()
        return
    
    try:
        with open(FIELD_KEYWORDS_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        FIELD_KEYWORDS = {}
        REVERSE_FIELD_KEYWORDS = {}
        
        for field, keywords in data.items():
            if field == "__priority_rules__":
                continue
                
            # Chu·∫©n h√≥a field name
            if field.startswith("about_company."):
                norm_field = field
            elif field.startswith("faq."):
                norm_field = field
            elif field.startswith("contact."):
                norm_field = field
            else:
                norm_field = field.split('.')[-1]
            
            FIELD_KEYWORDS[norm_field] = [normalize_text(kw) for kw in keywords]
            
            # T·∫°o reverse mapping
            for keyword in keywords:
                norm_keyword = normalize_text(keyword)
                REVERSE_FIELD_KEYWORDS[norm_keyword] = norm_field
        
        logger.info(f"‚úÖ Loaded {len(FIELD_KEYWORDS)} field keyword groups")
        logger.info(f"‚úÖ Loaded {len(REVERSE_FIELD_KEYWORDS)} keyword mappings")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load field keywords: {e}")
        create_default_field_keywords()


def create_default_field_keywords():
    """T·∫°o t·ª´ kh√≥a m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ file"""
    global FIELD_KEYWORDS, REVERSE_FIELD_KEYWORDS
    
    default_keywords = {
        "tour_name": ["tour n√†y t√™n g√¨", "tour g√¨", "t√™n tour", "tour n√†o", "h√†nh tr√¨nh g√¨"],
        "summary": ["t√≥m t·∫Øt", "gi·ªõi thi·ªáu", "m√¥ t·∫£", "overview", "t·ªïng quan"],
        "location": ["ƒëi ƒë√¢u", "ƒë·ªãa ƒëi·ªÉm", "ƒëi·ªÉm ƒë·∫øn", "location", "khu v·ª±c"],
        "duration": ["th·ªùi gian", "bao l√¢u", "m·∫•y ng√†y", "duration", "k√©o d√†i"],
        "price": ["gi√°", "chi ph√≠", "bao nhi√™u ti·ªÅn", "price", "gi√° tour"],
        "includes": ["bao g·ªìm", "g·ªìm nh·ªØng g√¨", "n·ªôi dung", "includes", "ho·∫°t ƒë·ªông"],
        "notes": ["l∆∞u √Ω", "ch√∫ √Ω", "notes", "c·∫ßn bi·∫øt", "y√™u c·∫ßu"],
        "style": ["phong c√°ch", "style", "concept", "ƒë·ªãnh h∆∞·ªõng", "lo·∫°i h√¨nh"],
        "transport": ["ph∆∞∆°ng ti·ªán", "xe", "di chuy·ªÉn", "transport", "v·∫≠n chuy·ªÉn"],
        "accommodation": ["·ªü ƒë√¢u", "l∆∞u tr√∫", "kh√°ch s·∫°n", "homestay", "accommodation"],
        "meals": ["ƒÉn u·ªëng", "b·ªØa ƒÉn", "·∫©m th·ª±c", "meals", "ƒë·ªì ƒÉn"],
        "event_support": ["h·ªó tr·ª£", "support", "d·ªãch v·ª•", "event support", "chƒÉm s√≥c"],
        "hotline": ["hotline", "s·ªë ƒëi·ªán tho·∫°i", "li√™n h·ªá", "contact", "sdt"],
        "mission": ["s·ª© m·ªánh", "mission", "m·ª•c ti√™u", "gi√° tr·ªã", "√Ω nghƒ©a"],
        "includes_extra": ["th√™m g√¨", "extra", "b·ªï sung", "t√πy ch·ªçn th√™m"],
        "extras": ["kh√¥ng bao g·ªìm", "ngo√†i gi√°", "ph·ª• ph√≠", "extras", "t·ª± t√∫c"],
        "additional": ["ph·ª• thu", "extra fee", "chi ph√≠ th√™m", "ph√°t sinh"],
        "about_company.overview": ["gi·ªõi thi·ªáu c√¥ng ty", "ruby wings l√† g√¨", "v·ªÅ ruby wings"],
        "about_company.mission": ["s·ª© m·ªánh c√¥ng ty", "mission ruby wings", "t·∫ßm nh√¨n c√¥ng ty"],
        "faq.cancellation_policy": ["ch√≠nh s√°ch h·ªßy", "h·ªßy tour", "refund", "ho√†n ti·ªÅn"],
        "faq.booking_method": ["ƒë·∫∑t tour", "c√°ch ƒë·∫∑t", "book tour", "ƒëƒÉng k√Ω"],
        "faq.who_can_join": ["ai tham gia", "ƒë·ªëi t∆∞·ª£ng", "ph√π h·ª£p v·ªõi ai", "tr·∫ª em c√≥ ƒëi ƒë∆∞·ª£c kh√¥ng"],
        "contact.hotline": ["hotline c√¥ng ty", "s·ªë ƒëi·ªán tho·∫°i c√¥ng ty", "li√™n h·ªá c√¥ng ty"],
        "contact.email": ["email c√¥ng ty", "g·ª≠i mail", "email li√™n h·ªá"],
        "contact.office_hours": ["gi·ªù l√†m vi·ªác", "th·ªùi gian t∆∞ v·∫•n", "m·ªü c·ª≠a l√∫c n√†o"]
    }
    
    FIELD_KEYWORDS = default_keywords
    REVERSE_FIELD_KEYWORDS = {}
    
    for field, keywords in default_keywords.items():
        for keyword in keywords:
            REVERSE_FIELD_KEYWORDS[normalize_text(keyword)] = field
    
    logger.info("‚úÖ Created default field keywords")


def detect_field_from_query(query: str, context_tour_index: Optional[int] = None) -> Tuple[Optional[str], float]:
    """
    Ph√°t hi·ªán tr∆∞·ªùng d·ªØ li·ªáu t·ª´ c√¢u h·ªèi
    Tr·∫£ v·ªÅ (field_name, confidence_score)
    """
    query_norm = normalize_text(query)
    query_keywords = extract_keywords(query)
    
    best_field = None
    best_score = 0.0
    
    # T√¨m ki·∫øm trong reverse field keywords
    for keyword, field in REVERSE_FIELD_KEYWORDS.items():
        if keyword in query_norm:
            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n ƒë·ªô d√†i keyword v√† v·ªã tr√≠
            score = len(keyword.split('_')) * 0.3  # ∆Øu ti√™n bigram
            
            # ∆Øu ti√™n field trong context tour
            if context_tour_index is not None and not field.startswith(('about_company.', 'faq.', 'contact.')):
                score += 0.2
            
            if score > best_score:
                best_score = score
                best_field = field
    
    # N·∫øu kh√¥ng t√¨m th·∫•y, s·ª≠ d·ª•ng heuristic
    if best_score < 0.3:
        query_lower = query.lower()
        
        # Heuristic cho c√°c tr∆∞·ªùng ph·ªï bi·∫øn
        heuristics = [
            ("gi√°", "price", 0.5),
            ("bao nhi√™u ti·ªÅn", "price", 0.7),
            ("chi ph√≠", "price", 0.6),
            ("th·ªùi gian", "duration", 0.5),
            ("m·∫•y ng√†y", "duration", 0.6),
            ("bao l√¢u", "duration", 0.5),
            ("ƒëi ƒë√¢u", "location", 0.7),
            ("·ªü ƒë√¢u", "location", 0.6),
            ("ƒë·ªãa ƒëi·ªÉm", "location", 0.5),
            ("bao g·ªìm", "includes", 0.6),
            ("g·ªìm nh·ªØng g√¨", "includes", 0.7),
            ("ƒÉn u·ªëng", "meals", 0.7),
            ("b·ªØa ƒÉn", "meals", 0.6),
            ("ph∆∞∆°ng ti·ªán", "transport", 0.6),
            ("xe", "transport", 0.5),
            ("·ªü ƒë√¢u", "accommodation", 0.6),
            ("kh√°ch s·∫°n", "accommodation", 0.5),
            ("hotline", "hotline", 0.8),
            ("s·ªë ƒëi·ªán tho·∫°i", "hotline", 0.7),
            ("li√™n h·ªá", "hotline", 0.6),
            ("l∆∞u √Ω", "notes", 0.7),
            ("c·∫ßn bi·∫øt", "notes", 0.5),
            ("phong c√°ch", "style", 0.6),
            ("h·ªó tr·ª£", "event_support", 0.5),
            ("s·ª© m·ªánh", "mission", 0.7),
            ("t·∫ßm nh√¨n", "mission", 0.6)
        ]
        
        for keyword, field, base_score in heuristics:
            if keyword in query_lower:
                score = base_score
                
                # ∆Øu ti√™n trong context tour
                if context_tour_index is not None and not field.startswith(('about_company.', 'faq.', 'contact.')):
                    score += 0.2
                
                if score > best_score:
                    best_score = score
                    best_field = field
    
    return best_field, best_score


# ---------- Tour Detection ----------

def extract_tour_name_from_query(query: str) -> Optional[Tuple[str, int]]:
    """
    Tr√≠ch xu·∫•t t√™n tour t·ª´ c√¢u h·ªèi
    Tr·∫£ v·ªÅ (tour_name, tour_index) n·∫øu t√¨m th·∫•y
    """
    query_norm = normalize_text(query)
    
    best_match = None
    best_score = 0.0
    
    for tour_name, tour_index in TOUR_NAME_TO_INDEX.items():
        # Ki·ªÉm tra xem tour_name c√≥ trong query kh√¥ng
        if tour_name in query_norm:
            score = len(tour_name.split()) * 0.3
            if score > best_score:
                best_score = score
                best_match = (tour_name, tour_index)
        
        # Ki·ªÉm tra ƒë·ªô t∆∞∆°ng ƒë·ªìng token
        else:
            tour_tokens = token_set(tour_name)
            query_tokens = token_set(query_norm)
            
            jaccard_score = jaccard_similarity(tour_tokens, query_tokens)
            if jaccard_score > 0.3 and jaccard_score > best_score:
                best_score = jaccard_score
                best_match = (tour_name, tour_index)
    
    return best_match if best_score > 0.3 else None


def find_tour_in_query(query: str) -> List[Tuple[str, int, float]]:
    """
    T√¨m t·∫•t c·∫£ c√°c tour c√≥ th·ªÉ c√≥ trong c√¢u h·ªèi
    Tr·∫£ v·ªÅ danh s√°ch (tour_name, tour_index, confidence_score)
    """
    results = []
    query_norm = normalize_text(query)
    
    for tour_name, tour_index in TOUR_NAME_TO_INDEX.items():
        score = 0.0
        
        # Exact match
        if tour_name in query_norm:
            score = 0.8
        
        # Partial match
        elif any(word in query_norm for word in tour_name.split()):
            tour_words = set(tour_name.split())
            query_words = set(query_norm.split())
            common_words = tour_words & query_words
            score = len(common_words) / len(tour_words) * 0.6
        
        # Similarity
        else:
            sim_score = levenshtein_similarity(tour_name, query_norm)
            if sim_score > 0.7:
                score = sim_score * 0.5
        
        if score > 0.3:
            results.append((tour_name, tour_index, score))
    
    # S·∫Øp x·∫øp theo ƒëi·ªÉm
    results.sort(key=lambda x: x[2], reverse=True)
    return results[:3]  # Ch·ªâ l·∫•y top 3


# ---------- Knowledge Loading ----------

def load_knowledge_data():
    """T·∫£i d·ªØ li·ªáu knowledge t·ª´ file"""
    global KNOWLEDGE_DATA
    
    if not os.path.exists(KNOWLEDGE_PATH):
        logger.error(f"‚ùå Knowledge file not found: {KNOWLEDGE_PATH}")
        KNOWLEDGE_DATA = {}
        return
    
    try:
        with open(KNOWLEDGE_PATH, 'r', encoding='utf-8') as f:
            KNOWLEDGE_DATA = json.load(f)
        logger.info(f"‚úÖ Loaded knowledge data with {len(KNOWLEDGE_DATA.get('tours', []))} tours")
    except Exception as e:
        logger.error(f"‚ùå Failed to load knowledge data: {e}")
        KNOWLEDGE_DATA = {}


def load_mapping_data():
    """T·∫£i mapping data t·ª´ file"""
    global MAPPING, METADATA, TOUR_NAME_TO_INDEX, TOUR_INDEX_TO_INFO
    
    if not os.path.exists(FAISS_MAPPING_PATH):
        logger.warning(f"‚ö†Ô∏è Mapping file not found: {FAISS_MAPPING_PATH}")
        MAPPING = []
        METADATA = []
        return
    
    try:
        with open(FAISS_MAPPING_PATH, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        # Ki·ªÉm tra c·∫•u tr√∫c m·ªõi
        if isinstance(mapping_data, dict) and "mapping" in mapping_data:
            MAPPING = mapping_data["mapping"]
            METADATA = mapping_data.get("metadata", [])
            logger.info(f"‚úÖ Loaded {len(MAPPING)} mapping entries")
        else:
            # C·∫•u tr√∫c c≈©
            MAPPING = mapping_data
            METADATA = []
            logger.info(f"‚úÖ Loaded {len(MAPPING)} mapping entries (legacy format)")
        
        # Build tour indices
        TOUR_NAME_TO_INDEX.clear()
        TOUR_INDEX_TO_INFO.clear()
        
        for entry in MAPPING:
            if entry.get("is_tour") and entry.get("tour_name") and entry.get("tour_index") is not None:
                tour_name_norm = normalize_text(entry["tour_name"])
                tour_index = entry["tour_index"]
                
                if tour_name_norm not in TOUR_NAME_TO_INDEX:
                    TOUR_NAME_TO_INDEX[tour_name_norm] = tour_index
                
                if tour_index not in TOUR_INDEX_TO_INFO:
                    TOUR_INDEX_TO_INFO[tour_index] = {
                        "name": entry["tour_name"],
                        "name_norm": tour_name_norm,
                        "fields": set()
                    }
                
                TOUR_INDEX_TO_INFO[tour_index]["fields"].add(entry.get("field", ""))
        
        logger.info(f"‚úÖ Indexed {len(TOUR_NAME_TO_INDEX)} unique tours")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load mapping data: {e}")
        MAPPING = []
        METADATA = []


def get_tour_info(tour_index: int) -> Optional[Dict]:
    """L·∫•y th√¥ng tin chi ti·∫øt c·ªßa m·ªôt tour"""
    if tour_index in TOUR_INDEX_TO_INFO:
        info = TOUR_INDEX_TO_INFO[tour_index].copy()
        info["fields"] = list(info["fields"])
        return info
    return None


# ---------- Embedding & Index Management ----------

@lru_cache(maxsize=1024)
def get_text_embedding(text: str) -> np.ndarray:
    """L·∫•y embedding cho vƒÉn b·∫£n (c√≥ cache)"""
    if not text or not text.strip():
        return np.zeros(1536, dtype=np.float32)
    
    # C·∫Øt ng·∫Øn n·∫øu qu√° d√†i
    text = text[:4000]
    
    # S·ª≠ d·ª•ng OpenAI embedding n·∫øu c√≥
    if client and OPENAI_API_KEY:
        try:
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text
            )
            embedding = np.array(response.data[0].embedding, dtype=np.float32)
            return embedding
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è OpenAI embedding failed: {e}")
    
    # Fallback: t·∫°o embedding gi·∫£
    return generate_synthetic_embedding(text)


def generate_synthetic_embedding(text: str) -> np.ndarray:
    """T·∫°o embedding t·ªïng h·ª£p cho fallback"""
    text_norm = normalize_text(text)
    words = text_norm.split()
    
    # T·∫°o vector d·ª±a tr√™n hash c·ªßa t·ª´
    vector = np.zeros(1536, dtype=np.float32)
    
    for i, word in enumerate(words[:100]):  # Gi·ªõi h·∫°n 100 t·ª´
        word_hash = hash(word) % 10000
        idx = word_hash % 1536
        vector[idx] += (i + 1) * 0.01  # Th√™m tr·ªçng s·ªë theo v·ªã tr√≠
    
    # Chu·∫©n h√≥a
    norm = np.linalg.norm(vector)
    if norm > 0:
        vector = vector / norm
    
    return vector


def load_index():
    """T·∫£i FAISS index ho·∫∑c fallback index"""
    global INDEX
    
    with INDEX_LOCK:
        if INDEX is not None:
            return INDEX
        
        # Th·ª≠ t·∫£i FAISS index
        if HAS_FAISS and FAISS_ENABLED and os.path.exists(FAISS_INDEX_PATH):
            try:
                INDEX = faiss.read_index(FAISS_INDEX_PATH)
                logger.info(f"‚úÖ Loaded FAISS index with {INDEX.ntotal} vectors")
                return INDEX
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load FAISS index: {e}")
        
        # Th·ª≠ t·∫£i fallback vectors
        if os.path.exists(FALLBACK_VECTORS_PATH):
            try:
                data = np.load(FALLBACK_VECTORS_PATH)
                vectors = data['matrix']
                
                # T·∫°o index ƒë∆°n gi·∫£n
                class SimpleIndex:
                    def __init__(self, vectors):
                        self.vectors = vectors.astype(np.float32)
                        self.ntotal = vectors.shape[0]
                        
                        # Chu·∫©n h√≥a
                        norms = np.linalg.norm(self.vectors, axis=1, keepdims=True)
                        self.vectors = self.vectors / (norms + 1e-12)
                    
                    def search(self, query_vector, k):
                        query_vector = query_vector.astype(np.float32)
                        query_norm = np.linalg.norm(query_vector)
                        if query_norm > 0:
                            query_vector = query_vector / query_norm
                        
                        similarities = np.dot(self.vectors, query_vector)
                        indices = np.argsort(-similarities)[:k]
                        distances = similarities[indices]
                        
                        return distances, indices
                
                INDEX = SimpleIndex(vectors)
                logger.info(f"‚úÖ Loaded fallback index with {INDEX.ntotal} vectors")
                return INDEX
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load fallback vectors: {e}")
        
        # T·∫°o index r·ªóng
        logger.warning("‚ö†Ô∏è No index available, creating empty index")
        INDEX = None
        return None


# ---------- Search Functions ----------

def semantic_search(query: str, top_k: int = TOP_K, context_tour_index: Optional[int] = None) -> List[Tuple[float, Dict]]:
    """
    T√¨m ki·∫øm ng·ªØ nghƒ©a trong index
    Tr·∫£ v·ªÅ danh s√°ch (score, passage) ƒë∆∞·ª£c s·∫Øp x·∫øp
    """
    # L·∫•y embedding cho query
    query_embedding = get_text_embedding(query)
    
    # T·∫£i index
    index = load_index()
    if index is None:
        return []
    
    # Th·ª±c hi·ªán t√¨m ki·∫øm
    try:
        distances, indices = index.search(query_embedding.reshape(1, -1), top_k * 3)  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ filter
    except Exception as e:
        logger.error(f"‚ùå Search error: {e}")
        return []
    
    results = []
    query_norm = normalize_text(query)
    
    for dist, idx in zip(distances[0], indices[0]):
        if idx < 0 or idx >= len(MAPPING):
            continue
        
        passage = MAPPING[idx]
        
        # T√≠nh ƒëi·ªÉm ng·ªØ c·∫£nh
        context_score = 1.0
        if context_tour_index is not None:
            if passage.get("tour_index") == context_tour_index:
                context_score = 2.0  # ∆Øu ti√™n cao cho tour hi·ªán t·∫°i
            elif passage.get("tour_index") is not None:
                context_score = 0.5  # Gi·∫£m ƒëi·ªÉm cho tour kh√°c
        
        # T√≠nh ƒëi·ªÉm t·ª´ kh√≥a
        passage_text = passage.get("text", "")
        passage_norm = normalize_text(passage_text)
        
        keyword_score = 0.0
        query_words = set(query_norm.split())
        passage_words = set(passage_norm.split())
        
        if query_words and passage_words:
            common = query_words & passage_words
            keyword_score = len(common) / len(query_words) * 0.5
        
        # T·ªïng ƒëi·ªÉm
        total_score = float(dist) * 0.6 + context_score * 0.3 + keyword_score * 0.1
        
        results.append((total_score, passage))
    
    # S·∫Øp x·∫øp v√† ch·ªâ l·∫•y top_k
    results.sort(key=lambda x: x[0], reverse=True)
    return results[:top_k]


def field_specific_search(field: str, context_tour_index: Optional[int] = None) -> List[Tuple[float, Dict]]:
    """
    T√¨m ki·∫øm theo tr∆∞·ªùng c·ª• th·ªÉ
    ∆Øu ti√™n th√¥ng tin trong tour hi·ªán t·∫°i
    """
    results = []
    
    for passage in MAPPING:
        passage_field = passage.get("field", "")
        
        # Ki·ªÉm tra xem c√≥ kh·ªõp field kh√¥ng
        field_match = False
        if field == passage_field:
            field_match = True
        elif field in FIELD_KEYWORDS and passage_field in FIELD_KEYWORDS[field]:
            field_match = True
        
        if not field_match:
            continue
        
        # T√≠nh ƒëi·ªÉm
        score = 1.0
        
        # ∆Øu ti√™n tour hi·ªán t·∫°i
        if context_tour_index is not None:
            if passage.get("tour_index") == context_tour_index:
                score = 3.0  # R·∫•t cao cho ƒë√∫ng tour
            elif passage.get("tour_index") is not None:
                score = 0.5  # Th·∫•p h∆°n cho tour kh√°c
            else:
                score = 0.3  # Th·∫•p nh·∫•t cho th√¥ng tin chung
        
        # ∆Øu ti√™n th√¥ng tin core
        if passage.get("is_core_info", False):
            score += 0.2
        
        results.append((score, passage))
    
    # S·∫Øp x·∫øp
    results.sort(key=lambda x: x[0], reverse=True)
    return results[:TOP_K]


def hybrid_search(query: str, context_tour_index: Optional[int] = None) -> List[Tuple[float, Dict]]:
    """
    T√¨m ki·∫øm k·∫øt h·ª£p: semantic + field-specific + context-aware
    """
    all_results = []
    
    # 1. Semantic search
    semantic_results = semantic_search(query, TOP_K, context_tour_index)
    all_results.extend(semantic_results)
    
    # 2. Detect field v√† field-specific search
    field, confidence = detect_field_from_query(query, context_tour_index)
    if field and confidence > 0.5:
        field_results = field_specific_search(field, context_tour_index)
        
        # TƒÉng ƒëi·ªÉm cho field-specific results
        boosted_results = []
        for score, passage in field_results:
            boosted_score = score * (1.0 + confidence * 0.5)
            boosted_results.append((boosted_score, passage))
        
        all_results.extend(boosted_results)
    
    # 3. Context-aware: th√™m th√¥ng tin t·ª´ tour hi·ªán t·∫°i
    if context_tour_index is not None:
        context_results = []
        for passage in MAPPING:
            if passage.get("tour_index") == context_tour_index:
                # T√≠nh ƒëi·ªÉm d·ª±a tr√™n ƒë·ªô li√™n quan v·ªõi query
                passage_text = passage.get("text", "")
                query_norm = normalize_text(query)
                passage_norm = normalize_text(passage_text)
                
                similarity = jaccard_similarity(
                    set(query_norm.split()),
                    set(passage_norm.split())
                )
                
                if similarity > 0.1:
                    score = 2.0 + similarity
                    context_results.append((score, passage))
        
        all_results.extend(context_results)
    
    # Lo·∫°i b·ªè tr√πng l·∫∑p v√† s·∫Øp x·∫øp
    unique_results = {}
    for score, passage in all_results:
        passage_id = f"{passage.get('path', '')}:{passage.get('text', '')[:50]}"
        if passage_id not in unique_results or score > unique_results[passage_id][0]:
            unique_results[passage_id] = (score, passage)
    
    final_results = list(unique_results.values())
    final_results.sort(key=lambda x: x[0], reverse=True)
    
    return final_results[:TOP_K_CONTEXT]


# ---------- Session Management ----------

def create_session_id() -> str:
    """T·∫°o session ID m·ªõi"""
    return str(uuid.uuid4())


def get_session(session_id: Optional[str] = None) -> Tuple[str, Dict]:
    """L·∫•y ho·∫∑c t·∫°o session"""
    if not session_id:
        session_id = request.cookies.get("session_id") if request else None
    
    # Redis session
    if REDIS_CLIENT is not None and session_id:
        try:
            key = f"session:{session_id}"
            data_json = REDIS_CLIENT.get(key)
            if data_json:
                data = json.loads(data_json)
                
                # C·∫≠p nh·∫≠t th·ªùi gian
                data["last_activity"] = datetime.now().isoformat()
                REDIS_CLIENT.setex(key, SESSION_TIMEOUT, json.dumps(data))
                
                return session_id, data
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis session error: {e}")
    
    # Memory session ho·∫∑c t·∫°o m·ªõi
    if not session_id or session_id not in USER_SESSIONS:
        session_id = create_session_id()
        USER_SESSIONS[session_id] = {
            "created_at": datetime.now().isoformat(),
            "last_activity": datetime.now().isoformat(),
            "context_tour_index": None,
            "context_tour_name": None,
            "conversation_history": [],
            "query_count": 0
        }
    
    # C·∫≠p nh·∫≠t th·ªùi gian
    USER_SESSIONS[session_id]["last_activity"] = datetime.now().isoformat()
    USER_SESSIONS[session_id]["query_count"] += 1
    
    return session_id, USER_SESSIONS[session_id]


def save_session(session_id: str, data: Dict):
    """L∆∞u session"""
    # Redis
    if REDIS_CLIENT is not None:
        try:
            key = f"session:{session_id}"
            REDIS_CLIENT.setex(key, SESSION_TIMEOUT, json.dumps(data))
            return
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis save error: {e}")
    
    # Memory
    USER_SESSIONS[session_id] = data


def update_session_context(session_data: Dict, query: str, tour_index: Optional[int] = None, tour_name: Optional[str] = None):
    """C·∫≠p nh·∫≠t ng·ªØ c·∫£nh session"""
    
    # L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i
    history = session_data.get("conversation_history", [])
    history.append({
        "query": query,
        "time": datetime.now().isoformat(),
        "tour_index": tour_index,
        "tour_name": tour_name
    })
    
    # Gi·ªØ ch·ªâ 10 m·ª•c g·∫ßn nh·∫•t
    if len(history) > 10:
        history = history[-10:]
    
    session_data["conversation_history"] = history
    
    # C·∫≠p nh·∫≠t tour context n·∫øu c√≥
    if tour_index is not None:
        session_data["context_tour_index"] = tour_index
        session_data["context_tour_name"] = tour_name
        
        # Reset query count khi chuy·ªÉn tour
        session_data["query_count"] = 1
    else:
        # N·∫øu kh√¥ng c√≥ tour m·ªõi, tƒÉng query count
        session_data["query_count"] = session_data.get("query_count", 0) + 1
        
        # N·∫øu ƒë√£ h·ªèi nhi·ªÅu m√† kh√¥ng nh·∫Øc ƒë·∫øn tour, gi·∫£m context
        if session_data["query_count"] > CONTEXT_MEMORY:
            session_data["context_tour_index"] = None
            session_data["context_tour_name"] = None


# ---------- Response Generation ----------

def generate_deterministic_response(query: str, search_results: List[Tuple[float, Dict]], 
                                  context_tour_index: Optional[int] = None) -> str:
    """T·∫°o ph·∫£n h·ªìi x√°c ƒë·ªãnh t·ª´ search results"""
    
    if not search_results:
        if context_tour_index:
            tour_name = TOUR_INDEX_TO_INFO.get(context_tour_index, {}).get("name", "tour n√†y")
            return f"Hi·ªán t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ v·ªÅ '{tour_name}' trong c∆° s·ªü d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ c√°c tour kh√°c ho·∫∑c li√™n h·ªá hotline 0332510486 ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n tr·ª±c ti·∫øp."
        else:
            return "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ h·ªèi c·ª• th·ªÉ h∆°n ho·∫∑c li√™n h·ªá Ruby Wings qua hotline 0332510486."
    
    # Nh√≥m k·∫øt qu·∫£ theo tour
    results_by_tour = defaultdict(list)
    general_results = []
    
    for score, passage in search_results:
        tour_index = passage.get("tour_index")
        if tour_index is not None:
            results_by_tour[tour_index].append((score, passage))
        else:
            general_results.append((score, passage))
    
    # X√¢y d·ª±ng ph·∫£n h·ªìi
    response_parts = []
    
    # ∆Øu ti√™n tour trong context
    if context_tour_index and context_tour_index in results_by_tour:
        tour_name = TOUR_INDEX_TO_INFO.get(context_tour_index, {}).get("name", f"Tour #{context_tour_index}")
        response_parts.append(f"**V·ªÅ tour '{tour_name}':**")
        
        for score, passage in results_by_tour[context_tour_index][:3]:
            text = passage.get("text", "")
            if text:
                response_parts.append(f"‚Ä¢ {text}")
        
        # X√≥a kh·ªèi dict ƒë·ªÉ kh√¥ng hi·ªÉn th·ªã l·∫°i
        del results_by_tour[context_tour_index]
    
    # C√°c tour kh√°c
    for tour_index, tour_results in results_by_tour.items():
        if len(tour_results) > 0:
            tour_name = TOUR_INDEX_TO_INFO.get(tour_index, {}).get("name", f"Tour #{tour_index}")
            response_parts.append(f"\n**Tour '{tour_name}':**")
            
            for score, passage in tour_results[:2]:
                text = passage.get("text", "")
                if text:
                    response_parts.append(f"‚Ä¢ {text}")
    
    # Th√¥ng tin chung
    if general_results and len(response_parts) < 3:
        response_parts.append("\n**Th√¥ng tin chung:**")
        for score, passage in general_results[:3]:
            text = passage.get("text", "")
            if text:
                response_parts.append(f"‚Ä¢ {text}")
    
    # N·∫øu c√≥ nhi·ªÅu tour, ƒë·ªÅ xu·∫•t ch·ªçn tour c·ª• th·ªÉ
    if len(results_by_tour) > 1 and not context_tour_index:
        response_parts.append(f"\nüí° T√¥i t√¨m th·∫•y th√¥ng tin trong {len(results_by_tour)} tour. Vui l√≤ng h·ªèi c·ª• th·ªÉ v·ªÅ m·ªôt tour ƒë·ªÉ nh·∫≠n th√¥ng tin chi ti·∫øt h∆°n.")
    
    response = "\n".join(response_parts)
    
    # Th√™m th√¥ng tin li√™n h·ªá n·∫øu c·∫ßn
    if "hotline" not in response.lower() and "li√™n h·ªá" not in response.lower():
        response += "\n\nüìû ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt ho·∫∑c ƒë·∫∑t tour, vui l√≤ng li√™n h·ªá Ruby Wings: 0332510486"
    
    return response


def generate_llm_response(query: str, search_results: List[Tuple[float, Dict]], 
                         context_tour_index: Optional[int] = None) -> str:
    """T·∫°o ph·∫£n h·ªìi s·ª≠ d·ª•ng LLM (n·∫øu c√≥)"""
    
    if not client or not OPENAI_API_KEY:
        return generate_deterministic_response(query, search_results, context_tour_index)
    
    # Chu·∫©n b·ªã context t·ª´ search results
    context_parts = []
    
    # Th√™m th√¥ng tin tour context n·∫øu c√≥
    if context_tour_index:
        tour_info = get_tour_info(context_tour_index)
        if tour_info:
            context_parts.append(f"NG·ªÆ C·∫¢NH HI·ªÜN T·∫†I: Ng∆∞·ªùi d√πng ƒëang h·ªèi v·ªÅ tour '{tour_info['name']}' (tour_index={context_tour_index})")
            context_parts.append("H√£y ∆∞u ti√™n th√¥ng tin t·ª´ tour n√†y trong c√¢u tr·∫£ l·ªùi.")
    
    # Th√™m search results
    context_parts.append("\nTH√îNG TIN T√åM TH·∫§Y T·ª™ C∆† S·ªû D·ªÆ LI·ªÜU:")
    
    added_passages = set()
    for score, passage in search_results[:8]:  # Gi·ªõi h·∫°n 8 passage
        passage_id = f"{passage.get('tour_index', 'general')}:{passage.get('text', '')[:50]}"
        
        if passage_id not in added_passages:
            tour_marker = ""
            if passage.get("tour_index") is not None:
                tour_name = TOUR_INDEX_TO_INFO.get(passage["tour_index"], {}).get("name", f"Tour #{passage['tour_index']}")
                tour_marker = f" [Tour: {tour_name}]"
            
            field_marker = f"[{passage.get('field', 'unknown')}]" if passage.get("field") else ""
            
            context_parts.append(f"\n{field_marker}{tour_marker} (ƒê·ªô li√™n quan: {score:.2f}):")
            context_parts.append(passage.get("text", ""))
            added_passages.add(passage_id)
    
    context = "\n".join(context_parts)
    
    # System prompt
    system_prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI c·ªßa Ruby Wings Travel - c√¥ng ty chuy√™n t·ªï ch·ª©c c√°c tour du l·ªãch tr·∫£i nghi·ªám, retreat, thi·ªÅn v√† h√†nh tr√¨nh ch·ªØa l√†nh.

QUY T·∫ÆC TR·∫¢ L·ªúI:
1. ∆ØU TI√äN CAO: S·ª≠ d·ª•ng th√¥ng tin t·ª´ NG·ªÆ C·∫¢NH HI·ªÜN T·∫†I (n·∫øu c√≥) tr∆∞·ªõc
2. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu cung c·∫•p b√™n d∆∞·ªõi
3. KH√îNG b·ªãa ra th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu
4. N·∫øu kh√¥ng c√≥ th√¥ng tin, n√≥i r√µ "T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin v·ªÅ..."
5. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n, th√¢n thi·ªán
6. Gi·ªØ c√¢u tr·∫£ l·ªùi t·∫≠p trung, kh√¥ng lan man
7. N·∫øu c√≥ th·ªÉ, ƒë·ªÅ xu·∫•t h·ªèi th√™m v·ªÅ c√°c tr∆∞·ªùng th√¥ng tin kh√°c c·ªßa tour

{context}

C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG: {query}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tr√™n:"""
    
    try:
        response = client.chat.completions.create(
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            temperature=0.3,
            max_tokens=800,
            top_p=0.9
        )
        
        return response.choices[0].message.content.strip()
    
    except Exception as e:
        logger.error(f"‚ùå LLM generation error: {e}")
        return generate_deterministic_response(query, search_results, context_tour_index)


# ---------- Main Chat Handler ----------

@app.route('/api/chat', methods=['POST'])
def chat_handler():
    """X·ª≠ l√Ω chat request"""
    
    # L·∫•y session
    session_id, session_data = get_session()
    
    # Parse request
    try:
        data = request.get_json()
        query = data.get('message', '').strip()
        
        if not query:
            return jsonify({
                'reply': 'Vui l√≤ng nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n.',
                'session_id': session_id,
                'context_tour': session_data.get('context_tour_name')
            })
    except Exception:
        return jsonify({
            'reply': 'ƒê·ªãnh d·∫°ng request kh√¥ng h·ª£p l·ªá.',
            'session_id': session_id
        }), 400
    
    # 1. Ph√°t hi·ªán tour t·ª´ query
    tour_from_query = extract_tour_name_from_query(query)
    current_tour_index = None
    current_tour_name = None
    
    if tour_from_query:
        current_tour_name, current_tour_index = tour_from_query
    else:
        # S·ª≠ d·ª•ng tour t·ª´ context n·∫øu c√≥
        current_tour_index = session_data.get('context_tour_index')
        current_tour_name = session_data.get('context_tour_name')
    
    # 2. T√¨m ki·∫øm th√¥ng tin
    search_results = hybrid_search(query, current_tour_index)
    
    # 3. T·∫°o ph·∫£n h·ªìi
    try:
        reply = generate_llm_response(query, search_results, current_tour_index)
    except Exception as e:
        logger.error(f"‚ùå Response generation error: {e}")
        reply = generate_deterministic_response(query, search_results, current_tour_index)
    
    # 4. C·∫≠p nh·∫≠t session context
    update_session_context(session_data, query, current_tour_index, current_tour_name)
    save_session(session_id, session_data)
    
    # 5. Chu·∫©n b·ªã response
    response_data = {
        'reply': reply,
        'session_id': session_id,
        'context_tour': current_tour_name,
        'has_context': current_tour_index is not None,
        'sources_count': len(search_results)
    }
    
    # Th√™m th√¥ng tin debug n·∫øu c·∫ßn
    if app.debug:
        response_data['debug'] = {
            'detected_tour_index': current_tour_index,
            'detected_tour_name': current_tour_name,
            'search_results_count': len(search_results),
            'session_query_count': session_data.get('query_count', 0)
        }
    
    # Set cookie
    response = jsonify(response_data)
    response.set_cookie(
        'session_id',
        session_id,
        max_age=SESSION_TIMEOUT,
        httponly=True,
        secure=False,  # Set True in production with HTTPS
        samesite='Lax'
    )
    
    return response


@app.route('/api/tours', methods=['GET'])
def list_tours():
    """API li·ªát k√™ t·∫•t c·∫£ tours"""
    
    tours = []
    for tour_index, tour_info in TOUR_INDEX_TO_INFO.items():
        tours.append({
            'id': tour_index,
            'name': tour_info['name'],
            'fields': list(tour_info.get('fields', [])),
            'has_full_info': True
        })
    
    return jsonify({
        'tours': tours,
        'count': len(tours)
    })


@app.route('/api/context', methods=['GET'])
def get_context():
    """API l·∫•y ng·ªØ c·∫£nh hi·ªán t·∫°i"""
    session_id = request.cookies.get('session_id')
    if not session_id:
        return jsonify({'context': None})
    
    _, session_data = get_session(session_id)
    
    return jsonify({
        'context_tour': session_data.get('context_tour_name'),
        'context_tour_index': session_data.get('context_tour_index'),
        'query_count': session_data.get('query_count', 0),
        'conversation_length': len(session_data.get('conversation_history', []))
    })


@app.route('/api/reset', methods=['POST'])
def reset_context():
    """API reset ng·ªØ c·∫£nh"""
    session_id = request.cookies.get('session_id')
    if not session_id:
        return jsonify({'success': False, 'message': 'No session'})
    
    _, session_data = get_session(session_id)
    
    # Reset context
    session_data['context_tour_index'] = None
    session_data['context_tour_name'] = None
    session_data['query_count'] = 0
    session_data['conversation_history'] = []
    
    save_session(session_id, session_data)
    
    return jsonify({'success': True, 'message': 'Context reset'})


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'tours_count': len(TOUR_NAME_TO_INDEX),
        'mapping_count': len(MAPPING),
        'index_loaded': INDEX is not None,
        'openai_available': client is not None,
        'session_count': len(USER_SESSIONS)
    })


@app.route('/api/reindex', methods=['POST'])
def reindex():
    """Reindex endpoint (admin)"""
    # Ki·ªÉm tra auth ƒë∆°n gi·∫£n
    auth_key = request.headers.get('X-Admin-Key')
    if auth_key != os.environ.get('ADMIN_KEY', 'secret'):
        return jsonify({'success': False, 'message': 'Unauthorized'}), 401
    
    # Reload data
    load_knowledge_data()
    load_mapping_data()
    load_field_keywords()
    
    # Clear index cache
    global INDEX
    INDEX = None
    load_index()
    
    return jsonify({
        'success': True,
        'message': 'Reindex completed',
        'tours_count': len(TOUR_NAME_TO_INDEX),
        'mapping_count': len(MAPPING)
    })


# ---------- Initialization ----------

def initialize_app():
    """Kh·ªüi t·∫°o ·ª©ng d·ª•ng"""
    logger.info("üöÄ Initializing Ruby Wings Chatbot...")
    
    # T·∫£i d·ªØ li·ªáu
    load_knowledge_data()
    load_mapping_data()
    load_field_keywords()
    
    # T·∫£i index
    load_index()
    
    # Log th√¥ng tin
    logger.info(f"‚úÖ Knowledge: {len(KNOWLEDGE_DATA.get('tours', []))} tours")
    logger.info(f"‚úÖ Mapping: {len(MAPPING)} passages")
    logger.info(f"‚úÖ Field keywords: {len(FIELD_KEYWORDS)} fields")
    logger.info(f"‚úÖ Tour index: {len(TOUR_NAME_TO_INDEX)} unique tours")
    logger.info(f"‚úÖ OpenAI: {'Available' if client else 'Not available'}")
    logger.info(f"‚úÖ FAISS: {'Available' if HAS_FAISS else 'Not available'}")
    logger.info(f"‚úÖ Index: {'Loaded' if INDEX else 'Not loaded'}")
    
    logger.info("üéâ Ruby Wings Chatbot initialized successfully!")


# ---------- Cleanup ----------

@app.teardown_appcontext
def cleanup_session_store(exception=None):
    """D·ªçn d·∫πp session store khi ·ª©ng d·ª•ng shutdown"""
    # ƒê·ªëi v·ªõi memory store, c√≥ th·ªÉ clear sessions c≈©
    current_time = datetime.now()
    expired_sessions = []
    
    for session_id, session_data in USER_SESSIONS.items():
        last_activity = session_data.get('last_activity')
        if last_activity:
            try:
                last_time = datetime.fromisoformat(last_activity)
                if (current_time - last_time).total_seconds() > SESSION_TIMEOUT:
                    expired_sessions.append(session_id)
            except ValueError:
                expired_sessions.append(session_id)
    
    for session_id in expired_sessions:
        USER_SESSIONS.pop(session_id, None)


# ---------- Main ----------

if __name__ == '__main__':
    # Kh·ªüi t·∫°o
    initialize_app()
    
    # Ch·∫°y ·ª©ng d·ª•ng
    port = int(os.environ.get('PORT', 10000))
    debug = os.environ.get('DEBUG', 'false').lower() == 'true'
    
    logger.info(f"üåê Starting server on port {port} (debug={debug})")
    app.run(host='0.0.0.0', port=port, debug=debug, threaded=True)
else:
    # Kh·ªüi t·∫°o khi ch·∫°y v·ªõi WSGI
    initialize_app()