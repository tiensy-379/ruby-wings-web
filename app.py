#!/usr/bin/env python3
"""
app.py ‚Äî Ruby Wings Chatbot v·ªõi Ng·ªØ C·∫£nh Tour ∆Øu Ti√™n v√† NLP N√¢ng Cao
T·ªëi ∆∞u cho Render v·ªõi Python 3.8+ v√† package compatibility
"""

import os
import json
import re
import unicodedata
import threading
import logging
import uuid
import difflib
from datetime import datetime, timedelta
from functools import lru_cache
from typing import List, Dict, Optional, Tuple, Any
from flask import Flask, request, jsonify
from flask_cors import CORS
import numpy as np
from collections import defaultdict

# ---------- Conditional Imports ----------
# X·ª≠ l√Ω import linh ho·∫°t cho c√°c package optional

# Redis session store (optional)
try:
    import redis
    HAS_REDIS = True
except ImportError:
    redis = None
    HAS_REDIS = False

# FAISS vector search (optional)
try:
    import faiss
    HAS_FAISS = True
except ImportError:
    faiss = None
    HAS_FAISS = False

# OpenAI SDK (optional)
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    OpenAI = None
    HAS_OPENAI = False

# NLP packages (optional)
try:
    from rapidfuzz import fuzz, process
    HAS_RAPIDFUZZ = True
except ImportError:
    fuzz = process = None
    HAS_RAPIDFUZZ = False

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_SKLEARN = True
except ImportError:
    TfidfVectorizer = cosine_similarity = None
    HAS_SKLEARN = False

try:
    import nltk
    from nltk.tokenize import word_tokenize
    HAS_NLTK = True
except ImportError:
    nltk = word_tokenize = None
    HAS_NLTK = False

try:
    import Levenshtein
    HAS_LEVENSHTEIN = True
except ImportError:
    Levenshtein = None
    HAS_LEVENSHTEIN = False

# ---------- Logging ----------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("ruby_wings_chatbot")

# ---------- Configuration ----------
# L·∫•y t·ª´ environment variables v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.environ.get("CHAT_MODEL", "gpt-4o-mini")
KNOWLEDGE_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FIELD_KEYWORDS_PATH = os.environ.get("FIELD_KEYWORDS_PATH", "field_keywords.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")
FAISS_ENABLED = os.environ.get("FAISS_ENABLED", "true").lower() in ("1", "true", "yes")
TOP_K = int(os.environ.get("TOP_K", "8"))
TOP_K_CONTEXT = int(os.environ.get("TOP_K_CONTEXT", "12"))
SESSION_TIMEOUT = int(os.environ.get("SESSION_TIMEOUT", "600"))  # 10 ph√∫t
SESSION_STORE = os.environ.get("SESSION_STORE", "memory")
REDIS_URL = os.environ.get("REDIS_URL", "redis://localhost:6379/0")
CONTEXT_MEMORY = int(os.environ.get("CONTEXT_MEMORY", "5"))
SIMILARITY_THRESHOLD = float(os.environ.get("SIMILARITY_THRESHOLD", "0.4"))
USE_TFIDF_FALLBACK = os.environ.get("USE_TFIDF_FALLBACK", "true").lower() in ("1", "true", "yes")
DEBUG = os.environ.get("DEBUG", "false").lower() in ("1", "true", "yes")

# ---------- Flask App ----------
app = Flask(__name__)
CORS(app, resources={
    r"/api/*": {
        "origins": ["*"],
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization"]
    }
})

# ---------- Global State ----------
KNOWLEDGE_DATA: Dict[str, Any] = {}
MAPPING: List[Dict[str, Any]] = []
METADATA: List[Dict[str, Any]] = []
VECTOR_INDEX = None
TFIDF_INDEX = None
INDEX_LOCK = threading.Lock()

# Field keywords v√† reverse mapping
FIELD_KEYWORDS: Dict[str, List[str]] = {}
REVERSE_KEYWORD_MAP: Dict[str, str] = {}

# Tour indices
TOUR_NAME_TO_INDEX: Dict[str, int] = {}
TOUR_INDEX_TO_INFO: Dict[int, Dict[str, Any]] = {}

# Session management
USER_SESSIONS: Dict[str, Dict[str, Any]] = {}
if SESSION_STORE == "redis" and HAS_REDIS:
    try:
        REDIS_CLIENT = redis.from_url(REDIS_URL)
        logger.info("‚úÖ Redis session store initialized")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Redis connection failed: {e}. Falling back to memory store.")
        REDIS_CLIENT = None
else:
    REDIS_CLIENT = None

# ---------- Text Processing Utilities ----------

def normalize_text(text: str) -> str:
    """
    Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát
    - Lowercase
    - Lo·∫°i b·ªè d·∫•u
    - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    - Gi·ªØ l·∫°i s·ªë v√† ch·ªØ
    """
    if not text or not isinstance(text, str):
        return ""
    
    # Lowercase
    text = text.lower()
    
    # Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát
    text = unicodedata.normalize('NFD', text)
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
    
    # Thay th·∫ø c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i s·ªë v√† ch·ªØ)
    text = re.sub(r'[^\w\s\d]', ' ', text)
    
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def tokenize_text(text: str) -> List[str]:
    """
    Tokenize vƒÉn b·∫£n th√†nh c√°c t·ª´
    S·ª≠ d·ª•ng NLTK n·∫øu c√≥, fallback v·ªÅ split ƒë∆°n gi·∫£n
    """
    if HAS_NLTK:
        try:
            tokens = word_tokenize(text)
            return [token for token in tokens if token.isalnum()]
        except Exception:
            pass
    
    # Fallback: split ƒë∆°n gi·∫£n
    return [word for word in text.split() if word.isalnum()]


def extract_keywords(text: str) -> List[str]:
    """
    Tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ vƒÉn b·∫£n
    Lo·∫°i b·ªè stopwords v√† t·ª´ ng·∫Øn
    """
    text_norm = normalize_text(text)
    tokens = tokenize_text(text_norm)
    
    # Stopwords ti·∫øng Vi·ªát
    vietnamese_stopwords = {
        'c√≥', 'v√†', 'ho·∫∑c', 'cho', 'v·ªÅ', 't·ª´', 'ƒë·∫øn', '·ªü', 't·∫°i', 'l√†',
        'c·ªßa', 'v·ªõi', 'b·∫±ng', 'theo', 'khi', 'n√†o', 'g√¨', 'bao', 'nhi√™u',
        'c√°c', 'nh·ªØng', 'm·∫•y', 'nhi·ªÅu', '√≠t', 'r·∫•t', 'qu√°', 'l·∫Øm', 'ƒë√£',
        'ƒëang', 's·∫Ω', 'v·∫´n', 'c≈©ng', 'ƒë·ªÅu', 'm·ªçi', 'm·ªói', 't·ª´ng', 'nh∆∞',
        'nh∆∞ng', 'm√†', 'n√™n', 'th√¨', 'l√†m', 'c·∫ßn', 'ph·∫£i', 'ƒë∆∞·ª£c', 'b·ªã',
        'trong', 'ngo√†i', 'tr√™n', 'd∆∞·ªõi', 'tr∆∞·ªõc', 'sau', 'gi·ªØa', 'b√™n'
    }
    
    # L·ªçc stopwords v√† t·ª´ ng·∫Øn
    keywords = []
    for token in tokens:
        if (len(token) > 1 and 
            token not in vietnamese_stopwords and
            not token.isdigit()):
            keywords.append(token)
    
    # Th√™m bigram cho c√°c t·ª´ li√™n ti·∫øp
    if len(tokens) >= 2:
        for i in range(len(tokens) - 1):
            if len(tokens[i]) > 1 and len(tokens[i+1]) > 1:
                bigram = f"{tokens[i]}_{tokens[i+1]}"
                keywords.append(bigram)
    
    return list(set(keywords))  # Remove duplicates


def calculate_similarity(text1: str, text2: str) -> float:
    """
    T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa hai vƒÉn b·∫£n
    S·ª≠ d·ª•ng multiple methods v√† k·∫øt h·ª£p k·∫øt qu·∫£
    """
    if not text1 or not text2:
        return 0.0
    
    text1_norm = normalize_text(text1)
    text2_norm = normalize_text(text2)
    
    scores = []
    
    # 1. RapidFuzz similarity (n·∫øu c√≥)
    if HAS_RAPIDFUZZ:
        try:
            # Weighted Ratio - t·ªët cho ti·∫øng Vi·ªát
            score = fuzz.WRatio(text1_norm, text2_norm) / 100.0
            scores.append(score)
            
            # Token Sort Ratio (kh√¥ng quan t√¢m th·ª© t·ª± t·ª´)
            token_score = fuzz.token_sort_ratio(text1_norm, text2_norm) / 100.0
            scores.append(token_score * 0.8)
        except Exception:
            pass
    
    # 2. SequenceMatcher (fallback built-in)
    seq_score = difflib.SequenceMatcher(None, text1_norm, text2_norm).ratio()
    scores.append(seq_score * 0.7)
    
    # 3. Jaccard similarity tr√™n keywords
    keywords1 = extract_keywords(text1)
    keywords2 = extract_keywords(text2)
    
    if keywords1 and keywords2:
        set1 = set(keywords1)
        set2 = set(keywords2)
        union = set1 | set2
        if union:
            jaccard = len(set1 & set2) / len(union)
            scores.append(jaccard * 0.6)
    
    # 4. Levenshtein distance (n·∫øu c√≥ package)
    if HAS_LEVENSHTEIN:
        try:
            max_len = max(len(text1_norm), len(text2_norm))
            if max_len > 0:
                lev_dist = Levenshtein.distance(text1_norm, text2_norm)
                lev_score = 1 - (lev_dist / max_len)
                scores.append(lev_score * 0.5)
        except Exception:
            pass
    
    # Tr·∫£ v·ªÅ ƒëi·ªÉm trung b√¨nh
    return sum(scores) / len(scores) if scores else 0.0


# ---------- TF-IDF Index (Fallback khi kh√¥ng c√≥ embeddings) ----------

class TFIDFIndex:
    """TF-IDF index cho text search fallback"""
    
    def __init__(self):
        self.vectorizer = None
        self.tfidf_matrix = None
        self.documents = []
        self.is_built = False
    
    def build(self, documents: List[Dict[str, Any]]) -> bool:
        """X√¢y d·ª±ng TF-IDF index t·ª´ documents"""
        if not HAS_SKLEARN or not documents:
            return False
        
        try:
            texts = []
            self.documents = []
            
            for doc in documents:
                text = doc.get("text", "")
                if text and len(text.strip()) > 10:  # Ch·ªâ l·∫•y text ƒë·ªß d√†i
                    texts.append(text)
                    self.documents.append(doc)
            
            if len(texts) < 5:
                logger.warning("‚ö†Ô∏è Not enough documents for TF-IDF index")
                return False
            
            # T·∫°o vectorizer v·ªõi c√°c tham s·ªë t·ªëi ∆∞u cho ti·∫øng Vi·ªát
            self.vectorizer = TfidfVectorizer(
                max_features=2000,
                min_df=2,
                max_df=0.85,
                ngram_range=(1, 2),
                stop_words=None,
                token_pattern=r'\b\w+\b'
            )
            
            self.tfidf_matrix = self.vectorizer.fit_transform(texts)
            self.is_built = True
            
            logger.info(f"‚úÖ TF-IDF index built with {len(texts)} documents")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå TF-IDF build error: {e}")
            return False
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[float, Dict[str, Any]]]:
        """T√¨m ki·∫øm v·ªõi TF-IDF"""
        if not self.is_built or not self.vectorizer or not self.tfidf_matrix:
            return []
        
        try:
            query_vec = self.vectorizer.transform([query])
            similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
            
            # L·∫•y top_k k·∫øt qu·∫£
            top_indices = similarities.argsort()[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if idx < len(self.documents):
                    score = float(similarities[idx])
                    if score > 0.1:  # Ng∆∞·ª°ng t·ªëi thi·ªÉu
                        results.append((score, self.documents[idx]))
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå TF-IDF search error: {e}")
            return []


# ---------- Field Keywords Management ----------

def load_field_keywords():
    """T·∫£i t·ª´ kh√≥a tr∆∞·ªùng d·ªØ li·ªáu t·ª´ file"""
    global FIELD_KEYWORDS, REVERSE_KEYWORD_MAP
    
    FIELD_KEYWORDS = {}
    REVERSE_KEYWORD_MAP = {}
    
    if not os.path.exists(FIELD_KEYWORDS_PATH):
        logger.warning(f"‚ö†Ô∏è Field keywords file not found: {FIELD_KEYWORDS_PATH}")
        create_default_field_keywords()
        return
    
    try:
        with open(FIELD_KEYWORDS_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for field, keywords in data.items():
            if field.startswith("__"):  # B·ªè qua metadata
                continue
            
            # Chu·∫©n h√≥a field name
            if '.' in field:
                norm_field = field  # Gi·ªØ nguy√™n cho nested fields
            else:
                norm_field = field
            
            # Chu·∫©n h√≥a keywords
            norm_keywords = [normalize_text(kw) for kw in keywords]
            FIELD_KEYWORDS[norm_field] = norm_keywords
            
            # T·∫°o reverse mapping
            for keyword in norm_keywords:
                REVERSE_KEYWORD_MAP[keyword] = norm_field
        
        logger.info(f"‚úÖ Loaded {len(FIELD_KEYWORDS)} field keyword groups")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load field keywords: {e}")
        create_default_field_keywords()


def create_default_field_keywords():
    """T·∫°o t·ª´ kh√≥a m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ file"""
    global FIELD_KEYWORDS, REVERSE_KEYWORD_MAP
    
    default_keywords = {
        "tour_name": ["tour n√†y t√™n g√¨", "tour g√¨", "t√™n tour", "tour n√†o", "h√†nh tr√¨nh g√¨", "ch∆∞∆°ng tr√¨nh g√¨"],
        "summary": ["t√≥m t·∫Øt", "gi·ªõi thi·ªáu", "m√¥ t·∫£", "overview", "t·ªïng quan", "n·ªôi dung ch√≠nh"],
        "location": ["ƒëi ƒë√¢u", "ƒë·ªãa ƒëi·ªÉm", "ƒëi·ªÉm ƒë·∫øn", "location", "khu v·ª±c", "v√πng mi·ªÅn"],
        "duration": ["th·ªùi gian", "bao l√¢u", "m·∫•y ng√†y", "duration", "k√©o d√†i", "th·ªùi l∆∞·ª£ng"],
        "price": ["gi√°", "chi ph√≠", "bao nhi√™u ti·ªÅn", "price", "gi√° tour", "m·ª©c gi√°"],
        "includes": ["bao g·ªìm", "g·ªìm nh·ªØng g√¨", "n·ªôi dung", "includes", "ho·∫°t ƒë·ªông", "ƒëi·ªÉm tham quan"],
        "notes": ["l∆∞u √Ω", "ch√∫ √Ω", "notes", "c·∫ßn bi·∫øt", "y√™u c·∫ßu", "chu·∫©n b·ªã"],
        "style": ["phong c√°ch", "style", "concept", "ƒë·ªãnh h∆∞·ªõng", "lo·∫°i h√¨nh", "h√¨nh th·ª©c"],
        "transport": ["ph∆∞∆°ng ti·ªán", "xe", "di chuy·ªÉn", "transport", "v·∫≠n chuy·ªÉn", "ƒëi l·∫°i"],
        "accommodation": ["·ªü ƒë√¢u", "l∆∞u tr√∫", "kh√°ch s·∫°n", "homestay", "accommodation", "ch·ªó ngh·ªâ"],
        "meals": ["ƒÉn u·ªëng", "b·ªØa ƒÉn", "·∫©m th·ª±c", "meals", "ƒë·ªì ƒÉn", "th·ª©c ƒÉn"],
        "event_support": ["h·ªó tr·ª£", "support", "d·ªãch v·ª•", "event support", "chƒÉm s√≥c", "h·ªó tr·ª£ ƒëo√†n"],
        "hotline": ["hotline", "s·ªë ƒëi·ªán tho·∫°i", "li√™n h·ªá", "contact", "sdt", "g·ªçi ƒëi·ªán"],
        "mission": ["s·ª© m·ªánh", "mission", "m·ª•c ti√™u", "gi√° tr·ªã", "√Ω nghƒ©a", "t·∫ßm nh√¨n"],
        "includes_extra": ["th√™m g√¨", "extra", "b·ªï sung", "t√πy ch·ªçn th√™m", "d·ªãch v·ª• th√™m"],
        "extras": ["kh√¥ng bao g·ªìm", "ngo√†i gi√°", "ph·ª• ph√≠", "extras", "t·ª± t√∫c", "t·ª± chi tr·∫£"],
        "additional": ["ph·ª• thu", "extra fee", "chi ph√≠ th√™m", "ph√°t sinh", "n√¢ng c·∫•p"],
        "about_company.overview": ["gi·ªõi thi·ªáu c√¥ng ty", "ruby wings l√† g√¨", "v·ªÅ ruby wings", "c√¥ng ty l√†m g√¨"],
        "about_company.mission": ["s·ª© m·ªánh c√¥ng ty", "mission ruby wings", "t·∫ßm nh√¨n c√¥ng ty", "gi√° tr·ªã c·ªët l√µi"],
        "faq.cancellation_policy": ["ch√≠nh s√°ch h·ªßy", "h·ªßy tour", "refund", "ho√†n ti·ªÅn", "h·ªßy ƒë·∫∑t"],
        "faq.booking_method": ["ƒë·∫∑t tour", "c√°ch ƒë·∫∑t", "book tour", "ƒëƒÉng k√Ω", "ƒë·∫∑t ch·ªó"],
        "faq.who_can_join": ["ai tham gia", "ƒë·ªëi t∆∞·ª£ng", "ph√π h·ª£p v·ªõi ai", "tr·∫ª em c√≥ ƒëi ƒë∆∞·ª£c kh√¥ng"],
        "contact.hotline": ["hotline c√¥ng ty", "s·ªë ƒëi·ªán tho·∫°i c√¥ng ty", "li√™n h·ªá c√¥ng ty", "t·ªïng ƒë√†i"],
        "contact.email": ["email c√¥ng ty", "g·ª≠i mail", "email li√™n h·ªá", "mail c√¥ng ty"],
        "contact.office_hours": ["gi·ªù l√†m vi·ªác", "th·ªùi gian t∆∞ v·∫•n", "m·ªü c·ª≠a l√∫c n√†o", "gi·ªù h√†nh ch√≠nh"]
    }
    
    FIELD_KEYWORDS = default_keywords
    REVERSE_KEYWORD_MAP = {}
    
    for field, keywords in default_keywords.items():
        for keyword in keywords:
            norm_keyword = normalize_text(keyword)
            REVERSE_KEYWORD_MAP[norm_keyword] = field
    
    logger.info("‚úÖ Created default field keywords")


def detect_field_from_query(query: str) -> Tuple[Optional[str], float]:
    """
    Ph√°t hi·ªán tr∆∞·ªùng d·ªØ li·ªáu t·ª´ c√¢u h·ªèi
    Tr·∫£ v·ªÅ (field_name, confidence_score)
    """
    query_norm = normalize_text(query)
    
    best_field = None
    best_score = 0.0
    
    # T√¨m ki·∫øm trong reverse keyword map
    for keyword, field in REVERSE_KEYWORD_MAP.items():
        if keyword in query_norm:
            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n ƒë·ªô d√†i keyword
            score = min(len(keyword.split()), 3) * 0.2
            
            # ∆Øu ti√™n exact match
            if f" {keyword} " in f" {query_norm} ":
                score += 0.3
            
            if score > best_score:
                best_score = score
                best_field = field
    
    # N·∫øu kh√¥ng t√¨m th·∫•y, s·ª≠ d·ª•ng heuristic
    if best_score < 0.3:
        query_lower = query.lower()
        
        # Heuristic cho c√°c tr∆∞·ªùng ph·ªï bi·∫øn
        heuristics = [
            (["gi√°", "bao nhi√™u ti·ªÅn", "chi ph√≠"], "price", 0.7),
            (["th·ªùi gian", "m·∫•y ng√†y", "bao l√¢u"], "duration", 0.6),
            (["ƒëi ƒë√¢u", "·ªü ƒë√¢u", "ƒë·ªãa ƒëi·ªÉm"], "location", 0.6),
            (["bao g·ªìm", "g·ªìm nh·ªØng g√¨"], "includes", 0.5),
            (["ƒÉn u·ªëng", "b·ªØa ƒÉn", "·∫©m th·ª±c"], "meals", 0.5),
            (["ph∆∞∆°ng ti·ªán", "xe", "di chuy·ªÉn"], "transport", 0.5),
            (["hotline", "s·ªë ƒëi·ªán tho·∫°i", "li√™n h·ªá"], "hotline", 0.8),
            (["l∆∞u √Ω", "c·∫ßn bi·∫øt", "chu·∫©n b·ªã"], "notes", 0.5),
        ]
        
        for keywords, field, base_score in heuristics:
            for keyword in keywords:
                if keyword in query_lower:
                    if base_score > best_score:
                        best_score = base_score
                        best_field = field
                    break
    
    return best_field, best_score


# ---------- Tour Detection ----------

def load_tour_indices():
    """X√¢y d·ª±ng index cho tour t·ª´ mapping data"""
    global TOUR_NAME_TO_INDEX, TOUR_INDEX_TO_INFO
    
    TOUR_NAME_TO_INDEX.clear()
    TOUR_INDEX_TO_INFO.clear()
    
    for passage in MAPPING:
        tour_index = passage.get("tour_index")
        tour_name = passage.get("tour_name")
        
        if tour_index is not None and tour_name:
            # L∆∞u mapping t·ª´ t√™n tour chu·∫©n h√≥a ƒë·∫øn index
            tour_name_norm = normalize_text(tour_name)
            if tour_name_norm not in TOUR_NAME_TO_INDEX:
                TOUR_NAME_TO_INDEX[tour_name_norm] = tour_index
            
            # L∆∞u th√¥ng tin tour
            if tour_index not in TOUR_INDEX_TO_INFO:
                TOUR_INDEX_TO_INFO[tour_index] = {
                    "name": tour_name,
                    "name_norm": tour_name_norm,
                    "fields": set()
                }
            
            # Th√™m field v√†o set
            field = passage.get("field")
            if field:
                TOUR_INDEX_TO_INFO[tour_index]["fields"].add(field)
    
    logger.info(f"‚úÖ Indexed {len(TOUR_NAME_TO_INDEX)} unique tours")


def extract_tour_from_query(query: str) -> Optional[Tuple[str, int, float]]:
    """
    Tr√≠ch xu·∫•t tour t·ª´ c√¢u h·ªèi v·ªõi fuzzy matching
    Tr·∫£ v·ªÅ (tour_name, tour_index, confidence)
    """
    if not TOUR_NAME_TO_INDEX:
        return None
    
    query_norm = normalize_text(query)
    
    # S·ª≠ d·ª•ng RapidFuzz n·∫øu c√≥
    if HAS_RAPIDFUZZ:
        try:
            # T√¨m tour kh·ªõp nh·∫•t
            best_match = process.extractOne(
                query_norm,
                list(TOUR_NAME_TO_INDEX.keys()),
                scorer=fuzz.WRatio,
                score_cutoff=50  # Ng∆∞·ª°ng 50%
            )
            
            if best_match:
                tour_name_norm, score, _ = best_match
                tour_index = TOUR_NAME_TO_INDEX[tour_name_norm]
                tour_info = TOUR_INDEX_TO_INFO.get(tour_index)
                
                if tour_info:
                    confidence = score / 100.0
                    return tour_info["name"], tour_index, confidence
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è RapidFuzz search error: {e}")
    
    # Fallback: t√¨m ki·∫øm ƒë∆°n gi·∫£n
    best_match = None
    best_score = 0.0
    
    for tour_name_norm, tour_index in TOUR_NAME_TO_INDEX.items():
        tour_info = TOUR_INDEX_TO_INFO.get(tour_index)
        if not tour_info:
            continue
        
        # T√≠nh similarity
        similarity = calculate_similarity(query, tour_info["name"])
        
        # Bonus n·∫øu c√≥ t·ª´ kh√≥a chung
        query_keywords = extract_keywords(query)
        tour_keywords = extract_keywords(tour_info["name"])
        common_keywords = set(query_keywords) & set(tour_keywords)
        if common_keywords:
            similarity += len(common_keywords) * 0.1
        
        if similarity > best_score and similarity > SIMILARITY_THRESHOLD:
            best_score = similarity
            best_match = (tour_info["name"], tour_index, similarity)
    
    return best_match


# ---------- Embedding & Vector Search ----------

@lru_cache(maxsize=1024)
def get_text_embedding(text: str) -> np.ndarray:
    """
    L·∫•y embedding cho vƒÉn b·∫£n
    S·ª≠ d·ª•ng OpenAI n·∫øu c√≥, fallback synthetic
    """
    if not text or not text.strip():
        return np.zeros(1536, dtype=np.float32)
    
    # C·∫Øt ng·∫Øn n·∫øu qu√° d√†i
    text = text[:4000]
    
    # S·ª≠ d·ª•ng OpenAI embedding n·∫øu c√≥
    if HAS_OPENAI and OPENAI_API_KEY:
        try:
            client = OpenAI(api_key=OPENAI_API_KEY)
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=text
            )
            embedding = np.array(response.data[0].embedding, dtype=np.float32)
            return embedding
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è OpenAI embedding failed: {e}")
    
    # Fallback: t·∫°o embedding t·ªïng h·ª£p
    return generate_synthetic_embedding(text)


def generate_synthetic_embedding(text: str) -> np.ndarray:
    """T·∫°o embedding t·ªïng h·ª£p cho fallback"""
    text_norm = normalize_text(text)
    words = text_norm.split()[:100]  # Gi·ªõi h·∫°n 100 t·ª´
    
    vector = np.zeros(1536, dtype=np.float32)
    
    for i, word in enumerate(words):
        # T·∫°o hash deterministic t·ª´ word
        word_hash = hash(word) % 10000
        
        # Ph√¢n b·ªë v√†o vector
        for j in range(10):
            idx = (word_hash + i * j) % 1536
            vector[idx] += (i + 1) * 0.001
    
    # Chu·∫©n h√≥a
    norm = np.linalg.norm(vector)
    if norm > 0:
        vector = vector / norm
    
    return vector


def load_vector_index():
    """T·∫£i FAISS index ho·∫∑c t·∫°o fallback"""
    global VECTOR_INDEX
    
    with INDEX_LOCK:
        if VECTOR_INDEX is not None:
            return VECTOR_INDEX
        
        # Th·ª≠ t·∫£i FAISS index
        if HAS_FAISS and FAISS_ENABLED and os.path.exists(FAISS_INDEX_PATH):
            try:
                VECTOR_INDEX = faiss.read_index(FAISS_INDEX_PATH)
                logger.info(f"‚úÖ Loaded FAISS index with {VECTOR_INDEX.ntotal} vectors")
                return VECTOR_INDEX
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load FAISS index: {e}")
        
        # Th·ª≠ t·∫£i fallback vectors
        if os.path.exists(FALLBACK_VECTORS_PATH):
            try:
                data = np.load(FALLBACK_VECTORS_PATH)
                
                if 'matrix' in data:
                    vectors = data['matrix']
                elif 'mat' in data:
                    vectors = data['mat']
                else:
                    logger.error("‚ùå Unknown format in vectors file")
                    return None
                
                # T·∫°o SimpleIndex
                class SimpleIndex:
                    def __init__(self, vectors):
                        self.vectors = vectors.astype(np.float32)
                        self.ntotal = vectors.shape[0]
                        
                        # Chu·∫©n h√≥a vectors
                        norms = np.linalg.norm(self.vectors, axis=1, keepdims=True)
                        self.vectors = self.vectors / (norms + 1e-12)
                    
                    def search(self, query_vector, k):
                        query_vector = query_vector.astype(np.float32).reshape(1, -1)
                        query_norm = np.linalg.norm(query_vector)
                        if query_norm > 0:
                            query_vector = query_vector / query_norm
                        
                        similarities = np.dot(self.vectors, query_vector.T).flatten()
                        indices = np.argsort(-similarities)[:k]
                        distances = similarities[indices]
                        
                        return distances, indices
                
                VECTOR_INDEX = SimpleIndex(vectors)
                logger.info(f"‚úÖ Loaded fallback index with {VECTOR_INDEX.ntotal} vectors")
                return VECTOR_INDEX
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load fallback vectors: {e}")
        
        logger.warning("‚ö†Ô∏è No vector index available")
        return None


# ---------- Search Functions ----------

def semantic_search(query: str, top_k: int = TOP_K, 
                   context_tour_index: Optional[int] = None) -> List[Tuple[float, Dict]]:
    """T√¨m ki·∫øm ng·ªØ nghƒ©a v·ªõi embeddings"""
    
    # L·∫•y embedding cho query
    query_embedding = get_text_embedding(query)
    
    # T·∫£i index
    index = load_vector_index()
    if index is None:
        return []
    
    # Th·ª±c hi·ªán t√¨m ki·∫øm
    try:
        distances, indices = index.search(query_embedding.reshape(1, -1), top_k * 2)
    except Exception as e:
        logger.error(f"‚ùå Vector search error: {e}")
        return []
    
    results = []
    for dist, idx in zip(distances[0], indices[0]):
        if idx < 0 or idx >= len(MAPPING):
            continue
        
        passage = MAPPING[idx]
        
        # T√≠nh ƒëi·ªÉm ng·ªØ c·∫£nh
        score = float(dist)
        
        # ∆Øu ti√™n tour context
        if context_tour_index is not None:
            passage_tour_index = passage.get("tour_index")
            if passage_tour_index == context_tour_index:
                score *= 1.5  # TƒÉng ƒëi·ªÉm cho tour hi·ªán t·∫°i
            elif passage_tour_index is not None:
                score *= 0.7  # Gi·∫£m ƒëi·ªÉm cho tour kh√°c
        
        results.append((score, passage))
    
    # S·∫Øp x·∫øp
    results.sort(key=lambda x: x[0], reverse=True)
    return results[:top_k]


def keyword_search(query: str, top_k: int = 5) -> List[Tuple[float, Dict]]:
    """T√¨m ki·∫øm theo t·ª´ kh√≥a ƒë∆°n gi·∫£n"""
    query_keywords = extract_keywords(query)
    
    if not query_keywords:
        return []
    
    results = []
    
    for passage in MAPPING:
        passage_text = passage.get("text", "")
        passage_keywords = extract_keywords(passage_text)
        
        # T√≠nh ƒëi·ªÉm Jaccard similarity
        if query_keywords and passage_keywords:
            common = set(query_keywords) & set(passage_keywords)
            if common:
                score = len(common) / len(query_keywords)
                
                # Th√™m bonus cho exact match
                if HAS_RAPIDFUZZ:
                    fuzz_score = fuzz.partial_ratio(query, passage_text) / 100.0
                    score = score * 0.7 + fuzz_score * 0.3
                
                results.append((score, passage))
    
    results.sort(key=lambda x: x[0], reverse=True)
    return results[:top_k]


def hybrid_search(query: str, context_tour_index: Optional[int] = None) -> List[Tuple[float, Dict]]:
    """
    T√¨m ki·∫øm lai: semantic + keyword + field-specific
    """
    all_results = []
    
    # 1. Semantic search
    semantic_results = semantic_search(query, TOP_K, context_tour_index)
    all_results.extend(semantic_results)
    
    # 2. TF-IDF search (n·∫øu c√≥)
    if TFIDF_INDEX and USE_TFIDF_FALLBACK:
        tfidf_results = TFIDF_INDEX.search(query, TOP_K)
        # Gi·∫£m ƒëi·ªÉm TF-IDF ƒë·ªÉ ∆∞u ti√™n embeddings
        tfidf_results = [(score * 0.6, passage) for score, passage in tfidf_results]
        all_results.extend(tfidf_results)
    
    # 3. Field-specific search
    field, confidence = detect_field_from_query(query)
    if field and confidence > 0.4:
        # T√¨m passages v·ªõi field c·ª• th·ªÉ
        field_passages = []
        for passage in MAPPING:
            if passage.get("field") == field:
                score = 1.0
                
                # ∆Øu ti√™n tour context
                if context_tour_index is not None:
                    if passage.get("tour_index") == context_tour_index:
                        score = 2.0
                    elif passage.get("tour_index") is not None:
                        score = 0.5
                
                field_passages.append((score, passage))
        
        # S·∫Øp x·∫øp v√† gi·ªõi h·∫°n
        field_passages.sort(key=lambda x: x[0], reverse=True)
        all_results.extend(field_passages[:TOP_K//2])
    
    # 4. Keyword search (fallback)
    if len(all_results) < 3:
        keyword_results = keyword_search(query, TOP_K)
        all_results.extend(keyword_results)
    
    # 5. Context-aware: th√™m th√¥ng tin t·ª´ tour hi·ªán t·∫°i
    if context_tour_index is not None:
        context_results = []
        for passage in MAPPING:
            if passage.get("tour_index") == context_tour_index:
                # T√≠nh similarity v·ªõi query
                passage_text = passage.get("text", "")
                similarity = calculate_similarity(query, passage_text)
                
                if similarity > 0.2:
                    score = 1.5 + similarity
                    context_results.append((score, passage))
        
        all_results.extend(context_results)
    
    # Lo·∫°i b·ªè tr√πng l·∫∑p
    unique_results = {}
    for score, passage in all_results:
        passage_id = passage.get("path", "") + ":" + passage.get("text", "")[:30]
        if passage_id not in unique_results or score > unique_results[passage_id][0]:
            unique_results[passage_id] = (score, passage)
    
    # S·∫Øp x·∫øp v√† tr·∫£ v·ªÅ
    final_results = list(unique_results.values())
    final_results.sort(key=lambda x: x[0], reverse=True)
    
    return final_results[:TOP_K_CONTEXT]


# ---------- Session Management ----------

def create_session_id() -> str:
    """T·∫°o session ID m·ªõi"""
    return str(uuid.uuid4())


def get_session(session_id: Optional[str] = None) -> Tuple[str, Dict]:
    """L·∫•y ho·∫∑c t·∫°o session"""
    
    # L·∫•y session_id t·ª´ cookie n·∫øu kh√¥ng c√≥
    if not session_id and request:
        session_id = request.cookies.get("session_id")
    
    # Redis session
    if REDIS_CLIENT and session_id:
        try:
            key = f"session:{session_id}"
            data_json = REDIS_CLIENT.get(key)
            if data_json:
                data = json.loads(data_json)
                
                # C·∫≠p nh·∫≠t th·ªùi gian
                data["last_activity"] = datetime.now().isoformat()
                REDIS_CLIENT.setex(key, SESSION_TIMEOUT, json.dumps(data))
                
                return session_id, data
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis session error: {e}")
    
    # Memory session ho·∫∑c t·∫°o m·ªõi
    if not session_id or session_id not in USER_SESSIONS:
        session_id = create_session_id()
        USER_SESSIONS[session_id] = {
            "created_at": datetime.now().isoformat(),
            "last_activity": datetime.now().isoformat(),
            "context_tour_index": None,
            "context_tour_name": None,
            "query_count": 0,
            "conversation": []
        }
    
    # C·∫≠p nh·∫≠t th·ªùi gian
    data = USER_SESSIONS[session_id]
    data["last_activity"] = datetime.now().isoformat()
    data["query_count"] = data.get("query_count", 0) + 1
    
    return session_id, data


def save_session(session_id: str, data: Dict):
    """L∆∞u session"""
    
    # Redis
    if REDIS_CLIENT:
        try:
            key = f"session:{session_id}"
            REDIS_CLIENT.setex(key, SESSION_TIMEOUT, json.dumps(data))
            return
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis save error: {e}")
    
    # Memory
    USER_SESSIONS[session_id] = data


def update_session_context(session_data: Dict, query: str, 
                          tour_index: Optional[int] = None, 
                          tour_name: Optional[str] = None):
    """C·∫≠p nh·∫≠t ng·ªØ c·∫£nh session"""
    
    # C·∫≠p nh·∫≠t tour context
    if tour_index is not None:
        session_data["context_tour_index"] = tour_index
        session_data["context_tour_name"] = tour_name
        session_data["query_count"] = 1  # Reset khi chuy·ªÉn tour
    else:
        # TƒÉng query count
        session_data["query_count"] = session_data.get("query_count", 0) + 1
        
        # N·∫øu ƒë√£ h·ªèi nhi·ªÅu m√† kh√¥ng nh·∫Øc ƒë·∫øn tour, clear context
        if session_data["query_count"] > CONTEXT_MEMORY:
            session_data["context_tour_index"] = None
            session_data["context_tour_name"] = None
    
    # L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i
    conversation = session_data.get("conversation", [])
    conversation.append({
        "query": query,
        "time": datetime.now().isoformat(),
        "tour_index": tour_index,
        "tour_name": tour_name
    })
    
    # Gi·ªØ ch·ªâ 10 m·ª•c g·∫ßn nh·∫•t
    if len(conversation) > 10:
        conversation = conversation[-10:]
    
    session_data["conversation"] = conversation


# ---------- Response Generation ----------

def generate_deterministic_response(query: str, 
                                   search_results: List[Tuple[float, Dict]], 
                                   context_tour_index: Optional[int] = None) -> str:
    """T·∫°o ph·∫£n h·ªìi x√°c ƒë·ªãnh t·ª´ search results"""
    
    if not search_results:
        if context_tour_index:
            tour_name = TOUR_INDEX_TO_INFO.get(context_tour_index, {}).get("name", "tour n√†y")
            return f"Hi·ªán t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ v·ªÅ '{tour_name}' trong c∆° s·ªü d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ c√°c tr∆∞·ªùng kh√°c nh∆∞ gi√°, th·ªùi gian, ƒë·ªãa ƒëi·ªÉm, ho·∫∑c li√™n h·ªá hotline 0332510486 ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n tr·ª±c ti·∫øp."
        else:
            return "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ h·ªèi c·ª• th·ªÉ h∆°n (v√≠ d·ª•: t√™n tour, gi√° tour, th·ªùi gian, ƒë·ªãa ƒëi·ªÉm) ho·∫∑c li√™n h·ªá Ruby Wings qua hotline 0332510486."
    
    # Nh√≥m k·∫øt qu·∫£ theo tour
    results_by_tour = defaultdict(list)
    general_results = []
    
    for score, passage in search_results:
        tour_index = passage.get("tour_index")
        if tour_index is not None:
            results_by_tour[tour_index].append((score, passage))
        else:
            general_results.append((score, passage))
    
    # X√¢y d·ª±ng ph·∫£n h·ªìi
    response_parts = []
    
    # ∆Øu ti√™n tour trong context
    if context_tour_index and context_tour_index in results_by_tour:
        tour_name = TOUR_INDEX_TO_INFO.get(context_tour_index, {}).get("name", f"Tour #{context_tour_index}")
        response_parts.append(f"**V·ªÅ tour '{tour_name}':**")
        
        # L·∫•y c√°c k·∫øt qu·∫£ c√≥ ƒëi·ªÉm cao nh·∫•t
        top_results = sorted(results_by_tour[context_tour_index], key=lambda x: x[0], reverse=True)[:3]
        
        for score, passage in top_results:
            text = passage.get("text", "")
            if text:
                response_parts.append(f"‚Ä¢ {text}")
        
        # X√≥a ƒë·ªÉ kh√¥ng hi·ªÉn th·ªã l·∫°i
        del results_by_tour[context_tour_index]
    
    # C√°c tour kh√°c
    for tour_index, tour_results in results_by_tour.items():
        if tour_results:
            tour_name = TOUR_INDEX_TO_INFO.get(tour_index, {}).get("name", f"Tour #{tour_index}")
            response_parts.append(f"\n**Tour '{tour_name}':**")
            
            top_results = sorted(tour_results, key=lambda x: x[0], reverse=True)[:2]
            for score, passage in top_results:
                text = passage.get("text", "")
                if text:
                    response_parts.append(f"‚Ä¢ {text}")
    
    # Th√¥ng tin chung
    if general_results and len(response_parts) < 3:
        response_parts.append("\n**Th√¥ng tin chung:**")
        top_general = sorted(general_results, key=lambda x: x[0], reverse=True)[:3]
        for score, passage in top_general:
            text = passage.get("text", "")
            if text:
                response_parts.append(f"‚Ä¢ {text}")
    
    # N·∫øu c√≥ nhi·ªÅu tour, ƒë·ªÅ xu·∫•t ch·ªçn tour c·ª• th·ªÉ
    if len(results_by_tour) > 1 and not context_tour_index:
        response_parts.append(f"\nüí° T√¥i t√¨m th·∫•y th√¥ng tin trong {len(results_by_tour)} tour. Vui l√≤ng h·ªèi c·ª• th·ªÉ v·ªÅ m·ªôt tour ƒë·ªÉ nh·∫≠n th√¥ng tin chi ti·∫øt h∆°n.")
    
    response = "\n".join(response_parts)
    
    # Th√™m th√¥ng tin li√™n h·ªá n·∫øu c·∫ßn
    if "hotline" not in response.lower() and "li√™n h·ªá" not in response.lower():
        response += "\n\nüìû ƒê·ªÉ bi·∫øt th√™m chi ti·∫øt ho·∫∑c ƒë·∫∑t tour, vui l√≤ng li√™n h·ªá Ruby Wings: **0332510486**"
    
    return response


def generate_llm_response(query: str, 
                          search_results: List[Tuple[float, Dict]], 
                          context_tour_index: Optional[int] = None) -> str:
    """T·∫°o ph·∫£n h·ªìi s·ª≠ d·ª•ng LLM (n·∫øu c√≥)"""
    
    # Ki·ªÉm tra OpenAI availability
    if not HAS_OPENAI or not OPENAI_API_KEY:
        return generate_deterministic_response(query, search_results, context_tour_index)
    
    # Chu·∫©n b·ªã context
    context_parts = []
    
    # Th√™m th√¥ng tin tour context
    if context_tour_index:
        tour_info = TOUR_INDEX_TO_INFO.get(context_tour_index)
        if tour_info:
            context_parts.append(f"NG·ªÆ C·∫¢NH: Ng∆∞·ªùi d√πng ƒëang h·ªèi v·ªÅ tour '{tour_info['name']}'")
            context_parts.append("H√£y ∆∞u ti√™n th√¥ng tin t·ª´ tour n√†y trong c√¢u tr·∫£ l·ªùi.\n")
    
    # Th√™m search results
    context_parts.append("TH√îNG TIN T·ª™ C∆† S·ªû D·ªÆ LI·ªÜU RUBY WINGS:")
    
    added_passages = set()
    for i, (score, passage) in enumerate(search_results[:6], 1):
        passage_text = passage.get("text", "")
        if not passage_text:
            continue
        
        passage_id = hash(passage_text[:100])
        if passage_id in added_passages:
            continue
        
        added_passages.add(passage_id)
        
        # Th√™m metadata
        tour_marker = ""
        tour_index = passage.get("tour_index")
        if tour_index is not None:
            tour_name = TOUR_INDEX_TO_INFO.get(tour_index, {}).get("name", f"Tour #{tour_index}")
            tour_marker = f" [Tour: {tour_name}]"
        
        field_marker = f"[{passage.get('field', 'unknown')}]"
        
        context_parts.append(f"\n{i}. {field_marker}{tour_marker}:")
        context_parts.append(passage_text)
    
    context = "\n".join(context_parts)
    
    # System prompt
    system_prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI c·ªßa Ruby Wings Travel - c√¥ng ty chuy√™n t·ªï ch·ª©c c√°c tour du l·ªãch tr·∫£i nghi·ªám, retreat, thi·ªÅn v√† h√†nh tr√¨nh ch·ªØa l√†nh.

{context}

QUY T·∫ÆC TR·∫¢ L·ªúI:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu tr√™n
2. KH√îNG t·∫°o ra th√¥ng tin kh√¥ng c√≥ trong d·ªØ li·ªáu
3. N·∫øu kh√¥ng c√≥ th√¥ng tin, n√≥i r√µ "T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin v·ªÅ..."
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n, th√¢n thi·ªán
5. Gi·ªØ c√¢u tr·∫£ l·ªùi t·∫≠p trung, kh√¥ng lan man
6. N·∫øu c√≥ th·ªÉ, ƒë·ªÅ xu·∫•t h·ªèi th√™m v·ªÅ c√°c tr∆∞·ªùng th√¥ng tin kh√°c

C√¢u h·ªèi: {query}

Tr·∫£ l·ªùi:"""
    
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model=CHAT_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            temperature=0.3,
            max_tokens=800,
            top_p=0.9
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"‚ùå LLM generation error: {e}")
        return generate_deterministic_response(query, search_results, context_tour_index)


# ---------- API Routes ----------

@app.route('/api/chat', methods=['POST'])
def chat_handler():
    """X·ª≠ l√Ω chat request"""
    
    # L·∫•y session
    session_id, session_data = get_session()
    
    # Parse request
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                'error': 'Invalid request format',
                'reply': 'Vui l√≤ng g·ª≠i y√™u c·∫ßu d∆∞·ªõi d·∫°ng JSON.'
            }), 400
        
        query = data.get('message', '').strip()
        if not query:
            return jsonify({
                'reply': 'Vui l√≤ng nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n.',
                'session_id': session_id
            })
            
    except Exception as e:
        logger.error(f"‚ùå Request parsing error: {e}")
        return jsonify({
            'reply': 'ƒê·ªãnh d·∫°ng request kh√¥ng h·ª£p l·ªá.',
            'session_id': session_id
        }), 400
    
    # 1. Ph√°t hi·ªán tour t·ª´ query
    tour_match = extract_tour_from_query(query)
    current_tour_index = None
    current_tour_name = None
    
    if tour_match:
        current_tour_name, current_tour_index, confidence = tour_match
        logger.info(f"üîç Detected tour: {current_tour_name} (index={current_tour_index}, confidence={confidence:.2f})")
    else:
        # S·ª≠ d·ª•ng tour t·ª´ context n·∫øu c√≥
        current_tour_index = session_data.get('context_tour_index')
        current_tour_name = session_data.get('context_tour_name')
        if current_tour_index:
            logger.info(f"üîç Using context tour: {current_tour_name} (index={current_tour_index})")
    
    # 2. T√¨m ki·∫øm th√¥ng tin
    search_results = hybrid_search(query, current_tour_index)
    logger.info(f"üîç Found {len(search_results)} relevant passages")
    
    # 3. T·∫°o ph·∫£n h·ªìi
    try:
        # S·ª≠ d·ª•ng LLM n·∫øu c√≥ OpenAI, n·∫øu kh√¥ng d√πng deterministic
        if HAS_OPENAI and OPENAI_API_KEY:
            reply = generate_llm_response(query, search_results, current_tour_index)
        else:
            reply = generate_deterministic_response(query, search_results, current_tour_index)
    except Exception as e:
        logger.error(f"‚ùå Response generation error: {e}")
        reply = generate_deterministic_response(query, search_results, current_tour_index)
    
    # 4. C·∫≠p nh·∫≠t session context
    update_session_context(session_data, query, current_tour_index, current_tour_name)
    save_session(session_id, session_data)
    
    # 5. Chu·∫©n b·ªã response
    response_data = {
        'reply': reply,
        'session_id': session_id,
        'context_tour': current_tour_name,
        'has_context': current_tour_index is not None,
        'sources_count': len(search_results)
    }
    
    # Th√™m debug info n·∫øu enabled
    if DEBUG:
        response_data['debug'] = {
            'detected_tour_index': current_tour_index,
            'detected_tour_name': current_tour_name,
            'search_results_count': len(search_results),
            'session_query_count': session_data.get('query_count', 0)
        }
    
    # Set cookie
    response = jsonify(response_data)
    response.set_cookie(
        'session_id',
        session_id,
        max_age=SESSION_TIMEOUT,
        httponly=True,
        samesite='Lax'
    )
    
    return response


@app.route('/api/tours', methods=['GET'])
def list_tours():
    """API li·ªát k√™ t·∫•t c·∫£ tours"""
    tours = []
    
    for tour_index, tour_info in TOUR_INDEX_TO_INFO.items():
        tours.append({
            'id': tour_index,
            'name': tour_info['name'],
            'fields': list(tour_info.get('fields', [])),
            'has_info': True
        })
    
    return jsonify({
        'tours': sorted(tours, key=lambda x: x['id']),
        'count': len(tours)
    })


@app.route('/api/context', methods=['GET'])
def get_context():
    """API l·∫•y ng·ªØ c·∫£nh hi·ªán t·∫°i"""
    session_id = request.cookies.get('session_id')
    if not session_id:
        return jsonify({'context': None})
    
    _, session_data = get_session(session_id)
    
    return jsonify({
        'context_tour': session_data.get('context_tour_name'),
        'context_tour_index': session_data.get('context_tour_index'),
        'query_count': session_data.get('query_count', 0),
        'conversation_length': len(session_data.get('conversation', []))
    })


@app.route('/api/reset', methods=['POST'])
def reset_context():
    """API reset ng·ªØ c·∫£nh"""
    session_id = request.cookies.get('session_id')
    if not session_id:
        return jsonify({'success': False, 'message': 'No session'})
    
    _, session_data = get_session(session_id)
    
    # Reset context
    session_data['context_tour_index'] = None
    session_data['context_tour_name'] = None
    session_data['query_count'] = 0
    session_data['conversation'] = []
    
    save_session(session_id, session_data)
    
    return jsonify({'success': True, 'message': 'Context reset'})


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    health_status = {
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'ruby_wings_chatbot',
        'version': '2.0.0',
        'components': {
            'knowledge_data': len(KNOWLEDGE_DATA) > 0,
            'mapping': len(MAPPING) > 0,
            'tours_indexed': len(TOUR_NAME_TO_INDEX),
            'vector_index': VECTOR_INDEX is not None,
            'tfidf_index': TFIDF_INDEX is not None if TFIDF_INDEX else False,
            'openai': HAS_OPENAI and bool(OPENAI_API_KEY),
            'redis': REDIS_CLIENT is not None,
            'rapidfuzz': HAS_RAPIDFUZZ,
            'sklearn': HAS_SKLEARN,
            'nltk': HAS_NLTK
        },
        'counts': {
            'tours': len(TOUR_NAME_TO_INDEX),
            'passages': len(MAPPING),
            'sessions': len(USER_SESSIONS)
        }
    }
    
    return jsonify(health_status)


@app.route('/api/reindex', methods=['POST'])
def reindex_endpoint():
    """Reindex endpoint (admin only)"""
    # Simple auth check
    auth_key = request.headers.get('X-Admin-Key')
    if auth_key != os.environ.get('ADMIN_KEY', 'default_admin_key'):
        return jsonify({'success': False, 'message': 'Unauthorized'}), 401
    
    try:
        # Reload data
        load_knowledge_data()
        load_mapping_data()
        load_field_keywords()
        load_tour_indices()
        
        # Clear indexes
        global VECTOR_INDEX, TFIDF_INDEX
        VECTOR_INDEX = None
        TFIDF_INDEX = None
        
        # Reload vector index
        load_vector_index()
        
        # Rebuild TF-IDF index
        if HAS_SKLEARN and MAPPING and USE_TFIDF_FALLBACK:
            TFIDF_INDEX = TFIDFIndex()
            TFIDF_INDEX.build(MAPPING)
        
        return jsonify({
            'success': True,
            'message': 'Reindex completed',
            'tours': len(TOUR_NAME_TO_INDEX),
            'passages': len(MAPPING)
        })
        
    except Exception as e:
        logger.error(f"‚ùå Reindex error: {e}")
        return jsonify({
            'success': False,
            'message': f'Reindex failed: {str(e)}'
        }), 500


@app.route('/', methods=['GET'])
def home():
    """Home page"""
    return jsonify({
        'service': 'Ruby Wings Chatbot API',
        'version': '2.0.0',
        'endpoints': {
            'POST /api/chat': 'Chat with the bot',
            'GET /api/tours': 'List all tours',
            'GET /api/context': 'Get current context',
            'POST /api/reset': 'Reset context',
            'GET /api/health': 'Health check',
            'POST /api/reindex': 'Reindex data (admin)'
        },
        'status': 'operational'
    })


# ---------- Data Loading ----------

def load_knowledge_data():
    """T·∫£i d·ªØ li·ªáu knowledge t·ª´ file"""
    global KNOWLEDGE_DATA
    
    if not os.path.exists(KNOWLEDGE_PATH):
        logger.error(f"‚ùå Knowledge file not found: {KNOWLEDGE_PATH}")
        KNOWLEDGE_DATA = {}
        return
    
    try:
        with open(KNOWLEDGE_PATH, 'r', encoding='utf-8') as f:
            KNOWLEDGE_DATA = json.load(f)
        
        # Validate structure
        if 'tours' not in KNOWLEDGE_DATA:
            logger.warning("‚ö†Ô∏è Knowledge data missing 'tours' key")
        
        logger.info(f"‚úÖ Loaded knowledge data with {len(KNOWLEDGE_DATA.get('tours', []))} tours")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load knowledge data: {e}")
        KNOWLEDGE_DATA = {}


def load_mapping_data():
    """T·∫£i mapping data t·ª´ file"""
    global MAPPING, METADATA
    
    if not os.path.exists(FAISS_MAPPING_PATH):
        logger.warning(f"‚ö†Ô∏è Mapping file not found: {FAISS_MAPPING_PATH}")
        MAPPING = []
        METADATA = []
        return
    
    try:
        with open(FAISS_MAPPING_PATH, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        # Ki·ªÉm tra c·∫•u tr√∫c
        if isinstance(mapping_data, dict) and 'mapping' in mapping_data:
            MAPPING = mapping_data['mapping']
            METADATA = mapping_data.get('metadata', [])
        else:
            MAPPING = mapping_data
            METADATA = []
        
        # ƒê·∫£m b·∫£o m·ªói passage c√≥ c√°c tr∆∞·ªùng c·∫ßn thi·∫øt
        for i, passage in enumerate(MAPPING):
            if 'text' not in passage:
                passage['text'] = ''
            if 'field' not in passage:
                passage['field'] = 'unknown'
            if 'tour_index' not in passage:
                passage['tour_index'] = None
            if 'tour_name' not in passage:
                passage['tour_name'] = None
        
        logger.info(f"‚úÖ Loaded {len(MAPPING)} mapping entries")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load mapping data: {e}")
        MAPPING = []
        METADATA = []


def initialize_app():
    """Kh·ªüi t·∫°o ·ª©ng d·ª•ng"""
    logger.info("=" * 60)
    logger.info("üöÄ Initializing Ruby Wings Chatbot v2.0")
    logger.info("=" * 60)
    
    # T·∫£i d·ªØ li·ªáu
    load_knowledge_data()
    load_mapping_data()
    load_field_keywords()
    load_tour_indices()
    
    # T·∫£i vector index
    load_vector_index()
    
    # X√¢y d·ª±ng TF-IDF index (n·∫øu ƒë∆∞·ª£c enable)
    global TFIDF_INDEX
    if HAS_SKLEARN and MAPPING and USE_TFIDF_FALLBACK:
        TFIDF_INDEX = TFIDFIndex()
        if not TFIDF_INDEX.build(MAPPING):
            logger.warning("‚ö†Ô∏è TF-IDF index build failed")
            TFIDF_INDEX = None
    else:
        TFIDF_INDEX = None
    
    # Log system status
    logger.info("üìä System Status:")
    logger.info(f"  ‚Ä¢ Knowledge: {len(KNOWLEDGE_DATA.get('tours', []))} tours")
    logger.info(f"  ‚Ä¢ Mapping: {len(MAPPING)} passages")
    logger.info(f"  ‚Ä¢ Tours indexed: {len(TOUR_NAME_TO_INDEX)}")
    logger.info(f"  ‚Ä¢ Vector index: {'Loaded' if VECTOR_INDEX else 'Not available'}")
    logger.info(f"  ‚Ä¢ TF-IDF index: {'Built' if TFIDF_INDEX else 'Not available'}")
    logger.info(f"  ‚Ä¢ OpenAI: {'Available' if HAS_OPENAI and OPENAI_API_KEY else 'Not available'}")
    logger.info(f"  ‚Ä¢ Redis: {'Available' if REDIS_CLIENT else 'Not available'}")
    logger.info(f"  ‚Ä¢ RapidFuzz: {'Available' if HAS_RAPIDFUZZ else 'Not available'}")
    logger.info(f"  ‚Ä¢ Scikit-learn: {'Available' if HAS_SKLEARN else 'Not available'}")
    logger.info(f"  ‚Ä¢ NLTK: {'Available' if HAS_NLTK else 'Not available'}")
    logger.info("=" * 60)
    logger.info("üéâ Ruby Wings Chatbot initialized successfully!")
    logger.info("=" * 60)


# ---------- Cleanup ----------

@app.teardown_appcontext
def cleanup_session_store(exception=None):
    """D·ªçn d·∫πp session store khi ·ª©ng d·ª•ng shutdown"""
    # ƒê·ªëi v·ªõi memory store, c√≥ th·ªÉ clear sessions c≈©
    current_time = datetime.now()
    expired_sessions = []
    
    for session_id, session_data in USER_SESSIONS.items():
        last_activity = session_data.get('last_activity')
        if last_activity:
            try:
                last_time = datetime.fromisoformat(last_activity)
                if (current_time - last_time).total_seconds() > SESSION_TIMEOUT:
                    expired_sessions.append(session_id)
            except (ValueError, TypeError):
                expired_sessions.append(session_id)
    
    for session_id in expired_sessions:
        USER_SESSIONS.pop(session_id, None)


# ---------- Main ----------

if __name__ == '__main__':
    # Kh·ªüi t·∫°o ·ª©ng d·ª•ng
    initialize_app()
    
    # Ch·∫°y server
    port = int(os.environ.get('PORT', 10000))
    logger.info(f"üåê Starting server on port {port}")
    
    # Ch·∫°y v·ªõi Flask dev server (ch·ªâ cho development)
    app.run(host='0.0.0.0', port=port, debug=DEBUG, threaded=True)
else:
    # Kh·ªüi t·∫°o khi ch·∫°y v·ªõi WSGI server (gunicorn)
    initialize_app()