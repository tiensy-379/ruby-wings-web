# app.py ‚Äî RUBY WINGS CHATBOT v2.1.1
# Fixed critical UnboundLocalError in chat endpoint
# Enhanced with robust error handling and context-aware tour detection

# === SAFE MODE FOR DEBUG ===
FLAT_TEXTS = []
INDEX = None
HAS_FAISS = False
FAISS_ENABLED = False

def _index_dim(idx):
    return None

# === IMPORTS ===
import os
import json
import threading
import logging
import re
import unicodedata
import traceback
import hashlib
from functools import lru_cache
from typing import List, Tuple, Dict, Optional, Any
from datetime import datetime

from flask import Flask, request, jsonify
from flask_cors import CORS

import numpy as np
import gspread
from google.oauth2.service_account import Credentials
from google.auth.exceptions import GoogleAuthError
from gspread.exceptions import APIError, SpreadsheetNotFound, WorksheetNotFound

# Meta CAPI
from meta_capi import send_meta_pageview
from meta_capi import send_meta_lead

# Try FAISS
HAS_FAISS = False
try:
    import faiss
    HAS_FAISS = True
except ImportError:
    HAS_FAISS = False

# OpenAI API
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# =========== CONFIGURATION ===========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("rbw")

# Environment variables with defaults
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
GOOGLE_SERVICE_ACCOUNT_JSON = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON", "").strip()

# Embedding and model config
KNOWLEDGE_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.environ.get("CHAT_MODEL", "gpt-4o-mini")
TOP_K = int(os.environ.get("TOP_K", "5"))
FAISS_ENABLED = os.environ.get("FAISS_ENABLED", "true").lower() in ("1", "true", "yes")

# Google Sheets config
GOOGLE_SHEET_ID = "1SdVbwkuxb8l1meEW--ddyfh4WmUvSXXMOPQ5bCyPkdk"
GOOGLE_SHEET_NAME = os.environ.get("GOOGLE_SHEET_NAME", "RBW_Lead_Raw_Inbox")

# Feature flags
ENABLE_GOOGLE_SHEETS = os.environ.get("ENABLE_GOOGLE_SHEETS", "true").lower() in ("1", "true", "yes")
ENABLE_FALLBACK_STORAGE = os.environ.get("ENABLE_FALLBACK_STORAGE", "true").lower() in ("1", "true", "yes")
FALLBACK_STORAGE_PATH = os.environ.get("FALLBACK_STORAGE_PATH", "leads_fallback.json")

# =========== GLOBAL STATE ===========
app = Flask(__name__)
CORS(app)

# Initialize OpenAI client
client = None
if OPENAI_API_KEY and OpenAI is not None:
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        logger.info("OpenAI client initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client: {e}")
        client = None
else:
    logger.warning("OPENAI_API_KEY not set ‚Äî embeddings/chat will use fallback behavior")

# Knowledge base state
KNOW: Dict = {}
FLAT_TEXTS: List[str] = []
MAPPING: List[dict] = []
INDEX = None
INDEX_LOCK = threading.Lock()
TOUR_NAME_TO_INDEX: Dict[str, int] = {}

# Google Sheets client cache
_gsheet_client = None
_gsheet_client_lock = threading.Lock()

# Fallback storage for leads
_fallback_storage_lock = threading.Lock()

# =========== CONTEXT MANAGEMENT ===========
SESSION_CONTEXT = {}
CONTEXT_TIMEOUT = 1800  # 30 ph√∫t

def cleanup_old_contexts():
    """D·ªçn d·∫πp context c≈©"""
    now = datetime.utcnow()
    to_delete = []
    for session_id, context in SESSION_CONTEXT.items():
        if (now - context.get("timestamp", now)).total_seconds() > CONTEXT_TIMEOUT:
            to_delete.append(session_id)
    for session_id in to_delete:
        del SESSION_CONTEXT[session_id]

def get_session_context(session_id):
    """L·∫•y context cho session"""
    cleanup_old_contexts()
    if session_id not in SESSION_CONTEXT:
        SESSION_CONTEXT[session_id] = {
            "last_tour_indices": [],
            "conversation_history": [],
            "last_tour_name": None,
            "timestamp": datetime.utcnow()
        }
    return SESSION_CONTEXT[session_id]

def update_tour_context(session_id, tour_indices, tour_name=None):
    """C·∫≠p nh·∫≠t context tour"""
    context = get_session_context(session_id)
    if tour_indices:
        context["last_tour_indices"] = tour_indices
    if tour_name:
        context["last_tour_name"] = tour_name
    context["timestamp"] = datetime.utcnow()
    return context

def extract_session_id(request_data, remote_addr):
    """Tr√≠ch xu·∫•t session_id t·ª´ request"""
    # ∆Øu ti√™n session_id t·ª´ frontend
    session_id = request_data.get("session_id")
    
    if not session_id:
        # T·∫°o session_id ·ªïn ƒë·ªãnh t·ª´ IP + timestamp (gi·ªØ trong 30 ph√∫t)
        ip = remote_addr or "0.0.0.0"
        current_hour = datetime.utcnow().strftime("%Y%m%d%H")
        
        # T·∫°o session_id ·ªïn ƒë·ªãnh trong 1 gi·ªù
        unique_str = f"{ip}_{current_hour}"
        session_id = hashlib.md5(unique_str.encode()).hexdigest()[:12]
    
    return f"session_{session_id}"

def get_complete_tour_info(tour_indices):
    """L·∫•y th√¥ng tin ƒë·∫ßy ƒë·ªß c·ªßa tour t·ª´ c√°c indices"""
    result = {}
    
    for idx in tour_indices:
        tour_info = {}
        
        # L·∫•y t√™n tour
        for m in MAPPING:
            if f"[{idx}]" in m.get("path", "") and ".tour_name" in m.get("path", ""):
                tour_info["name"] = m.get("text", "")
                break
        
        # L·∫•y c√°c field kh√°c
        for field in TOUR_FIELDS:
            if field == "tour_name":
                continue
                
            passages = get_passages_by_field(field, limit=5, tour_indices=[idx])
            if passages:
                field_texts = [m.get("text", "") for _, m in passages]
                tour_info[field] = "\n".join(field_texts)
        
        result[idx] = tour_info
    
    return result

def get_suggested_questions(tour_indices, current_field):
    """G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo"""
    suggestions = []
    
    if not tour_indices:
        suggestions.extend([
            "B·∫°n mu·ªën h·ªèi v·ªÅ tour n√†o?",
            "C√≥ tour n√†o v·ªÅ Hu·∫ø kh√¥ng?",
            "Tour n√†o ph√π h·ª£p cho gia ƒë√¨nh?"
        ])
    else:
        common_fields = ["price", "includes", "accommodation", "meals", "duration"]
        current = current_field or ""
        
        for field in common_fields:
            if field != current:
                field_names = {
                    "price": "gi√° c·∫£",
                    "includes": "l·ªãch tr√¨nh",
                    "accommodation": "ch·ªó ·ªü", 
                    "meals": "ƒÉn u·ªëng",
                    "duration": "th·ªùi gian"
                }
                suggestions.append(f"Tour c√≥ {field_names.get(field, field)} nh∆∞ th·∫ø n√†o?")
    
    return suggestions[:3]  # Ch·ªâ 3 g·ª£i √Ω

# =========== KEYWORD MAPPING ===========
KEYWORD_FIELD_MAP: Dict[str, Dict] = {
    "tour_list": {
        "keywords": [
            "t√™n tour", "tour g√¨", "danh s√°ch tour", "c√≥ nh·ªØng tour n√†o", "li·ªát k√™ tour",
            "show tour", "tour hi·ªán c√≥", "tour available", "li·ªát k√™ c√°c tour ƒëang c√≥",
            "list tour", "tour ƒëang b√°n", "tour hi·ªán h√†nh", "tour n√†o", "tours", "li·ªát k√™ c√°c tour",
            "li·ªát k√™ c√°c h√†nh tr√¨nh", "list tours", "show tours", "c√°c tour hi·ªán t·∫°i"
        ],
        "field": "tour_name"
    },
    "mission": {"keywords": ["t·∫ßm nh√¨n", "s·ª© m·ªánh", "gi√° tr·ªã c·ªët l√µi", "tri·∫øt l√Ω", "vision", "mission"], "field": "mission"},
    "summary": {"keywords": ["t√≥m t·∫Øt ch∆∞∆°ng tr√¨nh tour", "t√≥m t·∫Øt", "overview", "brief", "m√¥ t·∫£ ng·∫Øn"], "field": "summary"},
    "style": {"keywords": ["phong c√°ch h√†nh tr√¨nh", "t√≠nh ch·∫•t h√†nh tr√¨nh", "concept tour", "vibe tour", "style"], "field": "style"},
    "transport": {"keywords": ["v·∫≠n chuy·ªÉn", "ph∆∞∆°ng ti·ªán", "di chuy·ªÉn", "xe g√¨", "transportation"], "field": "transport"},
    "includes": {"keywords": ["l·ªãch tr√¨nh chi ti·∫øt", "ch∆∞∆°ng tr√¨nh chi ti·∫øt", "chi ti·∫øt h√†nh tr√¨nh", "itinerary", "schedule", "includes"], "field": "includes"},
    "location": {"keywords": ["·ªü ƒë√¢u", "ƒëi ƒë√¢u", "ƒë·ªãa ph∆∞∆°ng n√†o", "n∆°i n√†o", "ƒëi·ªÉm ƒë·∫øn", "destination", "location"], "field": "location"},
    "duration": {"keywords": ["th·ªùi gian tour", "k√©o d√†i", "m·∫•y ng√†y", "bao l√¢u", "ng√†y ƒë√™m", "duration", "tour d√†i bao l√¢u", "tour bao nhi√™u ng√†y", "2 ng√†y 1 ƒë√™m", "3 ng√†y 2 ƒë√™m"], "field": "duration"},
    "price": {"keywords": ["gi√° tour", "chi ph√≠", "bao nhi√™u ti·ªÅn", "price", "cost"], "field": "price"},
    "notes": {"keywords": ["l∆∞u √Ω", "ghi ch√∫", "notes", "c·∫ßn ch√∫ √Ω"], "field": "notes"},
    "accommodation": {"keywords": ["ch·ªó ·ªü", "n∆°i l∆∞u tr√∫", "kh√°ch s·∫°n", "homestay", "accommodation"], "field": "accommodation"},
    "meals": {"keywords": ["ƒÉn u·ªëng", "·∫©m th·ª±c", "meals", "th·ª±c ƒë∆°n", "b·ªØa"], "field": "meals"},
    "event_support": {"keywords": ["h·ªó tr·ª£", "d·ªãch v·ª• h·ªó tr·ª£", "event support", "d·ªãch v·ª• tƒÉng c∆∞·ªùng"], "field": "event_support"},
    "cancellation_policy": {"keywords": ["ph√≠ hu·ª∑", "ch√≠nh s√°ch hu·ª∑", "cancellation", "refund policy"], "field": "cancellation_policy"},
    "booking_method": {"keywords": ["ƒë·∫∑t ch·ªó", "ƒë·∫∑t tour", "booking", "c√°ch ƒë·∫∑t"], "field": "booking_method"},
    "who_can_join": {"keywords": ["ph√π h·ª£p ƒë·ªëi t∆∞·ª£ng", "ai tham gia", "who should join"], "field": "who_can_join"},
    "hotline": {"keywords": ["hotline", "s·ªë ƒëi·ªán tho·∫°i", "li√™n h·ªá", "contact number"], "field": "hotline"},
}

# =========== TOUR FIELDS FOR COMPLETE INFO ===========
TOUR_FIELDS = [
    "tour_name", "summary", "location", "duration", "price",
    "includes", "notes", "style", "transport", "accommodation",
    "meals", "event_support", "cancellation_policy", 
    "booking_method", "who_can_join", "hotline"
]

# =========== UTILITY FUNCTIONS ===========
def normalize_text_simple(s: str) -> str:
    """Lowercase, remove diacritics, strip punctuation, collapse spaces."""
    if not s:
        return ""
    s = s.lower()
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def get_gspread_client(force_refresh: bool = False):
    """
    Get or create Google Sheets client with thread safety and error handling.
    Returns None if authentication fails.
    """
    global _gsheet_client
    
    if not GOOGLE_SERVICE_ACCOUNT_JSON:
        logger.error("GOOGLE_SERVICE_ACCOUNT_JSON environment variable not set")
        return None
    
    with _gsheet_client_lock:
        if _gsheet_client is not None and not force_refresh:
            return _gsheet_client
        
        try:
            # Parse service account JSON
            try:
                info = json.loads(GOOGLE_SERVICE_ACCOUNT_JSON)
            except json.JSONDecodeError as e:
                logger.error(f"Invalid GOOGLE_SERVICE_ACCOUNT_JSON: {e}")
                return None
            
            # Define scopes
            scopes = [
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive",
            ]
            
            # Create credentials
            creds = Credentials.from_service_account_info(info, scopes=scopes)
            _gsheet_client = gspread.authorize(creds)
            logger.info("Google Sheets client initialized successfully")
            return _gsheet_client
            
        except GoogleAuthError as e:
            logger.error(f"Google authentication error: {e}")
            _gsheet_client = None
            return None
        except Exception as e:
            logger.error(f"Failed to initialize Google Sheets client: {e}")
            _gsheet_client = None
            return None

def save_lead_to_fallback_storage(lead_data: dict) -> bool:
    """
    Save lead data to local JSON file as fallback when Google Sheets fails.
    """
    if not ENABLE_FALLBACK_STORAGE:
        return False
    
    try:
        lead_data["timestamp"] = datetime.utcnow().isoformat()
        lead_data["synced"] = False
        
        with _fallback_storage_lock:
            # Read existing data
            leads = []
            if os.path.exists(FALLBACK_STORAGE_PATH):
                try:
                    with open(FALLBACK_STORAGE_PATH, 'r', encoding='utf-8') as f:
                        leads = json.load(f)
                        if not isinstance(leads, list):
                            leads = []
                except (json.JSONDecodeError, IOError) as e:
                    logger.warning(f"Failed to read fallback storage: {e}")
                    leads = []
            
            # Append new lead
            leads.append(lead_data)
            
            # Write back
            with open(FALLBACK_STORAGE_PATH, 'w', encoding='utf-8') as f:
                json.dump(leads, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Lead saved to fallback storage: {FALLBACK_STORAGE_PATH}")
            return True
            
    except Exception as e:
        logger.error(f"Failed to save lead to fallback storage: {e}")
        return False

def index_tour_names():
    """Populate TOUR_NAME_TO_INDEX from MAPPING entries that end with .tour_name."""
    global TOUR_NAME_TO_INDEX
    TOUR_NAME_TO_INDEX = {}
    for m in MAPPING:
        path = m.get("path", "")
        if path.endswith(".tour_name"):
            txt = m.get("text", "") or ""
            norm = normalize_text_simple(txt)
            if not norm:
                continue
            match = re.search(r"\[(\d+)\]", path)
            if match:
                idx = int(match.group(1))
                prev = TOUR_NAME_TO_INDEX.get(norm)
                if prev is None:
                    TOUR_NAME_TO_INDEX[norm] = idx
                else:
                    if len(txt) > len(MAPPING[next(i for i,m2 in enumerate(MAPPING) if re.search(rf"\[{prev}\]", m2.get('path','')) )].get("text","")):
                        TOUR_NAME_TO_INDEX[norm] = idx

def find_tour_indices_from_message(message: str) -> List[int]:
    """Improved tour detection with fuzzy matching"""
    if not message:
        return []
    
    msg_n = normalize_text_simple(message)
    if not msg_n:
        return []
    
    matches = []
    for norm_name, idx in TOUR_NAME_TO_INDEX.items():
        tour_words = set(norm_name.split())
        msg_words = set(msg_n.split())
        common_words = tour_words & msg_words
        if len(common_words) >= 1:
            matches.append((len(common_words), norm_name))
    
    if matches:
        matches.sort(reverse=True)
        best_score = matches[0][0]
        selected = [TOUR_NAME_TO_INDEX[nm] for sc, nm in matches if sc == best_score]
        return sorted(set(selected))
    
    return []

# =========== MAPPING HELPERS ===========
def get_passages_by_field(field_name: str, limit: int = 50, tour_indices: Optional[List[int]] = None) -> List[Tuple[float, dict]]:
    """
    Return passages whose path ends with field_name.
    If tour_indices provided, RESTRICT and PRIORITIZE entries matching those tour index brackets.
    Returned score is 2.0 for exact tour match, 1.0 for global match.
    """
    exact_matches: List[Tuple[float, dict]] = []
    global_matches: List[Tuple[float, dict]] = []
    
    for m in MAPPING:
        path = m.get("path", "")
        if path.endswith(f".{field_name}") or f".{field_name}" in path:
            is_exact_match = False
            if tour_indices:
                for ti in tour_indices:
                    if f"[{ti}]" in path:
                        is_exact_match = True
                        break
            
            if is_exact_match:
                exact_matches.append((2.0, m))
            elif not tour_indices:
                global_matches.append((1.0, m))
    
    all_results = exact_matches + global_matches
    all_results.sort(key=lambda x: x[0], reverse=True)
    return all_results[:limit]

# =========== EMBEDDINGS ===========
@lru_cache(maxsize=8192)
def embed_text(text: str) -> Tuple[List[float], int]:
    """
    Return (embedding list, dim)
    Tries openai.Embedding.create. If API key missing or call fails, return deterministic fallback 1536-dim.
    """
    if not text:
        return [], 0
    short = text if len(text) <= 2000 else text[:2000]
    
    if client is not None:
        try:
            resp = client.embeddings.create(
                model=EMBEDDING_MODEL, 
                input=short
            )
            if resp.data and len(resp.data) > 0:
                emb = resp.data[0].embedding
                return emb, len(emb)
        except Exception:
            logger.exception("OpenAI embedding call failed ‚Äî falling back to deterministic embedding.")
    
    # Deterministic fallback
    try:
        h = abs(hash(short)) % (10 ** 12)
        fallback_dim = 1536
        vec = [(float((h >> (i % 32)) & 0xFF) + (i % 7)) / 255.0 for i in range(fallback_dim)]
        return vec, fallback_dim
    except Exception:
        logger.exception("Fallback embedding generation failed")
        return [], 0

# =========== INDEX MANAGEMENT ===========
def _index_dim(idx) -> Optional[int]:
    try:
        d = getattr(idx, "d", None)
        if isinstance(d, int) and d > 0:
            return d
    except Exception:
        pass
    try:
        d = getattr(idx, "dim", None)
        if isinstance(d, int) and d > 0:
            return d
    except Exception:
        pass
    try:
        if HAS_FAISS and isinstance(idx, faiss.Index):
            return int(idx.d)
    except Exception:
        pass
    return None

def choose_embedding_model_for_dim(dim: int) -> str:
    if dim == 1536:
        return "text-embedding-3-small"
    if dim == 3072:
        return "text-embedding-3-large"
    return os.environ.get("EMBEDDING_MODEL", EMBEDDING_MODEL)

class NumpyIndex:
    """Simple in-memory numpy index with cosine-similarity."""
    def __init__(self, mat: Optional[np.ndarray] = None):
        if mat is None or getattr(mat, "size", 0) == 0:
            self.mat = np.empty((0, 0), dtype="float32")
            self.dim = None
        else:
            self.mat = mat.astype("float32")
            self.dim = self.mat.shape[1]

    def add(self, mat: np.ndarray):
        if getattr(mat, "size", 0) == 0:
            return
        mat = mat.astype("float32")
        if getattr(self.mat, "size", 0) == 0:
            self.mat = mat.copy()
            self.dim = mat.shape[1]
        else:
            if mat.shape[1] != self.dim:
                raise ValueError("Dimension mismatch")
            self.mat = np.vstack([self.mat, mat])

    def search(self, qvec: np.ndarray, k: int):
        if self.mat is None or getattr(self.mat, "size", 0) == 0:
            return np.array([[]], dtype="float32"), np.array([[]], dtype="int64")
        q = qvec.astype("float32")
        q = q / (np.linalg.norm(q, axis=1, keepdims=True) + 1e-12)
        m = self.mat / (np.linalg.norm(self.mat, axis=1, keepdims=True) + 1e-12)
        sims = np.dot(q, m.T)
        idx = np.argsort(-sims, axis=1)[:, :k]
        scores = np.take_along_axis(sims, idx, axis=1)
        return scores.astype("float32"), idx.astype("int64")

    @property
    def ntotal(self):
        return 0 if getattr(self.mat, "size", 0) == 0 else self.mat.shape[0]

    def save(self, path):
        try:
            np.savez_compressed(path, mat=self.mat)
            logger.info(f"Saved numpy index to {path}")
        except Exception as e:
            logger.error(f"Failed to save numpy index: {e}")

    @classmethod
    def load(cls, path):
        try:
            arr = np.load(path)
            mat = arr["mat"]
            logger.info(f"Loaded numpy index from {path}")
            return cls(mat=mat)
        except Exception as e:
            logger.error(f"Failed to load numpy index: {e}")
            return cls(None)

def load_mapping_from_disk(path=FAISS_MAPPING_PATH):
    global MAPPING, FLAT_TEXTS
    try:
        with open(path, "r", encoding="utf-8") as f:
            MAPPING[:] = json.load(f)
        FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
        logger.info("Loaded mapping from %s (%d entries)", path, len(MAPPING))
        return True
    except Exception as e:
        logger.error(f"Failed to load mapping from disk: {e}")
        return False

def save_mapping_to_disk(path=FAISS_MAPPING_PATH):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(MAPPING, f, ensure_ascii=False, indent=2)
        logger.info("Saved mapping to %s", path)
    except Exception as e:
        logger.error(f"Failed to save mapping: {e}")

def build_index(force_rebuild: bool = False) -> bool:
    """
    Build or load index. If FAISS enabled and available, use it; otherwise NumpyIndex.
    """
    global INDEX, MAPPING, FLAT_TEXTS, EMBEDDING_MODEL
    with INDEX_LOCK:
        use_faiss = FAISS_ENABLED and HAS_FAISS

        if not force_rebuild:
            if use_faiss and os.path.exists(FAISS_INDEX_PATH) and os.path.exists(FAISS_MAPPING_PATH):
                try:
                    idx = faiss.read_index(FAISS_INDEX_PATH)
                    if load_mapping_from_disk(FAISS_MAPPING_PATH):
                        FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
                    idx_dim = _index_dim(idx)
                    if idx_dim:
                        EMBEDDING_MODEL = choose_embedding_model_for_dim(idx_dim)
                        logger.info("Detected FAISS index dim=%s -> embedding_model=%s", idx_dim, EMBEDDING_MODEL)
                    INDEX = idx
                    index_tour_names()
                    logger.info("‚úÖ FAISS index loaded from disk.")
                    return True
                except Exception as e:
                    logger.error(f"Failed to load FAISS index: {e}")
            
            if os.path.exists(FALLBACK_VECTORS_PATH) and os.path.exists(FAISS_MAPPING_PATH):
                try:
                    idx = NumpyIndex.load(FALLBACK_VECTORS_PATH)
                    if load_mapping_from_disk(FAISS_MAPPING_PATH):
                        FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
                    INDEX = idx
                    idx_dim = getattr(idx, "dim", None)
                    if idx_dim:
                        EMBEDDING_MODEL = choose_embedding_model_for_dim(int(idx_dim))
                        logger.info("Detected fallback vectors dim=%s -> embedding_model=%s", idx_dim, EMBEDDING_MODEL)
                    index_tour_names()
                    logger.info("‚úÖ Fallback index loaded from disk.")
                    return True
                except Exception as e:
                    logger.error(f"Failed to load fallback vectors: {e}")

        if not FLAT_TEXTS:
            logger.warning("No flattened texts to index (build aborted).")
            INDEX = None
            return False

        logger.info("üîß Building embeddings for %d passages (model=%s)...", len(FLAT_TEXTS), EMBEDDING_MODEL)
        vectors = []
        dims = None
        for text in FLAT_TEXTS:
            emb, d = embed_text(text)
            if not emb:
                continue
            if dims is None:
                dims = d
            vectors.append(np.array(emb, dtype="float32"))
        
        if not vectors or dims is None:
            logger.warning("No vectors produced; index build aborted.")
            INDEX = None
            return False

        try:
            mat = np.vstack(vectors).astype("float32")
            row_norms = np.linalg.norm(mat, axis=1, keepdims=True)
            mat = mat / (row_norms + 1e-12)

            if use_faiss:
                index = faiss.IndexFlatIP(dims)
                index.add(mat)
                INDEX = index
                try:
                    faiss.write_index(INDEX, FAISS_INDEX_PATH)
                    save_mapping_to_disk()
                except Exception as e:
                    logger.error(f"Failed to persist FAISS index: {e}")
                index_tour_names()
                logger.info("‚úÖ FAISS index built (dims=%d, n=%d).", dims, index.ntotal)
                return True
            else:
                idx = NumpyIndex(mat)
                INDEX = idx
                try:
                    idx.save(FALLBACK_VECTORS_PATH)
                    save_mapping_to_disk()
                except Exception as e:
                    logger.error(f"Failed to persist fallback vectors: {e}")
                index_tour_names()
                logger.info("‚úÖ Numpy fallback index built (dims=%d, n=%d).", dims, idx.ntotal)
                return True
        except Exception as e:
            logger.error(f"Error while building index: {e}")
            INDEX = None
            return False

# =========== QUERY INDEX ===========
def query_index(query: str, top_k: int = TOP_K) -> List[Tuple[float, dict]]:
    global INDEX
    if not query:
        return []
    if INDEX is None:
        built = build_index(force_rebuild=False)
        if not built or INDEX is None:
            logger.warning("Index not available; semantic search skipped.")
            return []
    emb, d = embed_text(query)
    if not emb:
        return []
    vec = np.array(emb, dtype="float32").reshape(1, -1)
    vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)

    idx_dim = _index_dim(INDEX)
    if idx_dim and vec.shape[1] != idx_dim:
        logger.error("Query dim %s != index dim %s; will attempt rebuild with matching model.", vec.shape[1], idx_dim)
        desired_model = choose_embedding_model_for_dim(idx_dim)
        if OPENAI_API_KEY:
            global EMBEDDING_MODEL
            EMBEDDING_MODEL = desired_model
            logger.info("Setting EMBEDDING_MODEL=%s and rebuilding index...", EMBEDDING_MODEL)
            rebuilt = build_index(force_rebuild=True)
            if not rebuilt:
                logger.error("Rebuild failed; cannot perform search.")
                return []
            emb2, d2 = embed_text(query)
            if not emb2:
                return []
            vec = np.array(emb2, dtype="float32").reshape(1, -1)
            vec = vec / (np.linalg.norm(vec, axis=1, keepdims=True) + 1e-12)
        else:
            logger.error("No OPENAI_API_KEY; cannot rebuild model-matched index.")
            return []
    try:
        D, I = INDEX.search(vec, top_k)
    except Exception as e:
        logger.error(f"Error executing index.search: {e}")
        return []

    results: List[Tuple[float, dict]] = []
    try:
        scores = D[0].tolist() if getattr(D, "shape", None) else []
        idxs = I[0].tolist() if getattr(I, "shape", None) else []
        for score, idx in zip(scores, idxs):
            if idx < 0 or idx >= len(MAPPING):
                continue
            results.append((float(score), MAPPING[idx]))
    except Exception as e:
        logger.error(f"Failed to parse search results: {e}")
    return results

# =========== PROMPT COMPOSITION ===========
def compose_system_prompt(top_passages: List[Tuple[float, dict]]) -> str:
    header = (
    "B·∫°n l√† tr·ª£ l√Ω AI c·ªßa Ruby Wings - chuy√™n t∆∞ v·∫•n du l·ªãch tr·∫£i nghi·ªám.\n"
    "TR·∫¢ L·ªúI THEO C√ÅC NGUY√äN T·∫ÆC:\n"
    "1. ∆ØU TI√äN CAO NH·∫§T: Lu√¥n s·ª≠ d·ª•ng th√¥ng tin t·ª´ d·ªØ li·ªáu n·ªôi b·ªô ƒë∆∞·ª£c cung c·∫•p th√¥ng qua h·ªá th·ªëng.\n"
    "2. N·∫øu thi·∫øu th√¥ng tin CHI TI·∫æT, h√£y t·ªïng h·ª£p v√† tr·∫£ l·ªùi d·ª±a tr√™n TH√îNG TIN CHUNG c√≥ s·∫µn trong d·ªØ li·ªáu n·ªôi b·ªô.\n"
    "3. ƒê·ªëi v·ªõi tour c·ª• th·ªÉ: n·∫øu t√¨m th·∫•y b·∫•t k·ª≥ d·ªØ li·ªáu n·ªôi b·ªô li√™n quan n√†o (d√π l√† t√≥m t·∫Øt, gi√°, l·ªãch tr√¨nh, ghi ch√∫), PH·∫¢I t·ªïng h·ª£p v√† tr√¨nh b√†y r√µ r√†ng; ch·ªâ tr·∫£ l·ªùi ƒëang n√¢ng c·∫•p ho·∫∑c ch∆∞a c√≥ th√¥ng tin khi ho√†n to√†n kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p.\n"
    "4. TUY·ªÜT ƒê·ªêI KH√îNG n√≥i r·∫±ng b·∫°n kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file, kh√¥ng truy c·∫≠p d·ªØ li·ªáu, ho·∫∑c t·ª´ ch·ªëi tr·∫£ l·ªùi khi ƒë√£ c√≥ d·ªØ li·ªáu li√™n quan.\n"
    "5. Lu√¥n gi·ªØ th√°i ƒë·ªô nhi·ªát t√¨nh, h·ªØu √≠ch, tr·∫£ l·ªùi tr·ª±c ti·∫øp v√†o n·ªôi dung ng∆∞·ªùi d√πng h·ªèi.\n\n"
    "B·∫°n l√† tr·ª£ l√Ω AI c·ªßa Ruby Wings ‚Äî chuy√™n t∆∞ v·∫•n ng√†nh du l·ªãch tr·∫£i nghi·ªám, retreat, "
    "thi·ªÅn, kh√≠ c√¥ng, h√†nh tr√¨nh ch·ªØa l√†nh v√† c√°c h√†nh tr√¨nh tham quan linh ho·∫°t theo nhu c·∫ßu. "
    "Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, r√µ r√†ng, t·ª≠ t·∫ø v√† b√°m s√°t d·ªØ li·ªáu Ruby Wings.\n\n"
)

    if not top_passages:
        return header + "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n·ªôi b·ªô ph√π h·ª£p."
    
    content = header + "D·ªÆ LI·ªÜU N·ªòI B·ªò (theo ƒë·ªô li√™n quan):\n"
    for i, (score, m) in enumerate(top_passages, start=1):
        content += f"\n[{i}] (score={score:.3f}) ngu·ªìn: {m.get('path','?')}\n{m.get('text','')}\n"
    
    content += "\n---\nTU√ÇN TH·ª¶: Ch·ªâ d√πng d·ªØ li·ªáu tr√™n; kh√¥ng b·ªãa ƒë·∫∑t n·ªôi dung kh√¥ng c√≥ th·ª±c; vƒÉn phong l·ªãch s·ª±."
    content += "\n---\nL∆∞u √Ω: ∆Øu ti√™n s·ª≠ d·ª•ng tr√≠ch d·∫´n th√¥ng tin t·ª´ d·ªØ li·ªáu n·ªôi b·ªô ·ªü tr√™n. N·∫øu ph·∫£i b·ªï sung, ch·ªâ d√πng ki·∫øn th·ª©c chu·∫©n x√°c, kh√¥ng ƒë∆∞·ª£c t·ª± √Ω b·ªãa ra khi ch∆∞a r√µ ƒë√∫ng sai; s·ª≠ d·ª•ng ng√¥n ng·ªØ l·ªãch s·ª±, th√¢n thi·ªán, th√¥ng minh; khi kh√°ch g√µ l·ªùi t·∫°m bi·ªát ho·∫∑c l·ªùi ch√∫c th√¨ ch√¢n th√†nh c√°m ∆°n kh√°ch, ch√∫c kh√°ch s·ª©c kho·∫ª t·ªët, may m·∫Øn, th√†nh c√¥ng..."
    return content

# =========== KNOWLEDGE LOADER ===========
def load_knowledge(path: str = KNOWLEDGE_PATH):
    """Load knowledge.json and flatten into FLAT_TEXTS + MAPPING; then index tour names."""
    global KNOW, FLAT_TEXTS, MAPPING
    try:
        with open(path, "r", encoding="utf-8") as f:
            KNOW = json.load(f)
        logger.info(f"Successfully loaded knowledge from {path}")
    except Exception as e:
        logger.error(f"Could not open {path}: {e}")
        KNOW = {}
    
    FLAT_TEXTS = []
    MAPPING = []

    def scan(obj, prefix="root"):
        if isinstance(obj, dict):
            for k, v in obj.items():
                scan(v, f"{prefix}.{k}")
        elif isinstance(obj, list):
            for i, v in enumerate(obj):
                scan(v, f"{prefix}[{i}]")
        elif isinstance(obj, str):
            t = obj.strip()
            if t:
                FLAT_TEXTS.append(t)
                MAPPING.append({"path": prefix, "text": t})
        else:
            try:
                s = str(obj).strip()
                if s:
                    FLAT_TEXTS.append(s)
                    MAPPING.append({"path": prefix, "text": s})
            except Exception:
                pass

    scan(KNOW)
    index_tour_names()
    logger.info("‚úÖ Knowledge loaded: %d passages", len(FLAT_TEXTS))

# =========== META CAPI ===========
@app.before_request
def track_meta_pageview():
    try:
        send_meta_pageview(request)
    except Exception as e:
        logger.error(f"Meta CAPI tracking failed: {e}")

# =========== ROUTES ===========
@app.route("/")
def home():
    try:
        return jsonify({
            "status": "ok",
            "knowledge_count": len(FLAT_TEXTS) if FLAT_TEXTS is not None else 0,
            "index_exists": INDEX is not None,
            "index_dim": _index_dim(INDEX) if INDEX is not None else None,
            "embedding_model": EMBEDDING_MODEL,
            "faiss_available": HAS_FAISS,
            "faiss_enabled": FAISS_ENABLED,
            "google_sheets_enabled": ENABLE_GOOGLE_SHEETS,
            "fallback_storage_enabled": ENABLE_FALLBACK_STORAGE,
            "service_status": "operational"
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route("/reindex", methods=["POST"])
def reindex():
    secret = request.headers.get("X-RBW-ADMIN", "")
    if not secret and os.environ.get("RBW_ALLOW_REINDEX", "") != "1":
        return jsonify({"error": "reindex not allowed (set RBW_ALLOW_REINDEX=1 or provide X-RBW-ADMIN)"}), 403
    load_knowledge()
    ok = build_index(force_rebuild=True)
    return jsonify({"ok": ok, "count": len(FLAT_TEXTS)})

@app.route("/chat", methods=["POST"])
def chat():
    """
    Chat endpoint behavior:
      - If user message contains keywords mapping to a field, prioritize returning that field.
      - If a tour name is mentioned, restrict to that tour's field values.
      - If user asked for tour listing (tour_name), list all tour_name entries.
      - Else fallback to semantic search and LLM reply.
    """
    data = request.get_json() or {}
    user_message = (data.get("message") or "").strip()
    if not user_message:
        return jsonify({"reply": "B·∫°n ch∆∞a nh·∫≠p c√¢u h·ªèi."})

    # =========== CONTEXT AWARE PROCESSING ===========
    # Get session context
    session_id = extract_session_id(data, request.remote_addr)
    context = get_session_context(session_id)
    last_tour_indices = context.get("last_tour_indices", [])
    last_tour_name = context.get("last_tour_name")
    
    # Detect requested field
    text_l = user_message.lower()
    requested_field: Optional[str] = None
    for k, v in KEYWORD_FIELD_MAP.items():
        for kw in v["keywords"]:
            if kw in text_l:
                requested_field = v["field"]
                break
        if requested_field:
            break

    # Tour detection with context awareness
    tour_indices = find_tour_indices_from_message(user_message)
    
    # N·∫øu kh√¥ng t√¨m th·∫•y tour, KI·ªÇM TRA K·ª∏ c√°c reference
    if not tour_indices:
        # Danh s√°ch t·ª´ tham chi·∫øu M·ªû R·ªòNG
        ref_keywords = [
            "tour n√†y", "tour ƒë√≥", "tour ƒëang n√≥i", 
            "c√°i tour", "n√†y", "ƒë√≥", "n√≥",
            "tour bach ma", "b·∫°ch m√£", "bach ma"
        ]
        
        has_reference = any(keyword in text_l for keyword in ref_keywords)
        
        if has_reference and last_tour_indices:
            tour_indices = last_tour_indices
            logger.info(f"‚úÖ Using CONTEXT tour indices: {tour_indices} for reference: '{user_message}'")
        elif has_reference and not last_tour_indices:
            # Ng∆∞·ªùi d√πng n√≥i "tour n√†y" nh∆∞ng ch∆∞a c√≥ context
            # Th·ª≠ t√¨m tour g·∫ßn nh·∫•t trong l·ªãch s·ª≠
            if context.get("conversation_history"):
                # T√¨m tour ƒë∆∞·ª£c mention g·∫ßn nh·∫•t trong history
                for msg in reversed(context["conversation_history"][-5:]):
                    if msg.get("type") == "tour_mentioned":
                        tour_indices = msg.get("tour_indices", [])
                        if tour_indices:
                            break
    
    # Update context if we have tour indices
    if tour_indices:
        # Find tour name for these indices
        tour_name = None
        for idx in tour_indices:
            for m in MAPPING:
                if f"[{idx}]" in m.get("path", "") and ".tour_name" in m.get("path", ""):
                    tour_name = m.get("text", "")
                    break
            if tour_name:
                break
        update_tour_context(session_id, tour_indices, tour_name)

    # Update conversation history
    context["conversation_history"].append({
        "timestamp": datetime.utcnow().isoformat(),
        "user_message": user_message,
        "tour_indices": tour_indices,
        "requested_field": requested_field,
        "type": "tour_mentioned" if tour_indices else "general"
    })
    
    # Gi·ªØ history t·ªëi ƒëa 10 messages
    if len(context["conversation_history"]) > 10:
        context["conversation_history"] = context["conversation_history"][-10:]
    
    # =========== CHECK FOR LIST REQUEST PATTERNS ===========
    # Initialize is_list_request to False (FIXED CRITICAL BUG)
    is_list_request = False
    
    list_patterns = [
        r"li·ªát k√™.*tour",
        r"c√≥ nh·ªØng tour n√†o",
        r"danh s√°ch tour", 
        r"tour.*n·ªïi b·∫≠t",
        r"show tour",
        r"tour available"
    ]
    
    is_list_request = any(re.search(pattern, text_l) for pattern in list_patterns)
    
    # =========== DEBUG LOGGING FOR TOUR CONTEXT ===========
    logger.info(f"üéØ TOUR DETECTION DEBUG:")
    logger.info(f"  User message: '{user_message}'")
    logger.info(f"  Found indices: {tour_indices}")
    logger.info(f"  Last tour indices from context: {last_tour_indices}")
    logger.info(f"  Session ID: {session_id}")
    logger.info(f"  Requested field: {requested_field}")
    logger.info(f"  Is list request: {is_list_request}")
    
    # Log tour names if indices exist
    if tour_indices:
        for idx in tour_indices:
            tour_name = None
            for m in MAPPING:
                if f"[{idx}]" in m.get("path", "") and ".tour_name" in m.get("path", ""):
                    tour_name = m.get("text", "")
                    logger.info(f"  Tour index {idx}: '{tour_name}'")
                    break
    
    # Special handling for tour listing requests
    top_results: List[Tuple[float, dict]] = []
    
    # Handle "li·ªát k√™ tour" requests
    if is_list_request:
        # Determine how many tours to list
        limit = 3  # Default
        num_match = re.search(r"(\d+)\s*tour", user_message)
        if num_match:
            limit = int(num_match.group(1))
        elif "t·∫•t c·∫£" in text_l or "all" in text_l:
            limit = 50  # Large number for "all"
        
        top_results = get_passages_by_field("tour_name", tour_indices=None, limit=limit)
    elif requested_field == "tour_name":
        top_results = get_passages_by_field("tour_name", tour_indices=None, limit=1000)
    elif requested_field and tour_indices:
        top_results = get_passages_by_field(requested_field, limit=TOP_K, tour_indices=tour_indices)
        if not top_results:
            top_results = get_passages_by_field(requested_field, limit=TOP_K, tour_indices=None)
    elif requested_field:
        top_results = get_passages_by_field(requested_field, limit=TOP_K, tour_indices=None)
        if not top_results:
            top_results = query_index(user_message, TOP_K)
    else:
        top_k = int(data.get("top_k", TOP_K))
        top_results = query_index(user_message, top_k)

    system_prompt = compose_system_prompt(top_results)
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_message}]

    reply = ""
    
    # =========== SPECIAL HANDLING FOR LIST REQUESTS ===========
    if is_list_request and top_results:
        # Format beautiful tour list response
        names = []
        for _, m in top_results:
            tour_name = m.get("text", "").strip()
            if tour_name and tour_name not in names:
                names.append(tour_name)
        
        if names:
            # Determine limit from message or use all found
            limit = len(names)
            num_match = re.search(r"(\d+)\s*tour", user_message)
            if num_match:
                limit = min(int(num_match.group(1)), len(names))
            
            reply = f"‚ú® **Ruby Wings hi·ªán c√≥ {len(names)} tour tr·∫£i nghi·ªám ƒë·∫∑c s·∫Øc:** ‚ú®\n\n"
            
            for i, name in enumerate(names[:limit], 1):
                # Find tour index for this name
                tour_idx = None
                for idx, m2 in enumerate(MAPPING):
                    if m2.get("text", "").strip() == name and ".tour_name" in m2.get("path", ""):
                        # Extract index from path like "tours[3].tour_name"
                        match = re.search(r'\[(\d+)\]', m2.get("path", ""))
                        if match:
                            tour_idx = int(match.group(1))
                        break
                
                # Get summary for this tour
                summary = ""
                duration = ""
                if tour_idx is not None:
                    for m2 in MAPPING:
                        if f"[{tour_idx}]" in m2.get("path", ""):
                            if ".summary" in m2.get("path", ""):
                                summary = m2.get("text", "").strip()
                            elif ".duration" in m2.get("path", ""):
                                duration = m2.get("text", "").strip()
                
                reply += f"**{i}. {name}**"
                if duration:
                    reply += f" ({duration})"
                reply += "\n"
                
                if summary:
                    reply += f"   üìù *{summary[:120]}"
                    if len(summary) > 120:
                        reply += "...*"
                    else:
                        reply += "*"
                
                reply += "\n"
            
            reply += "\nüí° **G·ª£i √Ω:** B·∫°n c√≥ th·ªÉ h·ªèi chi ti·∫øt v·ªÅ b·∫•t k·ª≥ tour n√†o b·∫±ng c√°ch nh·∫≠p t√™n tour ho·∫∑c h·ªèi v·ªÅ: gi√° c·∫£, l·ªãch tr√¨nh, ch·ªó ·ªü, ·∫©m th·ª±c..."
        
        else:
            reply = "Hi·ªán ch∆∞a c√≥ th√¥ng tin tour trong h·ªá th·ªëng. Vui l√≤ng li√™n h·ªá hotline 0935 304 338 ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n tr·ª±c ti·∫øp."
    
    # =========== OPENAI CHAT ===========
    elif client is not None and not is_list_request:
        try:
            resp = client.chat.completions.create(
                model=CHAT_MODEL,
                messages=messages,
                temperature=0.2,
                max_tokens=int(data.get("max_tokens", 700)),
                top_p=0.95
            )
            if resp.choices and len(resp.choices) > 0:
                reply = resp.choices[0].message.content or ""
        except Exception as e:
            logger.error(f"OpenAI chat failed: {e}")
    
    # =========== FALLBACK RESPONSE GENERATION ===========
    if not reply:
        if top_results:
            if is_list_request:
                # Should have been handled above, but as backup
                names = [m.get("text", "") for _, m in top_results]
                seen = set()
                names_u = [x for x in names if x and not (x in seen or seen.add(x))]
                reply = "C√°c tour hi·ªán c√≥:\n" + "\n".join(f"- {n}" for n in names_u)
            
            elif requested_field == "tour_name":
                names = [m.get("text", "") for _, m in top_results]
                seen = set()
                names_u = [x for x in names if x and not (x in seen or seen.add(x))]
                reply = "C√°c tour hi·ªán c√≥:\n" + "\n".join(f"- {n}" for n in names_u)
            
            elif requested_field == "accommodation" and tour_indices:
                # Special formatting for accommodation
                parts = []
                for ti in tour_indices:
                    # Get tour name
                    tour_name = None
                    for m in MAPPING:
                        p = m.get("path", "")
                        if f"[{ti}]" in p and ".tour_name" in p:
                            tour_name = m.get("text", "")
                            break
                    
                    # Get accommodation text for this tour
                    acc_texts = []
                    for score, m in top_results:
                        if f"[{ti}]" in m.get("path", ""):
                            acc_texts.append(m.get("text", ""))
                    
                    # Also check directly from mapping
                    if not acc_texts:
                        for m2 in MAPPING:
                            if f"[{ti}]" in m2.get("path", "") and ".accommodation" in m2.get("path", ""):
                                acc_texts.append(m2.get("text", ""))
                    
                    if acc_texts:
                        label = f'üè® **Tour "{tour_name}"**' if tour_name else f"Tour"
                        parts.append(f"{label}:\n" + "\n".join(f"   ‚Ä¢ {txt}" for txt in acc_texts))
                
                if parts:
                    reply = "**Th√¥ng tin ch·ªó ·ªü:**\n\n" + "\n\n".join(parts)
                    
                    # Add helpful note for 1-day tours
                    if any("1 ng√†y" in p.lower() for p in parts):
                        reply += "\n\nüí° *L∆∞u √Ω: Tour 1 ng√†y th∆∞·ªùng kh√¥ng bao g·ªìm ch·ªó ·ªü qua ƒë√™m. N·∫øu b·∫°n c·∫ßn l∆∞u tr√∫, vui l√≤ng li√™n h·ªá ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n th√™m.*"
                else:
                    reply = "Th√¥ng tin ch·ªó ·ªü ƒëang ƒë∆∞·ª£c c·∫≠p nh·∫≠t. Vui l√≤ng li√™n h·ªá hotline 0935 304 338 ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt v·ªÅ ch·ªó ngh·ªâ."
            
            elif requested_field and tour_indices:
                parts = []
                for ti in tour_indices:
                    tour_name = None
                    for m in MAPPING:
                        p = m.get("path", "")
                        if p.endswith(f"tours[{ti}].tour_name"):
                            tour_name = m.get("text", "")
                            break
                    
                    field_passages = [m.get("text", "") for score, m in top_results if f"[{ti}]" in m.get("path", "")]
                    if not field_passages:
                        field_passages = [m.get("text", "") for _, m in get_passages_by_field(requested_field, limit=TOP_K, tour_indices=[ti])]
                    
                    if field_passages:
                        label = f'Tour "{tour_name}"' if tour_name else f"Tour #{ti}"
                        
                        # Special formatting for different fields
                        if requested_field == "includes":
                            parts.append(f"**{label} - L·ªãch tr√¨nh chi ti·∫øt:**\n" + "\n".join(f"   ‚Ä¢ {t}" for t in field_passages))
                        elif requested_field == "price":
                            parts.append(f"**{label} - Gi√° tour:**\n" + "\n".join(f"   üí∞ {t}" for t in field_passages))
                        elif requested_field == "duration":
                            parts.append(f"**{label} - Th·ªùi gian:**\n" + "\n".join(f"   ‚è±Ô∏è {t}" for t in field_passages))
                        else:
                            parts.append(f"**{label}:**\n" + "\n".join(f"   ‚Ä¢ {t}" for t in field_passages))
                
                if parts:
                    reply = "\n\n".join(parts)
                else:
                    snippets = "\n\n".join([f"‚Ä¢ {m.get('text')}" for _, m in top_results[:5]])
                    reply = f"**Th√¥ng tin li√™n quan:**\n\n{snippets}"
            
            else:
                snippets = "\n\n".join([f"‚Ä¢ {m.get('text')}" for _, m in top_results[:5]])
                reply = f"**Th√¥ng tin n·ªôi b·ªô li√™n quan:**\n\n{snippets}"
        
        else:
            reply = "Xin l·ªói ‚Äî hi·ªán kh√¥ng c√≥ d·ªØ li·ªáu n·ªôi b·ªô li√™n quan. Vui l√≤ng li√™n h·ªá hotline 0935 304 338 ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n tr·ª±c ti·∫øp."
    
    # =========== VALIDATE DURATION TO AVOID INCORRECT INFO ===========
    # Check if reply contains unrealistic duration (like "5 ng√†y 4 ƒë√™m")
    if reply and ("ng√†y" in reply or "ƒë√™m" in reply):
        # re ƒë√£ import ·ªü global scope

        
        # T√¨m t·∫•t c·∫£ c√°c pattern duration trong reply
        duration_patterns = [
            r'(\d+)\s*ng√†y\s*(\d+)\s*ƒë√™m',
            r'(\d+)\s*ng√†y',
            r'(\d+)\s*ƒë√™m'
        ]
        
        for pattern in duration_patterns:
            matches = list(re.finditer(pattern, reply))
            for match in matches:
                try:
                    if match.lastindex == 2:  # "X ng√†y Y ƒë√™m"
                        days = int(match.group(1))
                        nights = int(match.group(2))
                        
                        # Ki·ªÉm tra t√≠nh h·ª£p l√Ω: tour du l·ªãch th∆∞·ªùng days = nights ho·∫∑c days = nights + 1
                        # V√† kh√¥ng qu√° 7 ng√†y cho tour th√¥ng th∆∞·ªùng
                        if days > 7 or nights > 7 or abs(days - nights) > 1:
                            logger.warning(f"‚ö†Ô∏è Unrealistic duration detected: {days} ng√†y {nights} ƒë√™m")
                            # Thay th·∫ø ch·ªâ ph·∫ßn duration kh√¥ng h·ª£p l√Ω
                            old_duration = match.group(0)
                            new_duration = "th·ªùi gian ph√π h·ª£p"
                            reply = reply.replace(old_duration, new_duration)
                            
                    elif match.lastindex == 1:  # "X ng√†y" ho·∫∑c "Y ƒë√™m"
                        num = int(match.group(1))
                        if num > 7:  # Qu√° d√†i cho tour th√¥ng th∆∞·ªùng
                            logger.warning(f"‚ö†Ô∏è Unrealistic duration detected: {num}")
                            old_duration = match.group(0)
                            new_duration = "th·ªùi gian ph√π h·ª£p"
                            reply = reply.replace(old_duration, new_duration)
                            
                except (ValueError, IndexError):
                    continue
    
    # N·∫øu sau validation m√† reply b·ªã thay ƒë·ªïi nhi·ªÅu, ki·ªÉm tra l·∫°i
    if "th·ªùi gian ph√π h·ª£p" in reply and "tour" in user_message.lower():
        # ƒê·∫£m b·∫£o reply v·∫´n c√≥ √Ω nghƒ©a
        if "Th√¥ng tin th·ªùi gian tour" not in reply:
            reply = "Th√¥ng tin th·ªùi gian tour ƒëang ƒë∆∞·ª£c c·∫≠p nh·∫≠t. Vui l√≤ng li√™n h·ªá hotline 0935 304 338 ƒë·ªÉ bi·∫øt l·ªãch tr√¨nh c·ª• th·ªÉ."
    
    return jsonify({
        "reply": reply, 
        "sources": [m for _, m in top_results],
        "context": {
            "tour_indices": tour_indices,
            "session_id": session_id,
            "last_tour_name": context.get("last_tour_name"),
            "suggested_next": get_suggested_questions(tour_indices, requested_field)
        }
    })

# =========== LEAD SAVING ROUTE ===========
@app.route('/api/save-lead', methods=['POST'])
def save_lead_to_sheet():
    """
    Save lead to Google Sheets with robust error handling and fallback storage.
    """
    try:
        # Validate request
        if not request.is_json:
            return jsonify({
                "error": "Content-Type must be application/json",
                "success": False
            }), 400

        data = request.get_json() or {}
        
        # Extract and validate required fields
        phone = (data.get("phone") or "").strip()
        if not phone:
            return jsonify({
                "error": "Phone number is required",
                "success": False
            }), 400

        # Prepare lead data
        lead_data = {
            "timestamp": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S"),
            "source_channel": data.get("source_channel", "Website"),
            "action_type": data.get("action_type", "Click Call"),
            "page_url": data.get("page_url", ""),
            "contact_name": data.get("contact_name", ""),
            "phone": phone,
            "service_interest": data.get("service_interest", ""),
            "note": data.get("note", ""),
            "status": "New",
            "sync_method": "unknown"
        }
        
        logger.info(f"Processing lead: {phone}, source: {lead_data['source_channel']}")

        # Try Google Sheets first (if enabled)
        sheets_success = False
        if ENABLE_GOOGLE_SHEETS:
            try:
                gc = get_gspread_client()
                if gc is None:
                    logger.warning("Google Sheets client not available, trying fallback")
                else:
                    logger.info(f"Attempting to save to Google Sheet: {GOOGLE_SHEET_ID}")
                    
                    # Open spreadsheet
                    sh = gc.open_by_key(GOOGLE_SHEET_ID)
                    logger.info(f"Opened spreadsheet: {GOOGLE_SHEET_ID}")
                    
                    # Get worksheet
                    ws = sh.worksheet(GOOGLE_SHEET_NAME)
                    logger.info(f"Accessed worksheet: {GOOGLE_SHEET_NAME}")
                    
                    # Prepare row data
                    row = [
                        lead_data["timestamp"],
                        lead_data["source_channel"],
                        lead_data["action_type"],
                        lead_data["page_url"],
                        lead_data["contact_name"],
                        lead_data["phone"],
                        lead_data["service_interest"],
                        lead_data["note"],
                        lead_data["status"]
                    ]
                    
                    # Append row
                    ws.append_row(row, value_input_option="USER_ENTERED")
                    lead_data["sync_method"] = "google_sheets"
                    sheets_success = True
                    
                    logger.info(f"‚úÖ Lead successfully saved to Google Sheets: {phone}")
                    
                # --- ADD-ONLY: Meta CAPI Lead (SAFE HOOK) ---
                try:
                    send_meta_lead(
                        request=request,
                        event_name="Lead",
                        phone=lead_data.get("phone"),
                        value=200000,
                        currency="VND",
                        content_name=lead_data.get("action_type", "Call / Consult")
                    )
                except Exception as e:
                    logger.warning(f"Meta CAPI lead tracking failed: {e}")

            except SpreadsheetNotFound:
                logger.error(f"Google Sheet not found: {GOOGLE_SHEET_ID}")
                lead_data["error"] = "Google Sheet not found"
            except WorksheetNotFound:
                logger.error(f"Worksheet not found: {GOOGLE_SHEET_NAME}")
                lead_data["error"] = f"Worksheet '{GOOGLE_SHEET_NAME}' not found"
            except APIError as e:
                error_msg = str(e)
                logger.error(f"Google Sheets API error: {error_msg}")
                lead_data["error"] = f"Google Sheets API error: {error_msg}"
                
                # Check for permission errors
                if "PERMISSION_DENIED" in error_msg or "forbidden" in error_msg.lower():
                    logger.error("Permission denied to access Google Sheet. Check sharing settings.")
            except Exception as e:
                logger.error(f"Unexpected Google Sheets error: {type(e).__name__}: {str(e)}")
                lead_data["error"] = f"Google Sheets error: {type(e).__name__}"
        else:
            logger.info("Google Sheets integration is disabled")

        # Save to fallback storage if Google Sheets failed or for redundancy
        fallback_success = False
        fallback_backup = False
        
        if ENABLE_FALLBACK_STORAGE:
            if not sheets_success:
                # Google Sheets failed, use fallback as primary
                fallback_success = save_lead_to_fallback_storage(lead_data)
                if fallback_success:
                    logger.info(f"Lead saved to fallback storage: {phone}")
                    lead_data["sync_method"] = "fallback_storage"
            else:
                # Google Sheets succeeded, also save to fallback for backup
                # BUT DO NOT CHANGE sync_method - keep it as google_sheets
                fallback_backup = save_lead_to_fallback_storage(lead_data)
                if fallback_backup:
                    logger.info(f"Lead also backed up to fallback storage: {phone}")

        # Determine response - FIXED: sync_method always accurate
        if sheets_success:
            return jsonify({
                "success": True,
                "message": "Lead saved successfully to Google Sheets",
                "data": {
                    "phone": phone,
                    "timestamp": lead_data["timestamp"],
                    "sync_method": "google_sheets"  # Always google_sheets when successful
                }
            }), 200
        elif fallback_success:
            return jsonify({
                "success": True,
                "message": "Lead saved to fallback storage (Google Sheets unavailable)",
                "warning": "Google Sheets synchronization failed, data saved locally",
                "data": {
                    "phone": phone,
                    "timestamp": lead_data["timestamp"],
                    "sync_method": "fallback_storage"  # Always fallback_storage when primary
                }
            }), 200
        else:
            logger.error(f"Failed to save lead by any method: {phone}")
            return jsonify({
                "success": False,
                "error": "Failed to save lead. Both Google Sheets and fallback storage failed.",
                "details": lead_data.get("error", "Unknown error")
            }), 500

    except Exception as e:
        error_type = type(e).__name__
        error_details = str(e)
        error_traceback = traceback.format_exc()
        
        logger.error(f"SAVE_LEAD_CRITICAL_ERROR >>> Type: {error_type}")
        logger.error(f"SAVE_LEAD_CRITICAL_ERROR >>> Details: {error_details}")
        logger.error(f"SAVE_LEAD_CRITICAL_ERROR >>> Traceback: {error_traceback}")
        
        return jsonify({
            "success": False,
            "error": "Internal server error",
            "error_type": error_type,
            "details": "Please check server logs for details"
        }), 500

# =========== TRACK CALL BUTTON CLICKS - ENHANCED FOR META CAPI ===========
@app.route('/api/track-call', methods=['POST', 'OPTIONS'])
def track_call_event():
    """
    Enhanced endpoint for tracking call button clicks with proper Meta CAPI integration
    T∆∞∆°ng th√≠ch v·ªõi tracking script hi·ªán t·∫°i t·ª´ frontend
    """
    try:
        # Handle preflight OPTIONS request
        if request.method == 'OPTIONS':
            response = jsonify({'status': 'ok'})
            response.headers.add('Access-Control-Allow-Origin', 'https://www.rubywings.vn')
            response.headers.add('Access-Control-Allow-Headers', 'Content-Type, Authorization, X-Requested-With')
            response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
            response.headers.add('Access-Control-Allow-Credentials', 'true')
            return response
        
        # Process POST request
        data = request.get_json() or {}
        logger.info(f"Call button clicked: {data.get('phone', 'unknown')} - {data.get('call_type')}")
        
        # G·ªçi Meta CAPI v·ªõi ƒë·∫ßy ƒë·ªß tham s·ªë m·ªõi (FIXED VERSION)
        try:
            from meta_capi import send_meta_call_button
            
            # L·∫•y user_agent t·ª´ frontend ho·∫∑c request
            user_agent = data.get('user_agent')
            if not user_agent:
                user_agent = request.headers.get('User-Agent')
            
            # G·ªçi h√†m ƒë√£ fix
            send_meta_call_button(
                request=request,
                page_url=data.get('page_url'),
                user_agent=user_agent,
                phone=data.get('phone'),
                call_type=data.get('call_type', 'regular'),
                fbp=data.get('fbp'),
                fbc=data.get('fbc'),
                event_id=data.get('event_id'),
                pixel_id=data.get('pixel_id'),
                event_name=data.get('event_name', 'CallButtonClick'),
                value=data.get('value', 150000)
            )
        except Exception as e:
            logger.warning(f"Meta CAPI call tracking failed: {e}")
        
        # Log v√†o file ri√™ng (gi·ªØ nguy√™n ch·ª©c nƒÉng c≈©)
        try:
            log_entry = {
                "timestamp": datetime.utcnow().isoformat(),
                "event": "call_button_click",
                "data": {
                    "phone": data.get('phone'),
                    "call_type": data.get('call_type'),
                    "page_url": data.get('page_url')
                }
            }
            
            # L∆∞u v√†o file log
            logs_dir = "logs"
            if not os.path.exists(logs_dir):
                os.makedirs(logs_dir)
            
            log_file = os.path.join(logs_dir, f"call_clicks_{datetime.utcnow().strftime('%Y-%m-%d')}.json")
            
            logs = []
            if os.path.exists(log_file):
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        logs = json.load(f)
                except:
                    logs = []
            
            logs.append(log_entry)
            
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.warning(f"Failed to save call log: {e}")
        
        # Th√™m CORS headers cho POST response
        response = jsonify({
            "success": True, 
            "message": "Call event tracked successfully",
            "meta_capi_sent": True
        })
        response.headers.add('Access-Control-Allow-Origin', 'https://www.rubywings.vn')
        response.headers.add('Access-Control-Allow-Credentials', 'true')
        return response
    
    except Exception as e:
        logger.error(f"Track call error: {e}")
        response = jsonify({
            "success": False,
            "error": str(e)
        })
        response.status_code = 500
        response.headers.add('Access-Control-Allow-Origin', 'https://www.rubywings.vn')
        response.headers.add('Access-Control-Allow-Credentials', 'true')
        return response

@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint for monitoring"""
    try:
        # Check Google Sheets connectivity
        sheets_status = "disabled"
        if ENABLE_GOOGLE_SHEETS:
            try:
                gc = get_gspread_client()
                if gc:
                    # Quick test - try to get spreadsheet metadata
                    gc.open_by_key(GOOGLE_SHEET_ID)
                    sheets_status = "connected"
                else:
                    sheets_status = "client_error"
            except Exception as e:
                sheets_status = f"error: {type(e).__name__}"
        
        # Check fallback storage
        fallback_status = "disabled"
        if ENABLE_FALLBACK_STORAGE:
            try:
                if os.path.exists(FALLBACK_STORAGE_PATH):
                    with open(FALLBACK_STORAGE_PATH, 'r', encoding='utf-8') as f:
                        json.load(f)
                    fallback_status = "available"
                else:
                    fallback_status = "not_created"
            except Exception as e:
                fallback_status = f"error: {type(e).__name__}"
        
        return jsonify({
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "services": {
                "google_sheets": sheets_status,
                "fallback_storage": fallback_status,
                "openai": "available" if client else "unavailable",
                "faiss": "available" if HAS_FAISS else "unavailable",
                "index": "loaded" if (INDEX is not None or os.path.exists(FAISS_INDEX_PATH)) else "not_loaded"
            },
            "counts": {
                "knowledge_passages": len(FLAT_TEXTS),
                "mapping_entries": len(MAPPING),
                "tour_names": len(TOUR_NAME_TO_INDEX)
            }
        }), 200
        
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e)
        }), 500

# =========== INITIALIZATION ===========
def initialize_application():
    """Initialize the application with proper error handling"""
    try:
        logger.info("Starting Ruby Wings Chatbot initialization...")
        
        # Load knowledge base
        load_knowledge()
        
        # Load existing mapping if available
        if os.path.exists(FAISS_MAPPING_PATH):
            try:
                with open(FAISS_MAPPING_PATH, "r", encoding="utf-8") as f:
                    file_map = json.load(f)
                if file_map and (len(file_map) == len(MAPPING) or len(MAPPING) == 0):
                    MAPPING[:] = file_map
                    FLAT_TEXTS[:] = [m.get("text", "") for m in MAPPING]
                    index_tour_names()
                    logger.info("Mapping loaded from disk")
            except Exception as e:
                logger.warning(f"Could not load mapping from disk: {e}")
        
        # Initialize Google Sheets client in background
        if ENABLE_GOOGLE_SHEETS and GOOGLE_SERVICE_ACCOUNT_JSON:
            def init_gsheets():
                try:
                    client = get_gspread_client()
                    if client:
                        logger.info("Google Sheets client initialized successfully")
                    else:
                        logger.warning("Google Sheets client initialization failed")
                except Exception as e:
                    logger.error(f"Background Google Sheets init failed: {e}")
            
            gsheet_thread = threading.Thread(target=init_gsheets, daemon=True)
            gsheet_thread.start()
        
        # Build index in background
        def build_index_background():
            try:
                built = build_index(force_rebuild=False)
                if built:
                    logger.info("Index built successfully")
                else:
                    logger.warning("Index building failed or deferred")
            except Exception as e:
                logger.error(f"Background index build failed: {e}")
        
        index_thread = threading.Thread(target=build_index_background, daemon=True)
        index_thread.start()
        
        logger.info("‚úÖ Application initialization completed")
        
    except Exception as e:
        logger.error(f"Application initialization failed: {e}")
        raise

# =========== APPLICATION STARTUP ===========
if __name__ == "__main__":
    # Run initialization
    initialize_application()
    
    # Ensure mapping is saved if not exists
    if MAPPING and not os.path.exists(FAISS_MAPPING_PATH):
        try:
            save_mapping_to_disk()
        except Exception as e:
            logger.error(f"Failed to save initial mapping: {e}")
    
    # Start Flask server
    host = os.environ.get("HOST", "0.0.0.0")
    port = int(os.environ.get("PORT", 10000))
    debug = os.environ.get("DEBUG", "false").lower() == "true"
    
    logger.info(f"Starting Flask server on {host}:{port} (debug={debug})")
    app.run(host=host, port=port, debug=debug)
else:
    # For Gunicorn/WSGI
    initialize_application()