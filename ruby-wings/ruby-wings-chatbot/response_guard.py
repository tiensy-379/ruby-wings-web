#!/usr/bin/env python3
"""
response_guard.py v5.2

Enhanced "expert guard" to validate & format final answers before sending to user.
Now includes state-based templates, location-aware responses, and improved tour formatting.

Responsibilities:
- Ensure answers cite sources (e.g., [1], [2]) or attach retrieved snippets if LLM hallucinated.
- Ensure answer content is consistent with retrieved evidence (simple token overlap check).
- Ensure requested_field is respected (if provided) by preferring passages for that field.
- Enforce friendly "healing travel" tone with short sanitization heuristics.
- Provide deterministic fallback that only uses retrieved passages when LLM output fails checks.
- NEW: State-based response templates for different conversation stages.
- NEW: Location-aware response formatting with region suggestions.
- NEW: Tour response formatting with labels (üèÜ, ‚≠ê, üí∞).
- NEW: Intent-specific response templates.

Usage (minimal):
  from response_guard import validate_and_format_answer
  out = validate_and_format_answer(
      llm_text=llm_text,
      top_passages=top_passages,            # List[Tuple[score, mapping_entry]]
      requested_field=requested_field,      # optional string
      tour_indices=tour_indices,            # optional list[int]
      max_tokens=700,
      context={}                           # NEW: conversation context
  )
  return jsonify(out)

Return value:
  {
    "answer": "<final text to send user>",
    "sources": ["root.tours[2].price", ...],   # list of mapping paths used
    "guard_passed": True/False,
    "reason": "ok" | "no_evidence" | "mismatch_field" | ...,
    "state": "explore" | "suggest" | ...,
    "tour_labels": [],  # NEW: tour labels used in response
    "location_filtered": False  # NEW: if location filter was applied
  }
"""

import re
import html
import time
import random
from typing import List, Tuple, Dict, Any, Optional, Union
from collections import Counter
from datetime import datetime

# Import necessary enums from entities (simplified)
class ConversationStage:
    """Simplified ConversationStage for response_guard"""
    EXPLORE = "explore"
    SUGGEST = "suggest"
    COMPARE = "compare"
    SELECT = "select"
    BOOK = "book"
    LEAD = "lead"
    CALLBACK = "callback"

class Intent:
    """Simplified Intent for response_guard"""
    PROVIDE_PHONE = "provide_phone"
    CALLBACK_REQUEST = "callback_request"
    BOOKING_CONFIRM = "booking_confirm"
    MODIFY_REQUEST = "modify_request"
    SMALLTALK = "smalltalk"
    LEAD_CAPTURED = "lead_captured"
    GREETING = "greeting"
    FAREWELL = "farewell"
    TOUR_INQUIRY = "tour_inquiry"
    UNKNOWN = "unknown"

# --- Simple helpers ---
SRC_RE = re.compile(r"\[\d+\]")  # detect [1], [2] style citations

def extract_source_tokens(text: str) -> List[str]:
    """Return list of citation tokens like [1] found in text."""
    return SRC_RE.findall(text or "")

def normalize_for_overlap(s: str) -> List[str]:
    if not s:
        return []
    s = s.lower()
    s = re.sub(r"[^\w\s]", " ", s)
    toks = [t for t in s.split() if len(t) > 1]
    return toks

def overlap_ratio(a_tokens: List[str], b_tokens: List[str]) -> float:
    if not a_tokens or not b_tokens:
        return 0.0
    ca = Counter(a_tokens)
    cb = Counter(b_tokens)
    common = sum(min(ca[t], cb.get(t, 0)) for t in ca)
    return common / max(len(a_tokens), 1)

def collect_passage_texts(top_passages: List[Tuple[float, Dict]]) -> List[str]:
    return [m.get("text","") for _, m in (top_passages or [])]

def collect_passage_paths(top_passages: List[Tuple[float, Dict]]) -> List[str]:
    return [m.get("path","") for _, m in (top_passages or [])]

def safe_shorten(text: str, max_chars: int = 1200) -> str:
    if not text:
        return ""
    t = text.strip()
    if len(t) <= max_chars:
        return t
    # try to cut at sentence boundary
    cut = t[:max_chars].rfind(".")
    if cut > int(max_chars*0.5):
        return t[:cut+1]
    return t[:max_chars].rstrip() + "..."

# --- Guard rules / parameters ---
MIN_OVERLAP_RATIO = 0.12   # minimal overlap between LLM text and evidence to accept
MIN_FIELD_MENTION_RATIO = 0.02  # small threshold to allow field-specific match via text overlap
MAX_ANSWER_CHARS = 1500
BANNED_PHRASES = ["i think", "i guess", "maybe", "probably", "as far as i know", "i'm not sure"]

# --- NEW: Response Templates by State ---
STATE_TEMPLATES = {
    ConversationStage.EXPLORE: [
        "T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ tour du l·ªãch tr·∫£i nghi·ªám Ruby Wings? üåø",
        "B·∫°n mu·ªën t√¨m hi·ªÉu v·ªÅ tour du l·ªãch n√†o c·ªßa Ruby Wings? üòä",
        "Ch√†o b·∫°n! T√¥i c√≥ th·ªÉ t∆∞ v·∫•n cho b·∫°n v·ªÅ c√°c h√†nh tr√¨nh tr·∫£i nghi·ªám c·ªßa Ruby Wings."
    ],
    
    ConversationStage.SUGGEST: [
        "D·ª±a tr√™n y√™u c·∫ßu c·ªßa b·∫°n, t√¥i ƒë·ªÅ xu·∫•t 3 tour sau:",
        "T√¥i t√¨m th·∫•y m·ªôt s·ªë tour ph√π h·ª£p v·ªõi b·∫°n:",
        "D∆∞·ªõi ƒë√¢y l√† c√°c tour Ruby Wings b·∫°n c√≥ th·ªÉ quan t√¢m:"
    ],
    
    ConversationStage.COMPARE: [
        "ƒê·ªÉ so s√°nh c√°c tour, t√¥i t√≥m t·∫Øt th√¥ng tin ch√≠nh:",
        "D∆∞·ªõi ƒë√¢y l√† th√¥ng tin so s√°nh gi·ªØa c√°c tour:",
        "T√¥i s·∫Ω gi√∫p b·∫°n so s√°nh c√°c tour ƒë·ªÉ ch·ªçn ph√π h·ª£p nh·∫•t:"
    ],
    
    ConversationStage.SELECT: [
        "B·∫°n ƒë√£ ch·ªçn tour {tour_name}. B·∫°n mu·ªën ƒë·∫∑t tour n√†y kh√¥ng?",
        "Tour {tour_name} r·∫•t ph√π h·ª£p v·ªõi b·∫°n! B·∫°n mu·ªën ti·∫øp t·ª•c ƒë·∫∑t tour kh√¥ng?",
        "Tuy·ªát v·ªùi! Tour {tour_name} ƒë√£ ƒë∆∞·ª£c ch·ªçn. B·∫°n c√≥ mu·ªën ƒë·∫∑t ngay kh√¥ng?"
    ],
    
    ConversationStage.BOOK: [
        "Tour {tour_name} ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t. Vui l√≤ng cung c·∫•p s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ ch√∫ng t√¥i li√™n h·ªá x√°c nh·∫≠n.",
        "Booking th√†nh c√¥ng! Ch√∫ng t√¥i s·∫Ω li√™n h·ªá v·ªõi b·∫°n qua s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ x√°c nh·∫≠n chi ti·∫øt.",
        "ƒê√£ x√°c nh·∫≠n ƒë·∫∑t tour {tour_name}. Vui l√≤ng cho ch√∫ng t√¥i s·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ ho√†n t·∫•t th·ªß t·ª•c."
    ],
    
    ConversationStage.LEAD: [
        "ƒê√£ l∆∞u s·ªë {phone}. Ch√∫ng t√¥i s·∫Ω g·ªçi l·∫°i cho b·∫°n trong 30 ph√∫t.",
        "C·∫£m ∆°n b·∫°n ƒë√£ cung c·∫•p s·ªë ƒëi·ªán tho·∫°i {phone}. ƒê·ªôi ng≈© Ruby Wings s·∫Ω li√™n h·ªá s·ªõm nh·∫•t!",
        "S·ªë ƒëi·ªán tho·∫°i {phone} ƒë√£ ƒë∆∞·ª£c ghi nh·∫≠n. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá t∆∞ v·∫•n cho b·∫°n s·ªõm."
    ],
    
    ConversationStage.CALLBACK: [
        "ƒê√£ ghi nh·∫≠n y√™u c·∫ßu g·ªçi l·∫°i. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá s·ªë {phone} trong ng√†y h√¥m nay.",
        "Y√™u c·∫ßu g·ªçi l·∫°i ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n. Ch√∫ng t√¥i s·∫Ω g·ªçi s·ªë {phone} trong v√≤ng 2 gi·ªù.",
        "Ch√∫ng t√¥i ƒë√£ ghi nh·∫≠n c·∫ßn g·ªçi l·∫°i s·ªë {phone}. S·∫Ω li√™n h·ªá v·ªõi b·∫°n s·ªõm nh·∫•t c√≥ th·ªÉ."
    ]
}

# --- NEW: Intent Templates ---
INTENT_TEMPLATES = {
    Intent.PROVIDE_PHONE: [
        "C·∫£m ∆°n b·∫°n ƒë√£ cung c·∫•p s·ªë ƒëi·ªán tho·∫°i {phone}. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá s·ªõm nh·∫•t! üìû",
        "ƒê√£ nh·∫≠n s·ªë ƒëi·ªán tho·∫°i {phone}. ƒê·ªôi ng≈© Ruby Wings s·∫Ω g·ªçi t∆∞ v·∫•n cho b·∫°n!",
        "C·∫£m ∆°n b·∫°n! S·ªë {phone} ƒë√£ ƒë∆∞·ª£c l∆∞u l·∫°i. Ch√∫ng t√¥i s·∫Ω li√™n h·ªá trong th·ªùi gian s·ªõm nh·∫•t."
    ],
    
    Intent.CALLBACK_REQUEST: [
        "B·∫°n mu·ªën ch√∫ng t√¥i g·ªçi l·∫°i khi n√†o? (s√°ng/chi·ªÅu/t·ªëi)",
        "Vui l√≤ng cho bi·∫øt khung gi·ªù ph√π h·ª£p ƒë·ªÉ ch√∫ng t√¥i g·ªçi l·∫°i cho b·∫°n?",
        "ƒê·ªÉ thu·∫≠n ti·ªán cho b·∫°n, b·∫°n mu·ªën ƒë∆∞·ª£c g·ªçi l·∫°i v√†o kho·∫£ng th·ªùi gian n√†o trong ng√†y?"
    ],
    
    Intent.SMALLTALK: [
        "Xin ch√†o! T√¥i l√† Ruby Wings AI, r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n. üòä",
        "Ch√†o b·∫°n! T√¥i ·ªü ƒë√¢y ƒë·ªÉ gi√∫p b·∫°n t√¨m tour tr·∫£i nghi·ªám ph√π h·ª£p nh·∫•t.",
        "R·∫•t vui ƒë∆∞·ª£c tr√≤ chuy·ªán v·ªõi b·∫°n! B·∫°n c·∫ßn t∆∞ v·∫•n v·ªÅ tour n√†o kh√¥ng?"
    ],
    
    Intent.GREETING: [
        "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω AI c·ªßa Ruby Wings, chuy√™n t∆∞ v·∫•n tour tr·∫£i nghi·ªám thi√™n nhi√™n v√† ch·ªØa l√†nh. üåø",
        "Ch√†o b·∫°n! R·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n. T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n v·ªÅ c√°c tour Ruby Wings?",
        "Hello! T√¥i l√† chatbot Ruby Wings, s·∫µn s√†ng h·ªó tr·ª£ b·∫°n t√¨m tour ph√π h·ª£p nh·∫•t."
    ],
    
    Intent.FAREWELL: [
        "C·∫£m ∆°n b·∫°n ƒë√£ tr√≤ chuy·ªán! Hy v·ªçng s·ªõm ƒë∆∞·ª£c ƒë·ªìng h√†nh c√πng b·∫°n trong h√†nh tr√¨nh tr·∫£i nghi·ªám. ‚ú®",
        "T·∫°m bi·ªát b·∫°n! Li√™n h·ªá hotline 0332510486 n·∫øu c·∫ßn h·ªó tr·ª£ th√™m nh√©!",
        "Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh! Mong s·ªõm ƒë∆∞·ª£c g·∫∑p l·∫°i b·∫°n trong tour Ruby Wings."
    ]
}

# --- NEW: Location Templates ---
LOCATION_TEMPLATES = {
    "no_tour_exact": [
        "Kh√¥ng c√≥ tour t·∫°i {location}. B·∫°n c√≥ mu·ªën tham kh·∫£o c√°c tour t∆∞∆°ng t·ª± t·∫°i {region} kh√¥ng?",
        "Hi·ªán ch∆∞a c√≥ tour n√†o t·∫°i {location}. T√¥i c√≥ th·ªÉ ƒë·ªÅ xu·∫•t tour ·ªü khu v·ª±c {region} nh√©?",
        "Ruby Wings ch∆∞a c√≥ tour ·ªü {location}. B·∫°n c√≥ quan t√¢m ƒë·∫øn tour t·∫°i {region} kh√¥ng?"
    ],
    
    "tour_found": [
        "T√¨m th·∫•y {count} tour t·∫°i {location}:",
        "D∆∞·ªõi ƒë√¢y l√† c√°c tour Ruby Wings t·∫°i {location}:",
        "C√≥ {count} tour ph√π h·ª£p t·∫°i {location} b·∫°n c√≥ th·ªÉ tham kh·∫£o:"
    ]
}

# --- NEW: Region Mapping ---
REGION_MAPPING = {
    "ƒë√† n·∫µng": "Mi·ªÅn Trung",
    "hu·∫ø": "Mi·ªÅn Trung",
    "qu·∫£ng tr·ªã": "Mi·ªÅn Trung",
    "b·∫°ch m√£": "Mi·ªÅn Trung",
    "h·ªôi an": "Mi·ªÅn Trung",
    "h√† n·ªôi": "Mi·ªÅn Trung B·∫Øc",  # Special case
    "h·∫° long": "Mi·ªÅn B·∫Øc",
    "sapa": "Mi·ªÅn B·∫Øc",
    "ninh b√¨nh": "Mi·ªÅn B·∫Øc",
    "h·ªì ch√≠ minh": "Mi·ªÅn Nam",
    "s√†i g√≤n": "Mi·ªÅn Nam",
    "c·∫ßn th∆°": "Mi·ªÅn Nam",
    "ph√∫ qu·ªëc": "Mi·ªÅn Nam",
    "nha trang": "Mi·ªÅn Nam",
    "ƒë√† l·∫°t": "Mi·ªÅn Nam"
}

# --- NEW: Tour Formatting Helpers ---
def format_tour_response(tours: List[Dict[str, Any]], max_tours: int = 3) -> Tuple[str, List[str]]:
    """
    Format tours with labels and structured information.
    Returns: (formatted_text, tour_labels)
    """
    if not tours:
        return "", []
    
    # Limit to max_tours
    tours = tours[:max_tours]
    tour_labels = []
    formatted_parts = []
    
    # Define labels based on position
    label_map = {
        0: "üèÜ Ph√π h·ª£p nh·∫•t",
        1: "‚≠ê Ph·ªï bi·∫øn",
        2: "üí∞ Gi√° t·ªët"
    }
    
    for i, tour in enumerate(tours):
        if not tour:
            continue
            
        # Get label
        label = label_map.get(i, f"{i+1}.")
        tour_labels.append(label)
        
        # Build tour line
        tour_line = f"{label} **{tour.get('tour_name', 'Tour')}**\n"
        
        # Add details if available
        if tour.get('location'):
            tour_line += f"   üìç {tour['location']}\n"
        if tour.get('duration'):
            tour_line += f"   ‚è±Ô∏è {tour['duration']}\n"
        if tour.get('price'):
            price = tour['price']
            if len(price) > 100:  # Truncate very long prices
                price = price[:100] + "..."
            tour_line += f"   üí∞ {price}\n"
        
        formatted_parts.append(tour_line)
    
    return "\n".join(formatted_parts), tour_labels

def extract_tour_info_from_passages(passages: List[Tuple[float, Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """Extract structured tour information from passages."""
    tours = {}
    
    for score, passage in passages:
        text = passage.get("text", "")
        path = passage.get("path", "")
        
        # Extract tour index from path
        tour_match = re.search(r'tours\[(\d+)\]', path)
        if not tour_match:
            continue
            
        tour_idx = int(tour_match.group(1))
        
        # Initialize tour dict if not exists
        if tour_idx not in tours:
            tours[tour_idx] = {
                "index": tour_idx,
                "tour_name": "",
                "location": "",
                "duration": "",
                "price": "",
                "score": 0.0
            }
        
        # Update tour info based on text content
        if "T√™n tour:" in text:
            for line in text.split('\n'):
                if line.startswith("T√™n tour:"):
                    tours[tour_idx]["tour_name"] = line.replace("T√™n tour:", "").strip()
                    break
        elif "ƒê·ªãa ƒëi·ªÉm:" in text:
            for line in text.split('\n'):
                if line.startswith("ƒê·ªãa ƒëi·ªÉm:"):
                    tours[tour_idx]["location"] = line.replace("ƒê·ªãa ƒëi·ªÉm:", "").strip()
                    break
        elif "Th·ªùi l∆∞·ª£ng:" in text:
            for line in text.split('\n'):
                if line.startswith("Th·ªùi l∆∞·ª£ng:"):
                    tours[tour_idx]["duration"] = line.replace("Th·ªùi l∆∞·ª£ng:", "").strip()
                    break
        elif "Gi√°:" in text:
            for line in text.split('\n'):
                if line.startswith("Gi√°:"):
                    tours[tour_idx]["price"] = line.replace("Gi√°:", "").strip()
                    break
        
        # Update score (highest score for this tour)
        tours[tour_idx]["score"] = max(tours[tour_idx]["score"], score)
    
    # Convert to list and sort by score
    tour_list = list(tours.values())
    tour_list.sort(key=lambda x: x["score"], reverse=True)
    
    return tour_list

def get_random_template(template_dict: Dict[str, List[str]], key: str, default: str = "") -> str:
    """Get random template from dict."""
    templates = template_dict.get(key, [default])
    return random.choice(templates) if templates else default

# --- Core function (Enhanced) ---
def validate_and_format_answer(
    llm_text: str,
    top_passages: List[Tuple[float, Dict[str, Any]]],
    requested_field: Optional[str] = None,
    tour_indices: Optional[List[int]] = None,
    max_chars: int = MAX_ANSWER_CHARS,
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Validate LLM answer against retrieved top_passages.
    If fails safety checks, return deterministic aggregated snippets instead.
    
    NEW: Supports state-based templates, location-aware responses, and improved formatting.
    
    Parameters:
      - llm_text: text returned by LLM (may be empty)
      - top_passages: list of (score, mapping_entry) where mapping_entry has 'path' and 'text'
      - requested_field: if provided, ensure answer addresses that field
      - tour_indices: list of tour indices in context (optional)
      - context: conversation context dict with state, intent, location, etc.
    """
    start = time.time()
    context = context or {}
    
    # Extract context values
    state = context.get("stage", ConversationStage.EXPLORE)
    intent = context.get("intent")
    location = context.get("location")
    location_filtered = context.get("location_filtered", False)
    has_phone = context.get("has_phone", False)
    phone = context.get("phone") or context.get("lead_phone")
    selected_tour_name = context.get("selected_tour_name")
    
    passages = collect_passage_texts(top_passages)
    paths = collect_passage_paths(top_passages)

    # sanitize LLM text first
    candidate = (llm_text or "").strip()
    candidate = html.unescape(candidate)
    candidate = re.sub(r"\s+\n", "\n", candidate)
    candidate = safe_shorten(candidate, max_chars)

    # NEW: Handle intent-specific responses first
    if intent and intent in INTENT_TEMPLATES:
        intent_response = generate_intent_response(intent, context)
        if intent_response:
            return {
                "answer": intent_response,
                "sources": [],
                "guard_passed": True,
                "reason": "intent_template",
                "state": state,
                "intent": intent,
                "elapsed": time.time() - start
            }

    # 1) If no retrieved evidence at all -> state-based fallback
    if not passages:
        fallback = generate_state_fallback(state, context, top_passages, requested_field)
        return {
            "answer": fallback,
            "sources": [],
            "guard_passed": False,
            "reason": "no_evidence",
            "state": state,
            "elapsed": time.time() - start
        }

    # 2) Check for explicit citation tokens in LLM text
    cited_tokens = extract_source_tokens(candidate)
    if cited_tokens:
        # map numeric citation tokens to mapping paths if possible: assume [1] -> top_passages[0], ...
        cited_paths = []
        for tok in cited_tokens:
            try:
                idx = int(tok.strip("[]")) - 1
                if 0 <= idx < len(top_passages):
                    cited_paths.append(paths[idx])
            except Exception:
                pass
        # basic evidence overlap check
        evidence_concat = " ".join(passages[:5])
        if overlap_ratio(normalize_for_overlap(candidate), normalize_for_overlap(evidence_concat)) >= MIN_OVERLAP_RATIO:
            # NEW: Add state template if appropriate
            if state in [ConversationStage.SUGGEST, ConversationStage.COMPARE]:
                candidate = add_state_template(candidate, state, context)
            
            return {
                "answer": candidate,
                "sources": cited_paths or paths[:3],
                "guard_passed": True,
                "reason": "ok",
                "state": state,
                "elapsed": time.time() - start
            }

    # 3) Token-overlap heuristic between LLM output and evidence
    evidence_concat = " ".join(passages[:5])
    ov = overlap_ratio(normalize_for_overlap(candidate), normalize_for_overlap(evidence_concat))
    if ov >= MIN_OVERLAP_RATIO:
        # 3a) if requested_field is provided ensure candidate mentions field-specific content from passages
        if requested_field:
            # find passages matching requested_field by path suffix
            field_passages = [m.get("text","") for _, m in top_passages if (m.get("path","").endswith(f".{requested_field}") or f".{requested_field}" in m.get("path",""))]
            if field_passages:
                field_ov = overlap_ratio(normalize_for_overlap(candidate), normalize_for_overlap(" ".join(field_passages[:4])))
                if field_ov < MIN_FIELD_MENTION_RATIO:
                    # mismatch: LLM didn't address requested field sufficiently
                    fallback = generate_state_fallback(state, context, top_passages, requested_field)
                    return {
                        "answer": fallback,
                        "sources": collect_passage_paths(top_passages)[:3],
                        "guard_passed": False,
                        "reason": "mismatch_field",
                        "state": state,
                        "elapsed": time.time() - start
                    }
        # 3b) ban hedging phrases to enforce professional tone where possible
        low = candidate.lower()
        for banned in BANNED_PHRASES:
            if banned in low:
                # remove banned phrase and continue; if too many banned phrases, fallback
                low = low.replace(banned, "")
        
        candidate = safe_shorten(candidate, max_chars)
        
        # NEW: Add location context if applicable
        if location_filtered and location:
            candidate = add_location_context(candidate, location, len(passages))
        
        # NEW: Add state template
        candidate = add_state_template(candidate, state, context)
        
        return {
            "answer": candidate,
            "sources": collect_passage_paths(top_passages)[:3],
            "guard_passed": True,
            "reason": "ok",
            "overlap": ov,
            "state": state,
            "location_filtered": location_filtered,
            "elapsed": time.time() - start
        }

    # 4) Low overlap -> LLM likely hallucinated -> state-based deterministic fallback
    fallback = generate_state_fallback(state, context, top_passages, requested_field)
    
    # NEW: Extract tour info for formatting
    tours_info = extract_tour_info_from_passages(top_passages)
    formatted_tours, tour_labels = format_tour_response(tours_info, max_tours=3)
    
    # Add formatted tours to fallback if available
    if formatted_tours and state in [ConversationStage.SUGGEST, ConversationStage.COMPARE, ConversationStage.EXPLORE]:
        if not fallback.endswith("\n\n"):
            fallback += "\n\n"
        fallback += formatted_tours
    
    return {
        "answer": fallback,
        "sources": collect_passage_paths(top_passages)[:3],
        "guard_passed": False,
        "reason": "low_overlap",
        "overlap": ov,
        "state": state,
        "tour_labels": tour_labels,
        "location_filtered": location_filtered,
        "elapsed": time.time() - start
    }

# --- NEW: Template Generation Functions ---
def generate_intent_response(intent: str, context: Dict[str, Any]) -> Optional[str]:
    """Generate intent-specific response."""
    templates = INTENT_TEMPLATES.get(intent)
    if not templates:
        return None
    
    template = random.choice(templates)
    
    # Fill template variables
    phone = context.get("phone") or context.get("lead_phone") or ""
    
    if intent == Intent.PROVIDE_PHONE and phone:
        return template.format(phone=phone)
    elif intent == Intent.CALLBACK_REQUEST and phone:
        return template + f"\n\nS·ªë ƒëi·ªán tho·∫°i c·ªßa b·∫°n l√† {phone} ƒë√∫ng kh√¥ng?"
    elif intent == Intent.BOOKING_CONFIRM:
        tour_name = context.get("selected_tour_name") or "tour ƒë√£ ch·ªçn"
        return template.format(tour_name=tour_name)
    else:
        return template

def generate_state_fallback(state: str, context: Dict[str, Any], 
                           top_passages: List[Tuple[float, Dict[str, Any]]], 
                           requested_field: Optional[str] = None) -> str:
    """Generate state-based fallback response."""
    
    # Try to get state template
    if state in STATE_TEMPLATES:
        template = random.choice(STATE_TEMPLATES[state])
        
        # Fill template variables
        phone = context.get("phone") or context.get("lead_phone") or ""
        tour_name = context.get("selected_tour_name") or ""
        location = context.get("location") or ""
        
        if state == ConversationStage.SELECT and tour_name:
            return template.format(tour_name=tour_name)
        elif state == ConversationStage.BOOK and tour_name:
            return template.format(tour_name=tour_name)
        elif state == ConversationStage.LEAD and phone:
            return template.format(phone=phone)
        elif state == ConversationStage.CALLBACK and phone:
            return template.format(phone=phone)
        else:
            return template
    
    # Default to deterministic fallback
    return deterministic_fallback_answer(top_passages, requested_field)

def add_state_template(text: str, state: str, context: Dict[str, Any]) -> str:
    """Add state-appropriate template to text."""
    if state not in STATE_TEMPLATES:
        return text
    
    # Only add template for certain states
    if state in [ConversationStage.SUGGEST, ConversationStage.COMPARE]:
        template = random.choice(STATE_TEMPLATES[state])
        
        # Check if template already present
        if not any(template_part in text for template_part in STATE_TEMPLATES[state]):
            text = template + "\n\n" + text
    
    return text

def add_location_context(text: str, location: str, tour_count: int) -> str:
    """Add location context to response."""
    if not location:
        return text
    
    region = REGION_MAPPING.get(location.lower(), "khu v·ª±c t∆∞∆°ng t·ª±")
    
    # Check if location info already in text
    location_lower = location.lower()
    text_lower = text.lower()
    
    if location_lower not in text_lower and "location" not in text_lower and "ƒë·ªãa ƒëi·ªÉm" not in text_lower:
        if tour_count > 0:
            template = random.choice(LOCATION_TEMPLATES["tour_found"])
            prefix = template.format(count=tour_count, location=location)
        else:
            template = random.choice(LOCATION_TEMPLATES["no_tour_exact"])
            prefix = template.format(location=location, region=region)
        
        text = prefix + "\n\n" + text
    
    return text

# --- Deterministic fallback builder (Enhanced) ---
def deterministic_fallback_answer(
    top_passages: List[Tuple[float, Dict[str, Any]]], 
    requested_field: Optional[str] = None, 
    max_snippets: int = 3,
    context: Optional[Dict[str, Any]] = None
) -> str:
    """
    Build a safe answer using only retrieved passages. Short, friendly, cites indexed sources [1],[2].
    If requested_field provided, prioritize passages whose path mentions that field.
    """
    if not top_passages:
        return "Xin l·ªói ‚Äî hi·ªán kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu v·ªÅ y√™u c·∫ßu c·ªßa b·∫°n."

    # prioritize field passages
    prioritized = []
    others = []
    for score, m in top_passages:
        p = m.get("path","")
        if requested_field and (p.endswith(f".{requested_field}") or f".{requested_field}" in p):
            prioritized.append((score, m))
        else:
            others.append((score, m))
    chosen = (prioritized + others)[:max_snippets]

    pieces = []
    for i, (score, m) in enumerate(chosen, start=1):
        text = m.get("text","").strip()
        text = safe_shorten(text, 800)
        pieces.append(f"[{i}] {text}")

    header = ""
    if requested_field:
        header = f'V·ªÅ "{requested_field}", t√¥i t√¨m th·∫•y th√¥ng tin sau (tr√≠ch t·ª´ t√†i li·ªáu Ruby Wings):\n\n'
    else:
        header = "T√¥i t√¨m th·∫•y th√¥ng tin sau t·ª´ d·ªØ li·ªáu Ruby Wings:\n\n"

    footer = "\n\nüí° *Li√™n h·ªá hotline 0332510486 ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt v√† ƒë·∫∑t tour*"
    return header + "\n\n".join(pieces) + footer

# --- Small CLI for quick manual tests ---
if __name__ == "__main__":
    # quick smoke test with new features
    sample_passages = [
        (1.0, {"path": "root.tours[0].price", "text": "Gi√° tour: 2.500.000 VNƒê/kh√°ch (tham kh·∫£o)."}),
        (0.9, {"path": "root.tours[0].transport", "text": "Ph∆∞∆°ng ti·ªán: Xe 16 ch·ªó ƒë·ªùi m·ªõi."}),
        (0.8, {"path": "root.tours[1].tour_name", "text": "D·∫•u ·∫•n Vƒ© tuy·∫øn ‚Äì K·∫øt n·ªëi th·∫ø h·ªá"})
    ]
    
    # Test with context
    context = {
        "stage": ConversationStage.SUGGEST,
        "intent": Intent.TOUR_INQUIRY,
        "location": "Hu·∫ø",
        "location_filtered": True
    }
    
    llm_good = "Gi√° tour l√† 2.500.000 VNƒê/kh√°ch. [1]"
    llm_bad = "B·∫°n ch·ªâ c·∫ßn mang 10 tri·ªáu v√† m·ªçi th·ª© s·∫Ω ·ªïn."  # hallucination
    
    print("=== TEST WITH CONTEXT ===")
    print("GOOD:", validate_and_format_answer(
        llm_good, sample_passages, requested_field="price", context=context
    ))
    
    print("\n=== TEST INTENT TEMPLATE ===")
    intent_context = {
        "intent": Intent.PROVIDE_PHONE,
        "phone": "0909123456",
        "stage": ConversationStage.LEAD
    }
    print("INTENT RESPONSE:", generate_intent_response(Intent.PROVIDE_PHONE, intent_context))
    
    print("\n=== TEST TOUR FORMATTING ===")
    tours_info = [
        {"tour_name": "Non n∆∞·ªõc B·∫°ch M√£", "location": "Hu·∫ø", "duration": "1 ng√†y", "price": "890.000 VNƒê"},
        {"tour_name": "M∆∞a ƒê·ªè v√† Tr∆∞·ªùng S∆°n", "location": "Qu·∫£ng Tr·ªã", "duration": "2 ng√†y 1 ƒë√™m", "price": "1.500.000 VNƒê"},
        {"tour_name": "K√Ω ·ª©c L·ªãch S·ª≠", "location": "Qu·∫£ng Tr·ªã - Hu·∫ø", "duration": "2 ng√†y 1 ƒë√™m", "price": "2.200.000 VNƒê"}
    ]
    formatted, labels = format_tour_response(tours_info)
    print("FORMATTED TOURS:\n", formatted)
    print("LABELS:", labels)
    
    print("\n=== TEST LOCATION TEMPLATE ===")
    text = "Tour r·∫•t th√∫ v·ªã"
    enhanced = add_location_context(text, "ƒê√† N·∫µng", 3)
    print("ORIGINAL:", text)
    print("ENHANCED:", enhanced)