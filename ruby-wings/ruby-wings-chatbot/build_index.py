#!/usr/bin/env python3
# build_index.py ‚Äî build embeddings/faiss index + mapping + tour_entities.json (compatible with app.py v5.2 & entities.py v5.2)
# Usage:
#   pip install -r requirements.txt
#   export OPENAI_API_KEY="sk-..."
#   python build_index.py

import os
import sys
import json
import time
import datetime
import re
from typing import Any, List, Optional, Tuple, Dict
import numpy as np

# try imports with helpful fallbacks
try:
    import faiss  # type: ignore
    HAS_FAISS = True
except Exception:
    faiss = None
    HAS_FAISS = False

# New OpenAI SDK
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# =========== NEW: C√ÅC H√ÄM X·ª¨ L√ù M·ªöI CHO C·∫§U TR√öC TOUR_ENTITIES ===========

def extract_region(location_text: str) -> str:
    """
    Tr√≠ch xu·∫•t region (Mi·ªÅn B·∫Øc/Trung/Nam) t·ª´ location string
    V√≠ d·ª•: "H√† N·ªôi ‚Äì ƒê√¥ng H√† ‚Äì Th√†nh C·ªï ‚Äì C·ª≠a Vi·ªát ‚Äì C·ªìn C·ªè" ‚Üí "Mi·ªÅn Trung"
    """
    if not location_text:
        return "Kh√¥ng x√°c ƒë·ªãnh"
    
    location_lower = location_text.lower()
    
    # Mapping c√°c keyword cho t·ª´ng mi·ªÅn (ƒë√£ c·∫≠p nh·∫≠t v·ªõi knowledge.json m·∫´u)
    north_keywords = ["h√† n·ªôi", "sapa", "h·∫° long", "ninh b√¨nh", "tam ƒë·∫£o", "m·ªôc ch√¢u", "ph√∫ th·ªç"]
    central_keywords = [
        "ƒë√† n·∫µng", "hu·∫ø", "qu·∫£ng tr·ªã", "nha trang", "h·ªôi an", "ƒë√¥ng h√†", 
        "c·ª≠a vi·ªát", "c·ªìn c·ªè", "qu·∫£ng b√¨nh", "b·∫°ch m√£", "hi·ªÅn l∆∞∆°ng", "khe sanh",
        "h∆∞·ªõng h√≥a", "vƒ© tuy·∫øn 17", "ƒë√¥i b·ªù hi·ªÅn l∆∞∆°ng", "v∆∞·ªùn qu·ªëc gia b·∫°ch m√£"
    ]
    south_keywords = [
        "ph√∫ qu·ªëc", "c·∫ßn th∆°", "c√† mau", "s√†i g√≤n", "th√†nh ph·ªë h·ªì ch√≠ minh", 
        "v≈©ng t√†u", "ƒë√† l·∫°t", "bu√¥n ma thu·ªôt", "nha trang"
    ]
    
    # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ng mi·ªÅn
    north_count = sum(1 for kw in north_keywords if kw in location_lower)
    central_count = sum(1 for kw in central_keywords if kw in location_lower)
    south_count = sum(1 for kw in south_keywords if kw in location_lower)
    
    # Ch·ªçn mi·ªÅn c√≥ s·ªë l·∫ßn xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
    counts = {"Mi·ªÅn B·∫Øc": north_count, "Mi·ªÅn Trung": central_count, "Mi·ªÅn Nam": south_count}
    region = max(counts, key=counts.get)
    
    return region if counts[region] > 0 else "Kh√¥ng x√°c ƒë·ªãnh"

def extract_tags(tour_data: Dict[str, Any]) -> List[str]:
    """
    Tr√≠ch xu·∫•t tags t·ª´ style, includes, notes c·ªßa tour
    """
    tags = []
    
    # L·∫•y c√°c field c·∫ßn thi·∫øt
    style = tour_data.get("style", "").lower()
    includes = " ".join(tour_data.get("includes", [])).lower() if isinstance(tour_data.get("includes"), list) else str(tour_data.get("includes", "")).lower()
    notes = tour_data.get("notes", "").lower()
    summary = tour_data.get("summary", "").lower()
    tour_name = tour_data.get("tour_name", "").lower()
    
    # Danh s√°ch keyword mapping c·∫≠p nh·∫≠t theo knowledge.json m·∫´u
    keyword_mapping = {
        "retreat": ["retreat", "ngh·ªâ d∆∞·ª°ng", "th∆∞ gi√£n", "tƒ©nh t√¢m", "ch·ªØa l√†nh", "t√°i t·∫°o nƒÉng l∆∞·ª£ng"],
        "t√¢m_linh": ["t√¢m linh", "thi·ªÅn", "ch√°nh ni·ªám", "t·ªãnh t√¢m", "c·∫ßu nguy·ªán", "n·ªôi t√¢m"],
        "l·ªãch_s·ª≠": ["l·ªãch s·ª≠", "tri √¢n", "di t√≠ch", "chi·∫øn tranh", "c·ª±u chi·∫øn binh", "k√Ω ·ª©c", "kh√°ng chi·∫øn"],
        "bi·ªÉn_ƒë·∫£o": ["bi·ªÉn", "ƒë·∫£o", "b√£i bi·ªÉn", "c·ªìn c·ªè", "c·ª≠a vi·ªát", "ven bi·ªÉn"],
        "vƒÉn_h√≥a": ["vƒÉn h√≥a", "b·∫£n ƒë·ªãa", "d√¢n t·ªôc", "c·ªông ƒë·ªìng", "v√¢n ki·ªÅu", "pa k√¥", "c·ªìng chi√™ng"],
        "team_building": ["team building", "c√¥ng ty", "doanh nghi·ªáp", "t·∫≠p th·ªÉ", "corporate"],
        "gia_ƒë√¨nh": ["gia ƒë√¨nh", "tr·∫ª em", "tr·∫ª nh·ªè", "ph√π h·ª£p gia ƒë√¨nh"],
        "ng∆∞·ªùi_l·ªõn_tu·ªïi": ["ng∆∞·ªùi l·ªõn tu·ªïi", "ng∆∞·ªùi gi√†", "senior", "ng∆∞·ªùi cao tu·ªïi"],
        "thi·ªÅn": ["thi·ªÅn", "kh√≠ c√¥ng", "ch√°nh ni·ªám", "yoga", "t·∫≠p luy·ªán tinh th·∫ßn"],
        "thi√™n_nhi√™n": ["r·ª´ng", "n√∫i", "su·ªëi", "thi√™n nhi√™n", "b·∫°ch m√£", "nguy√™n sinh", "c√¢y c·ªè"],
        "m·∫°o_hi·ªÉm": ["trekking", "leo n√∫i", "kh√°m ph√°", "m·∫°o hi·ªÉm", "th·ª≠ th√°ch"],
        "tr·∫£i_nghi·ªám": ["tr·∫£i nghi·ªám", "h√†nh tr√¨nh", "kh√°m ph√°", "th·ª±c t·∫ø"],
        "du_l·ªãch_t√¢m_linh": ["t√¢m linh", "thi·ªÅn ƒë·ªãnh", "ch·ªØa l√†nh", "tinh th·∫ßn"],
        "du_l·ªãch_l·ªãch_s·ª≠": ["l·ªãch s·ª≠", "di s·∫£n", "vƒÉn h√≥a", "truy·ªÅn th·ªëng"],
        "du_l·ªãch_ngh·ªâ_d∆∞·ª°ng": ["ngh·ªâ d∆∞·ª°ng", "th∆∞ gi√£n", "spa", "wellness"],
        "du_l·ªãch_sinh_th√°i": ["sinh th√°i", "m√¥i tr∆∞·ªùng", "xanh", "b·ªÅn v·ªØng"]
    }
    
    # Ki·ªÉm tra t·ª´ng keyword
    all_text = f"{tour_name} {style} {includes} {notes} {summary}"
    for tag, keywords in keyword_mapping.items():
        if any(keyword in all_text for keyword in keywords):
            tags.append(tag)
    
    return list(set(tags))

def parse_duration(duration_text: str) -> int:
    """
    Parse duration text th√†nh s·ªë ng√†y
    V√≠ d·ª•: "3 ng√†y 2 ƒë√™m" ‚Üí 3, "1 ng√†y" ‚Üí 1
    """
    if not duration_text:
        return 1
    
    # T√¨m s·ªë trong text (∆∞u ti√™n s·ªë ƒë·∫ßu ti√™n)
    numbers = re.findall(r'\d+', duration_text)
    if numbers:
        try:
            return int(numbers[0])
        except:
            pass
    
    # Fallback: d·ª±a v√†o keyword
    duration_lower = duration_text.lower()
    if "ng√†y" in duration_lower or "day" in duration_lower:
        if "2" in duration_lower or "hai" in duration_lower:
            return 2
        elif "3" in duration_lower or "ba" in duration_lower:
            return 3
        elif "4" in duration_lower or "b·ªën" in duration_lower:
            return 4
        elif "5" in duration_lower or "nƒÉm" in duration_lower:
            return 5
        elif "6" in duration_lower or "s√°u" in duration_lower:
            return 6
        elif "7" in duration_lower or "b·∫£y" in duration_lower:
            return 7
        else:
            return 1
    
    return 1  # M·∫∑c ƒë·ªãnh 1 ng√†y

def parse_price(price_text: str) -> Tuple[int, int, int]:
    """
    Parse price text th√†nh min_price, max_price, avg_price
    V√≠ d·ª•: "2.200.000 ƒë·∫øn 4.500.000 VNƒê/ng∆∞·ªùi" ‚Üí (2200000, 4500000, 3350000)
    """
    if not price_text:
        return 1000000, 2000000, 1500000
    
    price_lower = price_text.lower()
    
    # T√¨m t·∫•t c·∫£ s·ªë trong text (ƒë√£ b·ªè d·∫•u ch·∫•m ph√¢n c√°ch ng√†n)
    # Pattern t√¨m s·ªë c√≥ d·∫°ng: 1.000.000, 2,500,000, 1500000
    numbers = re.findall(r'[\d\,\.]+', price_lower)
    
    clean_numbers = []
    for num in numbers:
        try:
            # Lo·∫°i b·ªè d·∫•u ch·∫•m v√† ph·∫©y ph√¢n c√°ch ng√†n
            clean_num_str = num.replace('.', '').replace(',', '')
            clean_num = int(clean_num_str)
            
            # Ch·ªâ l·∫•y s·ªë l·ªõn h∆°n 1000 (tr√°nh s·ªë nh·ªè nh∆∞ nƒÉm, s·ªë l∆∞·ª£ng ng∆∞·ªùi)
            if clean_num >= 1000:
                clean_numbers.append(clean_num)
        except:
            continue
    
    if len(clean_numbers) >= 2:
        # Lo·∫°i b·ªè outliers (s·ªë qu√° l·ªõn so v·ªõi c√°c s·ªë kh√°c)
        if len(clean_numbers) > 2:
            avg_val = sum(clean_numbers) / len(clean_numbers)
            clean_numbers = [n for n in clean_numbers if n <= avg_val * 3]
        
        min_price = min(clean_numbers)
        max_price = max(clean_numbers)
        avg_price = (min_price + max_price) // 2
    elif len(clean_numbers) == 1:
        min_price = clean_numbers[0]
        # ∆Ø·ªõc t√≠nh max_price d·ª±a tr√™n context
        if "tri·ªáu" in price_lower:
            max_price = min_price * 2 if min_price < 5000000 else min_price + 2000000
        else:
            max_price = min_price * 1.5
        avg_price = (min_price + max_price) // 2
    else:
        # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, ∆∞·ªõc l∆∞·ª£ng t·ª´ text
        if "tri·ªáu" in price_lower:
            # Ki·ªÉm tra s·ªë tri·ªáu
            million_match = re.search(r'(\d+)\s*(tri·ªáu|tr)', price_lower)
            if million_match:
                try:
                    million_val = int(million_match.group(1))
                    base_price = million_val * 1000000
                    min_price = base_price
                    max_price = base_price + 2000000
                    avg_price = (min_price + max_price) // 2
                except:
                    min_price, max_price, avg_price = 2000000, 4000000, 3000000
            else:
                # Gi·∫£ s·ª≠ 2-4 tri·ªáu
                min_price, max_price, avg_price = 2000000, 4000000, 3000000
        elif "ngh√¨n" in price_lower or "k" in price_lower:
            # Gi·∫£ s·ª≠ 500k-1.5 tri·ªáu
            min_price, max_price, avg_price = 500000, 1500000, 1000000
        else:
            # M·∫∑c ƒë·ªãnh 1-2 tri·ªáu
            min_price, max_price, avg_price = 1000000, 2000000, 1500000
    
    return int(min_price), int(max_price), int(avg_price)

def create_embedding_text(tour_data: Dict[str, Any]) -> str:
    """
    T·∫°o text cho embedding t·ª´ c√°c field quan tr·ªçng
    """
    fields = [
        tour_data.get("tour_name", ""),
        tour_data.get("summary", ""),
        tour_data.get("location", ""),
        tour_data.get("style", ""),
        " ".join(tour_data.get("includes", [])) if isinstance(tour_data.get("includes"), list) else str(tour_data.get("includes", "")),
        tour_data.get("notes", ""),
        str(tour_data.get("duration", "")),
        str(tour_data.get("price", "")),
        str(tour_data.get("accommodation", "")),
        str(tour_data.get("meals", "")),
        str(tour_data.get("transport", "")),
        str(tour_data.get("event_support", ""))
    ]
    return " ".join([field for field in fields if field and str(field).strip()])

def calculate_popularity_score(tour_index: int, total_tours: int) -> float:
    """
    T√≠nh popularity score d·ª±a tr√™n v·ªã tr√≠ tour (gi·∫£ ƒë·ªãnh tour ƒë·∫ßu popular h∆°n)
    """
    # Tour ƒë·∫ßu ti√™n c√≥ score cao nh·∫•t, gi·∫£m d·∫ßn
    base_score = 0.7
    position_factor = (total_tours - tour_index) / total_tours  # t·ª´ 1 ƒë·∫øn 0
    return base_score + (0.3 * position_factor)

def calculate_value_score(min_price: int, max_price: int, duration_days: int) -> float:
    """
    T√≠nh value score d·ª±a tr√™n gi√° v√† s·ªë ng√†y (gi√° th·∫•p + ng√†y nhi·ªÅu = value cao)
    """
    if duration_days == 0 or max_price == 0:
        return 0.5
    
    avg_price = (min_price + max_price) / 2
    price_per_day = avg_price / duration_days
    
    # Normalize: gi√° m·ªói ng√†y d∆∞·ªõi 1 tri·ªáu -> score cao
    if price_per_day < 1000000:
        return 0.8
    elif price_per_day < 2000000:
        return 0.6
    else:
        return 0.4

# =========== H√ÄM FLATTEN JSON HI·ªÜN T·∫†I (ƒê√É C·∫¨P NH·∫¨T) ===========

def flatten_json(path: str) -> List[dict]:
    # fallback simple flattener - m·ªói tour l√† 1 passage duy nh·∫•t
    if not os.path.exists(path):
        raise FileNotFoundError(f"{path} not found")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    mapping = []
    
    # X·ª≠ l√Ω about_company (gi·ªØ nguy√™n)
    about = data.get("about_company", {})
    for key, value in about.items():
        if isinstance(value, str) and value.strip():
            mapping.append({
                "path": f"root.about_company.{key}",
                "text": value
            })
    
    # X·ª≠ l√Ω tours - M·ªñI TOUR L√Ä 1 PASSAGE DUY NH·∫§T
    tours = data.get("tours", [])
    for i, tour in enumerate(tours):
        tour_text_parts = []
        
        # Th√™m c√°c tr∆∞·ªùng quan tr·ªçng
        fields_to_include = [
            ("tour_name", "T√™n tour"),
            ("summary", "T√≥m t·∫Øt"),
            ("location", "ƒê·ªãa ƒëi·ªÉm"),
            ("duration", "Th·ªùi l∆∞·ª£ng"),
            ("price", "Gi√°"),
            ("notes", "L∆∞u √Ω"),
            ("style", "Phong c√°ch"),
            ("transport", "Ph∆∞∆°ng ti·ªán"),
            ("accommodation", "Ch·ªó ·ªü"),
            ("meals", "B·ªØa ƒÉn"),
            ("event_support", "H·ªó tr·ª£ s·ª± ki·ªán")
        ]
        
        for field_key, field_label in fields_to_include:
            if field_key in tour:
                value = tour[field_key]
                if isinstance(value, list):
                    tour_text_parts.append(f"{field_label}: {', '.join(str(v) for v in value)}")
                elif value and str(value).strip():
                    tour_text_parts.append(f"{field_label}: {value}")
        
        # X·ª≠ l√Ω includes
        if "includes" in tour and tour["includes"]:
            includes_text = "D·ªãch v·ª• bao g·ªìm: " + "; ".join(str(item) for item in tour["includes"])
            tour_text_parts.append(includes_text)
        
        # G·ªôp th√†nh 1 passage
        full_tour_text = "\n".join(tour_text_parts)
        
        mapping.append({
            "path": f"root.tours[{i}]",
            "text": full_tour_text
        })
    
    # X·ª≠ l√Ω FAQ v√† contact (gi·ªØ nguy√™n)
    faq = data.get("faq", {})
    for key, value in faq.items():
        if isinstance(value, str) and value.strip():
            mapping.append({
                "path": f"root.faq.{key}",
                "text": value
            })
    
    contact = data.get("contact", {})
    for key, value in contact.items():
        if isinstance(value, str) and value.strip():
            mapping.append({
                "path": f"root.contact.{key}",
                "text": value
            })
    
    return mapping

# =========== H√ÄM T·∫†O TOUR_ENTITIES V·ªöI C·∫§U TR√öC M·ªöI ===========

def create_tour_entities(tours_data: List[Dict[str, Any]], mapping: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    T·∫°o tour_entities.json v·ªõi c·∫•u tr√∫c m·ªõi t∆∞∆°ng th√≠ch v·ªõi h·ªá th·ªëng v5.2
    """
    tour_entities = {}
    total_tours = len(tours_data)
    
    for i, tour in enumerate(tours_data):
        tour_id = f"tour_{i:03d}"
        
        # Parse c√°c th√¥ng tin c∆° b·∫£n
        tour_name = tour.get("tour_name", "")
        location = tour.get("location", "")
        duration_text = tour.get("duration", "")
        price_text = tour.get("price", "")
        
        # Th√™m c√°c field m·ªõi v·ªõi c√°c h√†m x·ª≠ l√Ω m·ªõi
        region = extract_region(location)
        tags = extract_tags(tour)
        duration_days = parse_duration(duration_text)
        min_price, max_price, avg_price = parse_price(price_text)
        
        # T·∫°o embedding text
        embedding_text = create_embedding_text(tour)
        
        # T√≠nh c√°c score
        popularity_score = calculate_popularity_score(i, total_tours)
        value_score = calculate_value_score(min_price, max_price, duration_days)
        
        # Ki·ªÉm tra family/senior/corporate friendly t·ª´ tags
        family_friendly = "gia_ƒë√¨nh" in tags
        senior_friendly = "ng∆∞·ªùi_l·ªõn_tu·ªïi" in tags or all(word not in embedding_text.lower() for word in ["m·∫°o hi·ªÉm", "trekking", "leo n√∫i", "th·ª≠ th√°ch"])
        corporate_friendly = "team_building" in tags
        
        # T·∫°o tour entity v·ªõi c·∫•u tr√∫c m·ªõi
        tour_entities[tour_id] = {
            "tour_id": tour_id,
            "index": i,  # Th√™m index ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi app.py
            "tour_name": tour_name,
            "location": location,
            "region": region,  # M·ªöI: cho fallback suggestion
            
            "tags": tags,  # M·ªöI: cho filtering v√† classification
            
            "duration": duration_text,
            "duration_days": duration_days,  # M·ªöI: s·ªë ng√†y d·∫°ng s·ªë
            
            "price_text": price_text,
            "min_price": min_price,  # M·ªöI: gi√° nh·ªè nh·∫•t
            "max_price": max_price,  # M·ªöI: gi√° l·ªõn nh·∫•t
            "avg_price": avg_price,  # M·ªöI: gi√° trung b√¨nh
            
            "embedding_text": embedding_text,  # Cho FAISS embedding
            
            # Metadata cho ranking v√† filtering
            "popularity_score": round(popularity_score, 2),
            "value_score": round(value_score, 2),
            "family_friendly": family_friendly,
            "senior_friendly": senior_friendly,
            "corporate_friendly": corporate_friendly,
            
            # C√°c field kh√°c t·ª´ mapping (cho response guard)
            "summary": tour.get("summary", ""),
            "style": tour.get("style", ""),
            "includes": tour.get("includes", []),
            "notes": tour.get("notes", ""),
            "transport": tour.get("transport", ""),
            "accommodation": tour.get("accommodation", ""),
            "meals": tour.get("meals", ""),
            "event_support": tour.get("event_support", ""),
            
            # Metadata b·ªï sung
            "created_at": datetime.datetime.utcnow().isoformat() + "Z",
            "last_updated": datetime.datetime.utcnow().isoformat() + "Z"
        }
    
    return tour_entities

# =========== C√ÅC H√ÄM C≈® (GI·ªÆ NGUY√äN V·ªöI T√çCH H·ª¢P M·ªöI) ===========

def synthetic_embedding(text: str, dim: int = 1536) -> List[float]:
    h = abs(hash(text)) % (10 ** 12)
    return [(float((h >> (i % 32)) & 0xFF) + (i % 7)) / 255.0 for i in range(dim)]

def call_embeddings_with_retry(inputs: List[str], model: str) -> List[List[float]]:
    if not OPENAI_KEY or OpenAI is None:
        dim = 1536 if "3-small" in model else 3072
        return [synthetic_embedding(t, dim) for t in inputs]

    client = OpenAI(api_key=OPENAI_KEY)
    attempt = 0
    while attempt <= RETRY_LIMIT:
        try:
            resp = client.embeddings.create(model=model, input=inputs)
            if getattr(resp, "data", None):
                out = [r.embedding for r in resp.data]
                print(f"‚úÖ Generated {len(out)} embeddings (model={model})", flush=True)
                return out
            else:
                raise ValueError("Empty response from OpenAI embeddings API")
        except Exception as e:
            attempt += 1
            if attempt > RETRY_LIMIT:
                print(f"‚ùå Embedding API failed after {RETRY_LIMIT} attempts: {e}", file=sys.stderr)
                dim = 1536 if "3-small" in model else 3072
                return [synthetic_embedding(t, dim) for t in inputs]
            delay = RETRY_BASE * (2 ** (attempt - 1))
            print(f"‚ö†Ô∏è Embedding API error (attempt {attempt}/{RETRY_LIMIT}): {e}. Retrying in {delay:.1f}s...", file=sys.stderr)
            time.sleep(delay)
    dim = 1536 if "3-small" in model else 3072
    return [synthetic_embedding(t, dim) for t in inputs]

# =========== CONFIG ===========

OPENAI_KEY = os.environ.get("OPENAI_API_KEY", "").strip()

KNOW_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")
META_PATH = os.environ.get("FAISS_META_PATH", "faiss_index_meta.json")
TOUR_ENTITIES_PATH = os.environ.get("TOUR_ENTITIES_PATH", "tour_entities.json")

EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
BATCH_SIZE = int(os.environ.get("BUILD_BATCH_SIZE", "8"))
RETRY_LIMIT = int(os.environ.get("RETRY_LIMIT", "5"))
RETRY_BASE = float(os.environ.get("RETRY_BASE_DELAY", "1.0"))

TMP_EMB_FILE = "emb_tmp.bin"

# =========== MAIN BUILD FLOW (ƒê√É C·∫¨P NH·∫¨T) ===========

def build_index():
    print("=" * 60)
    print("BUILDING INDEX FOR RUBY WINGS v5.2")
    print("=" * 60)
    
    # 1. ƒê·ªçc knowledge.json
    print(f"\nüìö Reading knowledge from {KNOW_PATH}...")
    if not os.path.exists(KNOW_PATH):
        print(f"‚ùå Error: {KNOW_PATH} not found", file=sys.stderr)
        sys.exit(1)
    
    with open(KNOW_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    tours_data = data.get("tours", [])
    print(f"‚úÖ Found {len(tours_data)} tours")
    
    # 2. Flatten knowledge.json th√†nh mapping cho FAISS
    print("\nüîÑ Flattening knowledge.json for FAISS mapping...")
    mapping = flatten_json(KNOW_PATH)
    texts = [m.get("text", "") for m in mapping]
    n = len(texts)
    print(f"‚úÖ Created {n} passages for FAISS indexing")
    
    if n == 0:
        print("‚ùå No passages to index -> exit", file=sys.stderr)
        sys.exit(1)
    
    # 3. T·∫°o tour_entities.json v·ªõi c·∫•u tr√∫c m·ªõi
    print("\nüèóÔ∏è Creating tour_entities.json with enhanced structure...")
    tour_entities = create_tour_entities(tours_data, mapping)
    
    # L∆∞u tour_entities.json
    try:
        with open(TOUR_ENTITIES_PATH, "w", encoding="utf-8") as f:
            json.dump(tour_entities, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Saved enhanced tour_entities.json to {TOUR_ENTITIES_PATH}")
        print(f"   - Contains {len(tour_entities)} tours with new fields:")
        print(f"     ‚Ä¢ region, tags, duration_days, min/max/avg_price")
        print(f"     ‚Ä¢ popularity_score, value_score")
        print(f"     ‚Ä¢ family_friendly, senior_friendly, corporate_friendly")
    except Exception as e:
        print(f"‚ùå Failed to save tour_entities.json: {e}", file=sys.stderr)
    
    # 4. T·∫°o embeddings cho FAISS
    print("\nüß† Creating embeddings for FAISS index...")
    
    # remove tmp if exists
    if os.path.exists(TMP_EMB_FILE):
        try:
            os.remove(TMP_EMB_FILE)
        except Exception:
            pass

    dim: Optional[int] = None
    total_rows = 0
    batches = (n + BATCH_SIZE - 1) // BATCH_SIZE

    for start in range(0, n, BATCH_SIZE):
        batch = texts[start:start+BATCH_SIZE]
        inputs = [t if (t and str(t).strip()) else " " for t in batch]
        print(f"   Embedding batch {start//BATCH_SIZE + 1}/{batches} ...", flush=True)
        vecs = call_embeddings_with_retry(inputs, EMBEDDING_MODEL)

        # ensure no None entries
        for j, v in enumerate(vecs):
            if v is None:
                vecs[j] = synthetic_embedding(inputs[j], 1536 if "3-small" in EMBEDDING_MODEL else 3072)

        if dim is None and vecs:
            dim = len(vecs[0])

        arr = np.array(vecs, dtype="float32")
        norms = np.linalg.norm(arr, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        arr = arr / (norms + 1e-12)

        with open(TMP_EMB_FILE, "ab") as f:
            f.write(arr.tobytes())

        total_rows += arr.shape[0]

    if total_rows == 0 or dim is None:
        print("‚ùå No embeddings created -> exit", file=sys.stderr)
        sys.exit(1)

    print(f"‚úÖ Generated {total_rows} embeddings with dimension {dim}")
    
    # 5. Load embeddings v√† build FAISS index
    print("\nüîç Building FAISS index...")
    try:
        emb = np.memmap(TMP_EMB_FILE, dtype="float32", mode="r", shape=(total_rows, dim))
    except Exception:
        # fallback: load entire array into memory
        raw = np.fromfile(TMP_EMB_FILE, dtype="float32")
        emb = raw.reshape((total_rows, dim))

    # Build FAISS index if available
    if HAS_FAISS:
        try:
            index = faiss.IndexFlatIP(dim)
            index.add(np.asarray(emb))
            try:
                faiss.write_index(index, FAISS_INDEX_PATH)
                print(f"‚úÖ Saved FAISS index to {FAISS_INDEX_PATH}")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to persist FAISS index: {e}", file=sys.stderr)
                HAS_FAISS_local = False
        except Exception as e:
            print(f"‚ö†Ô∏è FAISS index build failed: {e}", file=sys.stderr)
            HAS_FAISS_local = False
        else:
            HAS_FAISS_local = True
    else:
        HAS_FAISS_local = False
        print("‚ö†Ô∏è FAISS not available, skipping FAISS index creation")

    # 6. Lu√¥n l∆∞u fallback vectors (npz) cho numpy fallback
    try:
        np.savez_compressed(FALLBACK_VECTORS_PATH, mat=np.asarray(emb))
        print(f"‚úÖ Saved fallback vectors to {FALLBACK_VECTORS_PATH}")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save fallback vectors: {e}", file=sys.stderr)

    # 7. L∆∞u mapping (list of {"path","text"}) expected by app.py
    print(f"\nüóÇÔ∏è Saving mapping to {FAISS_MAPPING_PATH} ...")
    try:
        with open(FAISS_MAPPING_PATH, "w", encoding="utf-8") as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Saved {len(mapping)} mapping entries")
    except Exception as e:
        print(f"‚ùå Failed to save mapping: {e}", file=sys.stderr)

    # 8. T·∫°o faiss_mapping.json cho t∆∞∆°ng th√≠ch
    print(f"\nüìã Creating FAISS mapping index...")
    try:
        # T·∫°o mapping t·ª´ index FAISS sang tour_id
        faiss_mapping = {}
        for i in range(len(tours_data)):
            faiss_mapping[str(i)] = f"tour_{i:03d}"
        
        with open("faiss_mapping.json", "w", encoding="utf-8") as f:
            json.dump(faiss_mapping, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Saved FAISS mapping index")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save FAISS mapping index: {e}", file=sys.stderr)

    # 9. Write metadata
    meta = {
        "created_at": datetime.datetime.utcnow().isoformat() + "Z",
        "num_passages": int(total_rows),
        "num_tours": len(tours_data),
        "embedding_model": EMBEDDING_MODEL,
        "dimension": int(dim),
        "faiss_available": bool(HAS_FAISS_local),
        "system_version": "v5.2",
        "notes": "Built with enhanced tour_entities.json structure for Ruby Wings v5.2",
        "features": {
            "region_extraction": True,
            "tags_extraction": True,
            "price_parsing": True,
            "duration_parsing": True,
            "popularity_scoring": True,
            "value_scoring": True
        }
    }
    try:
        with open(META_PATH, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Saved metadata to {META_PATH}")
    except Exception:
        print(f"‚ö†Ô∏è Failed to save metadata", file=sys.stderr)

    # 10. Cleanup temp file
    try:
        os.remove(TMP_EMB_FILE)
    except Exception:
        pass

    # 11. Summary
    print("\n" + "=" * 60)
    print("üéâ BUILD COMPLETE")
    print("=" * 60)
    print(f"\nüìä Summary:")
    print(f"   ‚Ä¢ Tours processed: {len(tours_data)}")
    print(f"   ‚Ä¢ FAISS passages: {total_rows}")
    print(f"   ‚Ä¢ Embedding dimension: {dim}")
    print(f"\nüìÅ Files created:")
    print(f"   ‚Ä¢ tour_entities.json: {TOUR_ENTITIES_PATH} (enhanced structure)")
    print(f"   ‚Ä¢ FAISS index: {FAISS_INDEX_PATH if HAS_FAISS_local else '(not available)'}")
    print(f"   ‚Ä¢ FAISS mapping: {FAISS_MAPPING_PATH}")
    print(f"   ‚Ä¢ Fallback vectors: {FALLBACK_VECTORS_PATH}")
    print(f"   ‚Ä¢ FAISS mapping index: faiss_mapping.json")
    print(f"   ‚Ä¢ Metadata: {META_PATH}")
    
    # Hi·ªÉn th·ªã sample c·ªßa 1 tour trong tour_entities.json
    if tour_entities:
        sample_id = list(tour_entities.keys())[0]
        sample_tour = tour_entities[sample_id]
        print(f"\nüìù Sample tour structure (first tour):")
        print(f"   Tour ID: {sample_id}")
        print(f"   Name: {sample_tour.get('tour_name', 'N/A')[:50]}...")
        print(f"   Location: {sample_tour.get('location', 'N/A')}")
        print(f"   Region: {sample_tour.get('region', 'N/A')}")
        print(f"   Tags: {', '.join(sample_tour.get('tags', []))}")
        print(f"   Duration: {sample_tour.get('duration', 'N/A')} ({sample_tour.get('duration_days', 'N/A')} days)")
        print(f"   Price range: {sample_tour.get('min_price', 'N/A'):,} - {sample_tour.get('max_price', 'N/A'):,} VND")
        print(f"   Popularity score: {sample_tour.get('popularity_score', 'N/A')}")
        print(f"   Value score: {sample_tour.get('value_score', 'N/A')}")
    
    print("\n‚úÖ Index ready for Ruby Wings v5.2 system!")

if __name__ == "__main__":
    try:
        build_index()
    except Exception as e:
        print(f"\n‚ùå ERROR building index: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)