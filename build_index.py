#!/usr/bin/env python3
# build_index.py ‚Äî B·∫£n T·ªêI ∆ØU NG·ªÆ C·∫¢NH TOUR (v2.0)
# T∆∞∆°ng th√≠ch 100% v·ªõi knowledge.json m·ªõi, ∆∞u ti√™n context tour
# Xu·∫•t: faiss_index.bin, vectors.npz, faiss_mapping.json

import os, json, time, sys
import numpy as np
from typing import List, Dict, Optional, Tuple

# Try FAISS
try:
    import faiss
    HAS_FAISS = True
except Exception:
    faiss = None
    HAS_FAISS = False

# OpenAI API m·ªõi
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# ---------- CONFIG ----------
OPENAI_KEY = os.environ.get("OPENAI_API_KEY", "").strip()
KNOWLEDGE_PATH = os.environ.get("KNOWLEDGE_PATH", "knowledge.json")
FIELD_KEYWORDS_PATH = os.environ.get("FIELD_KEYWORDS_PATH", "field_keywords.json")
FAISS_INDEX_PATH = os.environ.get("FAISS_INDEX_PATH", "faiss_index.bin")
FAISS_MAPPING_PATH = os.environ.get("FAISS_MAPPING_PATH", "faiss_mapping.json")
FALLBACK_VECTORS_PATH = os.environ.get("FALLBACK_VECTORS_PATH", "vectors.npz")
META_PATH = os.environ.get("META_PATH", "faiss_meta.json")
EMBED_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
BATCH = int(os.environ.get("BUILD_BATCH_SIZE", "32"))

# T·ª± ƒë·ªông nh·∫≠n di·ªán tr∆∞·ªùng t·ª´ knowledge.json
def detect_canonical_fields(data: dict) -> List[str]:
    """T·ª± ƒë·ªông ph√°t hi·ªán c√°c tr∆∞·ªùng t·ª´ c·∫•u tr√∫c knowledge.json"""
    fields = set()
    
    # Th√™m c√°c tr∆∞·ªùng t·ª´ about_company
    if "about_company" in data:
        for key in data["about_company"].keys():
            fields.add(f"about_company.{key}")
    
    # Th√™m c√°c tr∆∞·ªùng t·ª´ tours
    if "tours" in data and isinstance(data["tours"], list):
        for tour in data["tours"]:
            if isinstance(tour, dict):
                for key in tour.keys():
                    fields.add(key)
    
    # Th√™m c√°c tr∆∞·ªùng t·ª´ faq
    if "faq" in data:
        for key in data["faq"].keys():
            fields.add(f"faq.{key}")
    
    # Th√™m c√°c tr∆∞·ªùng t·ª´ contact
    if "contact" in data:
        for key in data["contact"].keys():
            fields.add(f"contact.{key}")
    
    return sorted(list(fields))

def synthetic_embedding(text: str, dim: int = 1536):
    """Fallback embedding khi kh√¥ng c√≥ API"""
    h = abs(hash(text)) % (10**12)
    return [(float((h >> (i % 32)) & 0xFF) + (i % 7)) / 255.0 for i in range(dim)]

def embed_batch(texts: List[str], model: str):
    """Batch embed v·ªõi fallback khi kh√¥ng c√≥ API"""
    if not OPENAI_KEY or OpenAI is None:
        dim = 1536 if "3-small" in model else 3072
        return [synthetic_embedding(t, dim) for t in texts]
    
    client = OpenAI(api_key=OPENAI_KEY)
    try:
        response = client.embeddings.create(model=model, input=texts)
        return [data.embedding for data in response.data]
    except Exception as e:
        print(f"‚ö†Ô∏è OpenAI embedding failed, using fallback: {e}")
        dim = 1536 if "3-small" in model else 3072
        return [synthetic_embedding(t, dim) for t in texts]

# ---------- ENHANCED FLATTEN KNOWLEDGE ----------
def flatten_knowledge() -> List[dict]:
    """Flatten knowledge.json v·ªõi th√¥ng tin ng·ªØ c·∫£nh phong ph√∫ cho chatbot"""
    if not os.path.exists(KNOWLEDGE_PATH):
        raise FileNotFoundError(f"{KNOWLEDGE_PATH} kh√¥ng t·ªìn t·∫°i")
    
    with open(KNOWLEDGE_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    mapping = []
    
    # Load field keywords ƒë·ªÉ h·ªó tr·ª£ mapping ch√≠nh x√°c
    field_keywords = {}
    if os.path.exists(FIELD_KEYWORDS_PATH):
        with open(FIELD_KEYWORDS_PATH, "r", encoding="utf-8") as f:
            field_keywords = json.load(f)
    
    def scan(obj, path="root", context: dict = None):
        """Qu√©t ƒë·ªá quy v·ªõi ng·ªØ c·∫£nh tour"""
        if context is None:
            context = {"current_tour_index": None, "current_tour_name": None}
        
        if isinstance(obj, dict):
            # Ki·ªÉm tra xem c√≥ ph·∫£i l√† m·ªôt tour kh√¥ng
            if "tour_name" in obj and isinstance(obj["tour_name"], str):
                # ƒê√¢y l√† m·ªôt tour
                tour_index = len([m for m in mapping if m.get("is_tour")]) if "tours" in path else 0
                new_context = {
                    "current_tour_index": tour_index,
                    "current_tour_name": obj["tour_name"]
                }
                for key, value in obj.items():
                    scan(value, f"{path}.{key}", new_context)
            else:
                # Kh√¥ng ph·∫£i tour, gi·ªØ nguy√™n context
                for key, value in obj.items():
                    scan(value, f"{path}.{key}", context)
        
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                # N·∫øu path ch·ª©a "tours", ƒë√¢y c√≥ th·ªÉ l√† danh s√°ch tours
                if "tours" in path and isinstance(item, dict) and "tour_name" in item:
                    tour_index = i
                    new_context = {
                        "current_tour_index": tour_index,
                        "current_tour_name": item.get("tour_name")
                    }
                    scan(item, f"{path}[{i}]", new_context)
                else:
                    scan(item, f"{path}[{i}]", context)
        
        elif isinstance(obj, str):
            text = obj.strip()
            if text:
                # X√°c ƒë·ªãnh field t·ª´ path
                field = path.split(".")[-1].split("[")[0]
                
                # Chu·∫©n h√≥a field d·ª±a tr√™n field_keywords
                normalized_field = field
                for main_field, keywords in field_keywords.items():
                    if field in keywords or any(field in kw for kw in keywords):
                        normalized_field = main_field.split(".")[-1] if "." in main_field else main_field
                        break
                
                # T·∫°o passage v·ªõi metadata phong ph√∫
                passage = {
                    "path": path,
                    "text": text,
                    "field": normalized_field,
                    "original_field": field,
                    "tour_index": context["current_tour_index"],
                    "tour_name": context["current_tour_name"],
                    "is_tour": context["current_tour_index"] is not None,
                    "context_score": 1.0 if context["current_tour_index"] is not None else 0.5,
                    "search_keywords": [],
                    "is_core_info": field in ["tour_name", "summary", "price", "duration"]
                }
                
                # Th√™m t·ª´ kh√≥a t√¨m ki·∫øm t·ª´ field_keywords
                if normalized_field in field_keywords:
                    passage["search_keywords"] = field_keywords[normalized_field]
                
                # Th√™m context text ƒë·ªÉ c·∫£i thi·ªán t√¨m ki·∫øm ng·ªØ c·∫£nh
                if context["current_tour_name"]:
                    passage["context_text"] = f"{context['current_tour_name']} {text}"
                else:
                    passage["context_text"] = text
                
                mapping.append(passage)
        
        else:
            # X·ª≠ l√Ω c√°c ki·ªÉu d·ªØ li·ªáu kh√°c (s·ªë, boolean)
            try:
                text = str(obj).strip()
                if text:
                    field = path.split(".")[-1].split("[")[0]
                    
                    passage = {
                        "path": path,
                        "text": text,
                        "field": field,
                        "original_field": field,
                        "tour_index": context["current_tour_index"],
                        "tour_name": context["current_tour_name"],
                        "is_tour": context["current_tour_index"] is not None,
                        "context_score": 1.0 if context["current_tour_index"] is not None else 0.5,
                        "search_keywords": [],
                        "is_core_info": False
                    }
                    
                    if field in field_keywords:
                        passage["search_keywords"] = field_keywords[field]
                    
                    mapping.append(passage)
            except:
                pass
    
    scan(data)
    
    # Th√™m c√°c passages ƒë·∫∑c bi·ªát ƒë·ªÉ c·∫£i thi·ªán t√¨m ki·∫øm ng·ªØ c·∫£nh
    enhanced_mapping = []
    for passage in mapping:
        # Th√™m passage g·ªëc
        enhanced_mapping.append(passage)
        
        # T·∫°o passage t√¨m ki·∫øm ƒëa ng·ªØ c·∫£nh cho c√°c tour
        if passage["is_tour"] and passage["tour_name"]:
            # Passage v·ªõi t√™n tour + n·ªôi dung (cho t√¨m ki·∫øm theo context)
            enhanced_passage = passage.copy()
            enhanced_passage["text"] = f"{passage['tour_name']}: {passage['text']}"
            enhanced_passage["context_score"] = 1.2  # TƒÉng ƒëi·ªÉm cho context r√µ r√†ng
            enhanced_mapping.append(enhanced_passage)
            
            # Passage ch·ªâ v·ªõi t√™n tour cho field quan tr·ªçng
            if passage["is_core_info"]:
                tour_only_passage = passage.copy()
                tour_only_passage["text"] = passage["tour_name"]
                tour_only_passage["field"] = "tour_name_context"
                tour_only_passage["context_score"] = 1.5  # ƒêi·ªÉm r·∫•t cao cho t√™n tour
                enhanced_mapping.append(tour_only_passage)
    
    print(f"üìä Th·ªëng k√™ mapping:")
    print(f"  - T·ªïng passages: {len(enhanced_mapping)}")
    print(f"  - Passages tour: {len([m for m in enhanced_mapping if m['is_tour']])}")
    print(f"  - Passages c√¥ng ty: {len([m for m in enhanced_mapping if not m['is_tour']])}")
    print(f"  - S·ªë tour duy nh·∫•t: {len(set([m['tour_name'] for m in enhanced_mapping if m['tour_name']]))}")
    
    return enhanced_mapping

# ---------- BUILD ENHANCED INDEX ----------
def build_enhanced_index():
    print("üöÄ B·∫Øt ƒë·∫ßu x√¢y d·ª±ng index n√¢ng cao...")
    print("üìñ ƒê·ªçc v√† x·ª≠ l√Ω knowledge.json...")
    
    mapping = flatten_knowledge()
    
    if not mapping:
        raise RuntimeError("Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë·ªÉ index - knowledge.json c√≥ th·ªÉ tr·ªëng")
    
    # Chu·∫©n b·ªã texts cho embedding
    texts = []
    metadata = []
    
    for passage in mapping:
        # ∆Øu ti√™n s·ª≠ d·ª•ng context_text n·∫øu c√≥
        text_to_embed = passage.get("context_text", passage["text"])
        texts.append(text_to_embed)
        metadata.append({
            "original_text": passage["text"],
            "field": passage["field"],
            "tour_index": passage["tour_index"],
            "tour_name": passage["tour_name"],
            "is_tour": passage["is_tour"],
            "context_score": passage["context_score"],
            "path": passage["path"],
            "is_core_info": passage.get("is_core_info", False)
        })
    
    n = len(texts)
    print(f"‚úÖ ƒê√£ t·∫°o {n} passages cho embedding")
    
    # T·∫°o embeddings theo batch
    print(f"üß† T·∫°o embeddings s·ª≠ d·ª•ng model: {EMBED_MODEL}")
    vectors = []
    
    for i in range(0, n, BATCH):
        batch_texts = texts[i:i+BATCH]
        batch_embeddings = embed_batch(batch_texts, EMBED_MODEL)
        vectors.extend(batch_embeddings)
        
        if (i // BATCH) % 5 == 0 or i + BATCH >= n:
            print(f"  ‚úÖ ƒê√£ x·ª≠ l√Ω {len(vectors)}/{n} passages")
    
    # Chuy·ªÉn th√†nh numpy array
    matrix = np.array(vectors, dtype="float32")
    
    # Chu·∫©n h√≥a vectors cho cosine similarity
    norms = np.linalg.norm(matrix, axis=1, keepdims=True)
    matrix = matrix / (norms + 1e-12)
    
    print(f"üìê K√≠ch th∆∞·ªõc vector: {matrix.shape}")
    
    # L∆∞u vectors d·ª± ph√≤ng
    np.savez_compressed(FALLBACK_VECTORS_PATH, matrix=matrix)
    print(f"üíæ ƒê√£ l∆∞u vectors d·ª± ph√≤ng: {FALLBACK_VECTORS_PATH}")
    
    # L∆∞u mapping v·ªõi metadata ƒë·∫ßy ƒë·ªß
    with open(FAISS_MAPPING_PATH, "w", encoding="utf-8") as f:
        json.dump({
            "mapping": mapping,
            "metadata": metadata,
            "total_passages": n,
            "tour_count": len(set([m["tour_name"] for m in mapping if m["tour_name"]])),
            "fields": list(set([m["field"] for m in mapping]))
        }, f, ensure_ascii=False, indent=2)
    print(f"üíæ ƒê√£ l∆∞u mapping metadata: {FAISS_MAPPING_PATH}")
    
    # X√¢y d·ª±ng FAISS index n·∫øu c√≥
    if HAS_FAISS:
        dim = matrix.shape[1]
        print(f"üî® ƒêang x√¢y d·ª±ng FAISS index (dim={dim})...")
        
        # S·ª≠ d·ª•ng IndexFlatIP cho cosine similarity
        index = faiss.IndexFlatIP(dim)
        index.add(matrix)
        
        faiss.write_index(index, FAISS_INDEX_PATH)
        print(f"üíæ ƒê√£ l∆∞u FAISS index: {FAISS_INDEX_PATH}")
    else:
        print("‚ö†Ô∏è FAISS kh√¥ng kh·∫£ d·ª•ng, ch·ªâ l∆∞u vectors th√¥")
    
    # L∆∞u metadata h·ªá th·ªëng
    meta_info = {
        "created_at": time.time(),
        "created_date": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_passages": int(n),
        "dimension": int(matrix.shape[1]),
        "embedding_model": EMBED_MODEL,
        "has_faiss": HAS_FAISS,
        "tour_specific_passages": len([m for m in mapping if m["is_tour"]]),
        "company_passages": len([m for m in mapping if not m["is_tour"]]),
        "context_enhanced": True,
        "version": "2.0-tour-context-optimized"
    }
    
    with open(META_PATH, "w", encoding="utf-8") as f:
        json.dump(meta_info, f, indent=2)
    print(f"üíæ ƒê√£ l∆∞u metadata: {META_PATH}")
    
    # T·∫°o file t√≥m t·∫Øt
    summary = {
        "build_completed": True,
        "timestamp": time.time(),
        "statistics": {
            "total_passages": n,
            "tours_count": meta_info["tour_specific_passages"],
            "unique_tours": meta_info["tour_specific_passages"] // 10,  # ∆Ø·ªõc l∆∞·ª£ng
            "embedding_dimension": matrix.shape[1],
            "file_sizes": {
                "knowledge.json": os.path.getsize(KNOWLEDGE_PATH) if os.path.exists(KNOWLEDGE_PATH) else 0,
                "vectors.npz": os.path.getsize(FALLBACK_VECTORS_PATH) if os.path.exists(FALLBACK_VECTORS_PATH) else 0,
                "mapping.json": os.path.getsize(FAISS_MAPPING_PATH) if os.path.exists(FAISS_MAPPING_PATH) else 0
            }
        }
    }
    
    print("\n" + "="*60)
    print("üéâ X√ÇY D·ª∞NG INDEX HO√ÄN T·∫§T!")
    print("="*60)
    print(f"üìä Th·ªëng k√™ cu·ªëi c√πng:")
    print(f"  ‚Ä¢ T·ªïng s·ªë passages: {n}")
    print(f"  ‚Ä¢ Passages thu·ªôc tour: {meta_info['tour_specific_passages']}")
    print(f"  ‚Ä¢ Passages th√¥ng tin c√¥ng ty: {meta_info['company_passages']}")
    print(f"  ‚Ä¢ Chi·ªÅu kh√¥ng gian embedding: {matrix.shape[1]}")
    print(f"  ‚Ä¢ H·ªó tr·ª£ FAISS: {'C√≥' if HAS_FAISS else 'Kh√¥ng'}")
    print(f"  ‚Ä¢ ∆Øu ti√™n ng·ªØ c·∫£nh tour: C√≥")
    print(f"\nüìÅ C√°c file ƒë√£ t·∫°o:")
    print(f"  ‚Ä¢ {FAISS_MAPPING_PATH}")
    print(f"  ‚Ä¢ {FALLBACK_VECTORS_PATH}")
    if HAS_FAISS:
        print(f"  ‚Ä¢ {FAISS_INDEX_PATH}")
    print(f"  ‚Ä¢ {META_PATH}")
    print("="*60)
    
    return True

def validate_index():
    """Ki·ªÉm tra index sau khi x√¢y d·ª±ng"""
    print("\nüîç Ki·ªÉm tra ch·∫•t l∆∞·ª£ng index...")
    
    files_to_check = [
        (FAISS_MAPPING_PATH, "Mapping file"),
        (FALLBACK_VECTORS_PATH, "Vectors file"),
        (META_PATH, "Metadata file")
    ]
    
    if HAS_FAISS:
        files_to_check.append((FAISS_INDEX_PATH, "FAISS index file"))
    
    all_ok = True
    for file_path, description in files_to_check:
        if os.path.exists(file_path):
            size = os.path.getsize(file_path)
            print(f"  ‚úÖ {description}: {file_path} ({size:,} bytes)")
        else:
            print(f"  ‚ùå {description}: {file_path} - KH√îNG T·ªíN T·∫†I")
            all_ok = False
    
    if all_ok:
        print("‚úÖ T·∫•t c·∫£ file index ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")
        
        # Ki·ªÉm tra c·∫•u tr√∫c mapping
        try:
            with open(FAISS_MAPPING_PATH, "r", encoding="utf-8") as f:
                mapping_data = json.load(f)
            
            if "mapping" in mapping_data:
                tour_passages = [m for m in mapping_data["mapping"] if m.get("is_tour")]
                print(f"  ‚Ä¢ {len(tour_passages)} passages c√≥ th√¥ng tin tour")
                print(f"  ‚Ä¢ {len(set([m.get('tour_name') for m in tour_passages if m.get('tour_name')]))} tour duy nh·∫•t")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Kh√¥ng th·ªÉ ki·ªÉm tra chi ti·∫øt mapping: {e}")
    else:
        print("‚ö†Ô∏è C√≥ v·∫•n ƒë·ªÅ v·ªõi m·ªôt s·ªë file index")
    
    return all_ok

if __name__ == "__main__":
    print("="*60)
    print("BUILD INDEX - RUBY WINGS TOUR CHATBOT")
    print("="*60)
    
    try:
        start_time = time.time()
        
        # X√¢y d·ª±ng index
        build_enhanced_index()
        
        # Ki·ªÉm tra
        validate_index()
        
        end_time = time.time()
        elapsed = end_time - start_time
        print(f"\n‚è±Ô∏è Th·ªùi gian th·ª±c hi·ªán: {elapsed:.2f} gi√¢y")
        print("="*60)
        
    except FileNotFoundError as e:
        print(f"\n‚ùå L·ªñI: {e}")
        print("Vui l√≤ng ki·ªÉm tra file knowledge.json v√† field_keywords.json")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå L·ªñI KH√îNG X√ÅC ƒê·ªäNH: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)